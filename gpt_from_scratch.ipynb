{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3abc0e0",
   "metadata": {},
   "source": [
    "# GPT From Scratch\n",
    "This notebook builds a complete GPT (Generative Pre-trained Transformer) model from scratch using PyTorch. It covers tokenization, self-attention, multi-head attention, transformer blocks, and text generation and all explained step-by-step with a simple nursery rhyme corpus.\n",
    "#### Author: [Kevin Thomas](mailto:ket189@pitt.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c22f2f9",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Imports\n",
    "\n",
    "First, let's import the libraries we need:\n",
    "- **torch**: The main PyTorch library for tensor operations\n",
    "- **torch.nn**: Neural network modules (layers, loss functions, etc.)\n",
    "- **torch.nn.functional**: Functional interface for operations like softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dfadcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.9.1\n",
      "CUDA available: False\n",
      "MPS available: True\n",
      "Using MPS (Apple Silicon)\n",
      "\n",
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch                        # Main PyTorch library\n",
    "import torch.nn as nn               # Neural network modules\n",
    "import torch.nn.functional as F     # Functional operations (softmax, relu, etc.)\n",
    "\n",
    "# Check our PyTorch setup\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"MPS available:\", torch.backends.mps.is_available())\n",
    "\n",
    "# Determine the best available device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using CUDA: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Apple Silicon)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# Final device output\n",
    "print(f\"\\nDevice: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75705062",
   "metadata": {},
   "source": [
    "## Part 2: Preparing the Training Data\n",
    "\n",
    "### 2.1 Creating a Corpus\n",
    "\n",
    "GPT models learn from text. We'll use a small corpus of simple sentences.\n",
    "In real applications, this would be millions of documents!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c564aa11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined text:\n",
      "mary had a little lamb <END> little lamb little lamb <END> mary had a little lamb <END> its fleece was white as snow <END> and everywhere that mary went <END> mary went mary went <END> everywhere that mary went <END> the lamb was sure to go <END> it followed her to school one day <END> school one day school one day <END> it followed her to school one day <END> which was against the rules <END> it made the children laugh and play <END> laugh and play laugh and play <END> it made the children laugh and play <END> to see a lamb at school <END>\n"
     ]
    }
   ],
   "source": [
    "# Our training corpus - the classic nursery rhyme \"Mary Had a Little Lamb\"\n",
    "# In production, you'd use a massive dataset (books, websites, etc.)\n",
    "corpus = [\n",
    "    \"mary had a little lamb\",\n",
    "    \"little lamb little lamb\",\n",
    "    \"mary had a little lamb\",\n",
    "    \"its fleece was white as snow\",\n",
    "    \"and everywhere that mary went\",\n",
    "    \"mary went mary went\",\n",
    "    \"everywhere that mary went\",\n",
    "    \"the lamb was sure to go\",\n",
    "    \"it followed her to school one day\",\n",
    "    \"school one day school one day\",\n",
    "    \"it followed her to school one day\",\n",
    "    \"which was against the rules\",\n",
    "    \"it made the children laugh and play\",\n",
    "    \"laugh and play laugh and play\",\n",
    "    \"it made the children laugh and play\",\n",
    "    \"to see a lamb at school\"\n",
    "]\n",
    "\n",
    "# Add an <END> token to mark sentence boundaries\n",
    "# This helps the model learn when to stop generating\n",
    "corpus = [sentence + \" <END>\" for sentence in corpus]\n",
    "\n",
    "# Combine all sentences into one continuous text\n",
    "text = \" \".join(corpus)\n",
    "print(\"Combined text:\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff71f9c",
   "metadata": {},
   "source": [
    "### 2.2 Building the Vocabulary\n",
    "\n",
    "**Tokenization** is the process of converting text into numbers that the model can process.\n",
    "We need to:\n",
    "1. Find all unique words (our vocabulary)\n",
    "2. Assign each word a unique number (index)\n",
    "3. Create mappings to convert between words and indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08668120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words (35 total):\n",
      "['as', 'day', 'had', 'lamb', 'that', 'play', 'the', 'mary', 'rules', 'everywhere', 'one', 'children', 'school', 'a', '<END>', 'fleece', 'made', 'little', 'and', 'her', 'against', 'its', 'at', 'was', 'went', 'to', 'go', 'it', 'see', 'sure', 'white', 'which', 'laugh', 'followed', 'snow']\n",
      "\n",
      "Vocabulary size: 35\n"
     ]
    }
   ],
   "source": [
    "# Get all unique words in our text\n",
    "# set() removes duplicates, list() converts back to a list\n",
    "words = list(set(text.split()))\n",
    "print(f\"Unique words ({len(words)} total):\")\n",
    "print(words)\n",
    "\n",
    "# Our vocabulary size is the number of unique words\n",
    "vocab_size = len(words)\n",
    "print(f\"\\nVocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "267b48c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2idx (word ‚Üí number):\n",
      "{'as': 0, 'day': 1, 'had': 2, 'lamb': 3, 'that': 4, 'play': 5, 'the': 6, 'mary': 7, 'rules': 8, 'everywhere': 9, 'one': 10, 'children': 11, 'school': 12, 'a': 13, '<END>': 14, 'fleece': 15, 'made': 16, 'little': 17, 'and': 18, 'her': 19, 'against': 20, 'its': 21, 'at': 22, 'was': 23, 'went': 24, 'to': 25, 'go': 26, 'it': 27, 'see': 28, 'sure': 29, 'white': 30, 'which': 31, 'laugh': 32, 'followed': 33, 'snow': 34}\n",
      "\n",
      "idx2word (number ‚Üí word):\n",
      "{0: 'as', 1: 'day', 2: 'had', 3: 'lamb', 4: 'that', 5: 'play', 6: 'the', 7: 'mary', 8: 'rules', 9: 'everywhere', 10: 'one', 11: 'children', 12: 'school', 13: 'a', 14: '<END>', 15: 'fleece', 16: 'made', 17: 'little', 18: 'and', 19: 'her', 20: 'against', 21: 'its', 22: 'at', 23: 'was', 24: 'went', 25: 'to', 26: 'go', 27: 'it', 28: 'see', 29: 'sure', 30: 'white', 31: 'which', 32: 'laugh', 33: 'followed', 34: 'snow'}\n"
     ]
    }
   ],
   "source": [
    "# Create word-to-index mapping (word2idx)\n",
    "# This dictionary maps each word to a unique integer\n",
    "# Example: {\"hello\": 0, \"friends\": 1, \"how\": 2, ...}\n",
    "word2idx = {word: idx for idx, word in enumerate(words)}\n",
    "print(\"word2idx (word ‚Üí number):\")\n",
    "print(word2idx)\n",
    "\n",
    "# Create index-to-word mapping (idx2word)\n",
    "# This is the reverse mapping for decoding model outputs\n",
    "# Example: {0: \"hello\", 1: \"friends\", 2: \"how\", ...}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "print(\"\\nidx2word (number ‚Üí word):\")\n",
    "print(idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531ff8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data tensor shape: torch.Size([106])\n",
      "Total tokens: 106\n",
      "\n",
      "First 20 tokens: tensor([ 7,  2, 13, 17,  3, 14, 17,  3, 17,  3, 14,  7,  2, 13, 17,  3, 14, 21,\n",
      "        15, 23])\n",
      "Decoded: mary had a little lamb <END> little lamb little lamb <END> mary had a little lamb <END> its fleece was\n"
     ]
    }
   ],
   "source": [
    "# Convert our entire text into a tensor of indices\n",
    "# This is the numerical representation of our training data\n",
    "data = torch.tensor([word2idx[word] for word in text.split()], dtype=torch.long)\n",
    "\n",
    "# Display data tensor information\n",
    "print(f\"Data tensor shape: {data.shape}\")\n",
    "print(f\"Total tokens: {len(data)}\")\n",
    "print(f\"\\nFirst 20 tokens: {data[:20]}\")\n",
    "print(f\"Decoded: {' '.join([idx2word[int(i)] for i in data[:20]])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ba575c",
   "metadata": {},
   "source": [
    "## Part 3: Hyperparameters\n",
    "\n",
    "These are the key settings that control our model's architecture and training:\n",
    "\n",
    "| Parameter | Description |\n",
    "|-----------|-------------|\n",
    "| `block_size` | Context window - how many tokens the model can \"see\" at once |\n",
    "| `embedding_dim` | Size of the vector representation for each token |\n",
    "| `n_heads` | Number of attention heads (parallel attention mechanisms) |\n",
    "| `n_layers` | Number of transformer blocks stacked together |\n",
    "| `lr` | Learning rate - how fast the model learns |\n",
    "| `epochs` | Number of training iterations |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa490b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters set!\n",
      "Context window: 6 tokens\n",
      "Embedding dimension: 32\n",
      "Attention heads: 2\n",
      "Transformer layers: 2\n"
     ]
    }
   ],
   "source": [
    "# Model architecture hyperparameters\n",
    "block_size = 6          # Context window: model sees 6 tokens at a time\n",
    "embedding_dim = 32      # Each token represented as a 32-dimensional vector\n",
    "n_heads = 2             # 2 parallel attention heads\n",
    "n_layers = 2            # 2 transformer blocks stacked\n",
    "\n",
    "# Training hyperparameters\n",
    "lr = 1e-3               # Learning rate (0.001)\n",
    "epochs = 1500           # Number of training steps\n",
    "\n",
    "# Display hyperparameters\n",
    "print(\"Hyperparameters set!\")\n",
    "print(f\"Context window: {block_size} tokens\")\n",
    "print(f\"Embedding dimension: {embedding_dim}\")\n",
    "print(f\"Attention heads: {n_heads}\")\n",
    "print(f\"Transformer layers: {n_layers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b3be46",
   "metadata": {},
   "source": [
    "## Part 4: Data Loading - Creating Training Batches\n",
    "\n",
    "During training, we feed the model:\n",
    "- **Input (x)**: A sequence of `block_size` tokens\n",
    "- **Target (y)**: The same sequence shifted by 1 (next token prediction)\n",
    "\n",
    "For example, if `block_size=6` and our text starts with \"hello friends how are you doing\":\n",
    "- x = [hello, friends, how, are, you, doing]\n",
    "- y = [friends, how, are, you, doing, <next_word>]\n",
    "\n",
    "The model learns to predict each next token given all previous tokens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d653b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input (x):\n",
      "tensor([[16,  6, 11, 32, 18,  5],\n",
      "        [ 7,  2, 13, 17,  3, 14]])\n",
      "\n",
      "Decoded x[0]: made the children laugh and play\n",
      "\n",
      "Target (y):\n",
      "tensor([[ 6, 11, 32, 18,  5, 14],\n",
      "        [ 2, 13, 17,  3, 14, 21]])\n",
      "\n",
      "Decoded y[0]: the children laugh and play <END>\n"
     ]
    }
   ],
   "source": [
    "def get_batch(batch_size=16):\n",
    "    \"\"\"\n",
    "    Create a random batch of training examples.\n",
    "    \n",
    "    Args:\n",
    "        batch_size: Number of sequences in each batch\n",
    "        \n",
    "    Returns:\n",
    "        x: Input sequences [batch_size, block_size]\n",
    "        y: Target sequences [batch_size, block_size] (shifted by 1)\n",
    "    \"\"\"\n",
    "    # Generate random starting positions for each sequence in the batch\n",
    "    # We subtract block_size to ensure we don't go past the end\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    \n",
    "    # Create input sequences: tokens from position i to i+block_size\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    \n",
    "    # Create target sequences: tokens from position i+1 to i+block_size+1\n",
    "    # This is the \"next token\" for each position in x\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    \n",
    "    # Return the input and target tensors\n",
    "    return x, y\n",
    "\n",
    "# Let's see an example batch\n",
    "x_example, y_example = get_batch(batch_size=2)\n",
    "print(\"Input (x):\")\n",
    "print(x_example)\n",
    "print(f\"\\nDecoded x[0]: {' '.join([idx2word[int(i)] for i in x_example[0]])}\")\n",
    "print(f\"\\nTarget (y):\")\n",
    "print(y_example)\n",
    "print(f\"\\nDecoded y[0]: {' '.join([idx2word[int(i)] for i in y_example[0]])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79661dac",
   "metadata": {},
   "source": [
    "## Part 5: Self-Attention - The Heart of Transformers ‚ù§Ô∏è\n",
    "\n",
    "### What is Self-Attention?\n",
    "\n",
    "Self-attention allows each token to \"look at\" all other tokens and decide which ones are most relevant. It computes:\n",
    "\n",
    "1. **Query (Q)**: \"What am I looking for?\"\n",
    "2. **Key (K)**: \"What do I contain?\"\n",
    "3. **Value (V)**: \"What information do I have to share?\"\n",
    "\n",
    "The attention score between two tokens is: $\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$\n",
    "\n",
    "### Causal Masking\n",
    "\n",
    "In GPT (decoder-only), we use **causal masking** to prevent tokens from attending to future tokens. A token at position $i$ can only see tokens at positions $0, 1, ..., i$. This is done using a lower triangular mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064b773c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 6, 32])\n",
      "Output shape: torch.Size([2, 6, 16])\n"
     ]
    }
   ],
   "source": [
    "class SelfAttentionHead(nn.Module):\n",
    "    \"\"\"\n",
    "    A single head of self-attention.\n",
    "    \n",
    "    This computes attention scores between all positions in the sequence,\n",
    "    allowing each token to gather information from relevant tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim, block_size, head_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embedding_dim: Dimension of input embeddings\n",
    "            block_size: Maximum sequence length (for masking)\n",
    "            head_size: Dimension of this attention head's output\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Linear projections for Query, Key, Value\n",
    "        # These learn to extract different aspects of each token\n",
    "        self.key = nn.Linear(embedding_dim, head_size, bias=False)    # What do I contain?\n",
    "        self.query = nn.Linear(embedding_dim, head_size, bias=False)  # What am I looking for?\n",
    "        self.value = nn.Linear(embedding_dim, head_size, bias=False)  # What info do I share?\n",
    "        \n",
    "        # Causal mask: lower triangular matrix\n",
    "        # This prevents attending to future tokens\n",
    "        # register_buffer stores this as a non-trainable parameter\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of self-attention.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape [Batch, Time, Channels]\n",
    "               Batch = number of sequences\n",
    "               Time = sequence length (number of tokens)\n",
    "               Channels = embedding dimension\n",
    "               \n",
    "        Returns:\n",
    "            Output tensor of shape [Batch, Time, head_size]\n",
    "        \"\"\"\n",
    "        B, T, C = x.shape  # Batch, Time (sequence length), Channels (embedding dim)\n",
    "        \n",
    "        # Compute Key and Query projections\n",
    "        k = self.key(x)    # [B, T, head_size] - What each token contains\n",
    "        q = self.query(x)  # [B, T, head_size] - What each token is looking for\n",
    "        \n",
    "        # Compute attention scores: (Q @ K^T) / sqrt(d_k)\n",
    "        # Q @ K^T gives us an [T, T] matrix of attention weights\n",
    "        # Dividing by sqrt(C) prevents scores from becoming too large\n",
    "        wei = q @ k.transpose(-2, -1) / (C ** 0.5)  # [B, T, T]\n",
    "        \n",
    "        # Apply causal mask: set future positions to -infinity\n",
    "        # After softmax, -inf becomes 0, so no attention to future\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        \n",
    "        # Softmax normalizes scores to probabilities (sum to 1)\n",
    "        wei = F.softmax(wei, dim=-1)  # [B, T, T]\n",
    "        \n",
    "        # Compute Value projection and apply attention weights\n",
    "        v = self.value(x)  # [B, T, head_size] - Information to aggregate\n",
    "        out = wei @ v      # [B, T, head_size] - Weighted sum of values\n",
    "        \n",
    "        # Return the output of the attention head\n",
    "        return out\n",
    "\n",
    "# Test our attention head\n",
    "test_head = SelfAttentionHead(embedding_dim, block_size, head_size=16)\n",
    "test_input = torch.randn(2, block_size, embedding_dim)  # Random input\n",
    "test_output = test_head(test_input)\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {test_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857acd73",
   "metadata": {},
   "source": [
    "### Let's Visualize the Causal Mask\n",
    "\n",
    "The mask ensures token at position $i$ can only attend to positions $\\leq i$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "633bc476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Causal Mask (1 = can attend, 0 = masked):\n",
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n",
      "\n",
      "Interpretation:\n",
      "- Row i shows which positions token i can attend to\n",
      "- Token 0 can only see itself\n",
      "- Token 1 can see tokens 0 and 1\n",
      "- Token 5 can see all 6 tokens\n"
     ]
    }
   ],
   "source": [
    "# Visualize the causal (triangular) mask\n",
    "mask = torch.tril(torch.ones(block_size, block_size))\n",
    "print(\"Causal Mask (1 = can attend, 0 = masked):\")\n",
    "print(mask)\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Row i shows which positions token i can attend to\")\n",
    "print(\"- Token 0 can only see itself\")\n",
    "print(\"- Token 1 can see tokens 0 and 1\")\n",
    "print(\"- Token 5 can see all 6 tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e43ec1",
   "metadata": {},
   "source": [
    "## Part 6: Multi-Head Attention\n",
    "\n",
    "**Why multiple heads?** Each attention head can learn to focus on different types of relationships:\n",
    "- One head might learn syntactic relationships (subject-verb)\n",
    "- Another might learn semantic relationships (synonyms)\n",
    "- Another might learn positional patterns\n",
    "\n",
    "We run multiple attention heads in parallel and concatenate their outputs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b2654932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 6, 32])\n",
      "Output shape: torch.Size([2, 6, 32])\n",
      "\n",
      "With 2 heads, each head has dimension: 16\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multiple heads of self-attention running in parallel.\n",
    "    \n",
    "    Each head has its own Q, K, V projections and learns different patterns.\n",
    "    Outputs are concatenated and projected back to embedding dimension.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim, block_size, num_heads):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embedding_dim: Total embedding dimension\n",
    "            block_size: Maximum sequence length\n",
    "            num_heads: Number of parallel attention heads\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Each head gets a fraction of the total embedding dimension\n",
    "        # Example: embedding_dim=32, num_heads=2 ‚Üí head_size=16\n",
    "        head_size = embedding_dim // num_heads\n",
    "        \n",
    "        # Create a list of attention heads\n",
    "        # ModuleList registers these as submodules for proper parameter tracking\n",
    "        self.heads = nn.ModuleList([\n",
    "            SelfAttentionHead(embedding_dim, block_size, head_size) \n",
    "            for _ in range(num_heads)\n",
    "        ])\n",
    "        \n",
    "        # Output projection: combines all head outputs back to embedding_dim\n",
    "        self.proj = nn.Linear(num_heads * head_size, embedding_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Run all attention heads and combine their outputs.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor [Batch, Time, embedding_dim]\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor [Batch, Time, embedding_dim]\n",
    "        \"\"\"\n",
    "        # Run each head and concatenate outputs along the last dimension\n",
    "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        \n",
    "        # Project back to embedding dimension\n",
    "        return self.proj(out)\n",
    "\n",
    "# Test multi-head attention\n",
    "test_mha = MultiHeadAttention(embedding_dim, block_size, n_heads)\n",
    "test_output = test_mha(test_input)\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {test_output.shape}\")\n",
    "print(f\"\\nWith {n_heads} heads, each head has dimension: {embedding_dim // n_heads}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fda5fe3",
   "metadata": {},
   "source": [
    "## Part 7: Feed-Forward Network\n",
    "\n",
    "After attention, each token passes through a simple feed-forward network independently. This adds non-linearity and allows the model to process the attended information.\n",
    "\n",
    "The structure is:\n",
    "1. **Linear expansion**: `embedding_dim` ‚Üí `4 * embedding_dim`\n",
    "2. **ReLU activation**: Adds non-linearity\n",
    "3. **Linear projection**: `4 * embedding_dim` ‚Üí `embedding_dim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5cdffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 6, 32])\n",
      "Output shape: torch.Size([2, 6, 32])\n",
      "\n",
      "Intermediate dimension: 128\n"
     ]
    }
   ],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple feed-forward network applied to each token position.\n",
    "    \n",
    "    This is a 2-layer MLP that expands the dimension by 4x,\n",
    "    applies non-linearity, and projects back to the original dimension.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_embd: Embedding dimension (input and output size)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Sequential container for the feed-forward layers\n",
    "        self.net = nn.Sequential(\n",
    "            # Expand: n_embd ‚Üí 4*n_embd (the \"inner\" dimension)\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            \n",
    "            # Non-linearity: ReLU(x) = max(0, x)\n",
    "            # This allows the network to learn non-linear patterns\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Project back: 4*n_embd ‚Üí n_embd\n",
    "            nn.Linear(4 * n_embd, n_embd)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Apply feed-forward network to each position independently.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor [Batch, Time, n_embd]\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor [Batch, Time, n_embd]\n",
    "        \"\"\"\n",
    "        # Return the output of the feed-forward network\n",
    "        return self.net(x)\n",
    "\n",
    "# Test feed-forward\n",
    "test_ff = FeedForward(embedding_dim)\n",
    "test_output = test_ff(test_input)\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {test_output.shape}\")\n",
    "print(f\"\\nIntermediate dimension: {4 * embedding_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cbfb35",
   "metadata": {},
   "source": [
    "## Part 8: Transformer Block\n",
    "\n",
    "A Transformer Block combines:\n",
    "1. **Multi-Head Attention** with residual connection and layer normalization\n",
    "2. **Feed-Forward Network** with residual connection and layer normalization\n",
    "\n",
    "**Residual Connections** (the `x + ...`) help with gradient flow during training.\n",
    "\n",
    "**Layer Normalization** stabilizes training by normalizing activations.\n",
    "\n",
    "```\n",
    "Input ‚Üí LayerNorm ‚Üí Multi-Head Attention ‚Üí + (residual)\n",
    "                                            ‚Üì\n",
    "                    LayerNorm ‚Üí FeedForward ‚Üí + (residual) ‚Üí Output\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46477462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 6, 32])\n",
      "Output shape: torch.Size([2, 6, 32])\n",
      "\n",
      "The transformer block preserves the shape!\n"
     ]
    }
   ],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    A single Transformer block.\n",
    "    \n",
    "    Combines multi-head self-attention with a feed-forward network,\n",
    "    using residual connections and layer normalization.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim, block_size, n_heads):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embedding_dim: Embedding dimension\n",
    "            block_size: Maximum sequence length\n",
    "            n_heads: Number of attention heads\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Self-attention: allows tokens to communicate\n",
    "        self.sa = MultiHeadAttention(embedding_dim, block_size, n_heads)\n",
    "        \n",
    "        # Feed-forward: processes each token independently\n",
    "        self.ffwd = FeedForward(embedding_dim)\n",
    "        \n",
    "        # Layer normalization layers\n",
    "        # These normalize the activations to have mean=0 and std=1\n",
    "        self.ln1 = nn.LayerNorm(embedding_dim)  # Before attention\n",
    "        self.ln2 = nn.LayerNorm(embedding_dim)  # Before feed-forward\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the transformer block.\n",
    "        \n",
    "        Uses \"pre-norm\" architecture: LayerNorm is applied before each sub-layer.\n",
    "        Residual connections add the input to the output of each sub-layer.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor [Batch, Time, embedding_dim]\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor [Batch, Time, embedding_dim]\n",
    "        \"\"\"\n",
    "        # Self-attention with residual connection\n",
    "        # x = x + attention(normalize(x))\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        \n",
    "        # Feed-forward with residual connection\n",
    "        # x = x + feedforward(normalize(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        \n",
    "        # Return the output of the transformer block\n",
    "        return x\n",
    "\n",
    "# Test the transformer block\n",
    "test_block = Block(embedding_dim, block_size, n_heads)\n",
    "test_output = test_block(test_input)\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {test_output.shape}\")\n",
    "print(\"\\nThe transformer block preserves the shape!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6689bb3",
   "metadata": {},
   "source": [
    "## Part 9: The Complete GPT Model üéâ\n",
    "\n",
    "Now we put everything together into a complete GPT model!\n",
    "\n",
    "**Architecture:**\n",
    "1. **Token Embedding**: Convert token IDs to vectors\n",
    "2. **Position Embedding**: Add positional information\n",
    "3. **Transformer Blocks**: Stack of N transformer blocks\n",
    "4. **Final LayerNorm**: Normalize the final output\n",
    "5. **Language Model Head**: Project to vocabulary size for prediction\n",
    "\n",
    "```\n",
    "Token IDs ‚Üí Token Embedding + Position Embedding\n",
    "              ‚Üì\n",
    "        Transformer Block 1\n",
    "              ‚Üì\n",
    "        Transformer Block 2\n",
    "              ‚Üì\n",
    "          ... (N blocks)\n",
    "              ‚Üì\n",
    "         Layer Norm\n",
    "              ‚Üì\n",
    "         Linear Head ‚Üí Logits (vocab_size)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca79fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TinyGPT Model created!\n",
      "Total parameters: 27,747\n",
      "\n",
      "Model architecture:\n",
      "TinyGPT(\n",
      "  (token_embedding): Embedding(35, 32)\n",
      "  (position_embedding): Embedding(6, 32)\n",
      "  (blocks): Sequential(\n",
      "    (0): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-1): 2 x SelfAttentionHead(\n",
      "            (key): Linear(in_features=32, out_features=16, bias=False)\n",
      "            (query): Linear(in_features=32, out_features=16, bias=False)\n",
      "            (value): Linear(in_features=32, out_features=16, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "      )\n",
      "      (ffwd): FeedForward(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (1): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-1): 2 x SelfAttentionHead(\n",
      "            (key): Linear(in_features=32, out_features=16, bias=False)\n",
      "            (query): Linear(in_features=32, out_features=16, bias=False)\n",
      "            (value): Linear(in_features=32, out_features=16, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "      )\n",
      "      (ffwd): FeedForward(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "  (head): Linear(in_features=32, out_features=35, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class TinyGPT(nn.Module):\n",
    "    \"\"\"\n",
    "    A tiny GPT (Generative Pre-trained Transformer) model.\n",
    "    \n",
    "    This is a decoder-only transformer that predicts the next token\n",
    "    given a sequence of previous tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Token Embedding: maps each token ID to a dense vector\n",
    "        # Shape: vocab_size ‚Üí embedding_dim\n",
    "        # Example: word ID 5 ‚Üí [0.2, -0.1, 0.8, ...] (32 dimensions)\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Position Embedding: encodes the position of each token\n",
    "        # Shape: block_size ‚Üí embedding_dim\n",
    "        # Position 0 has embedding [a, b, c, ...], Position 1 has [d, e, f, ...], etc.\n",
    "        self.position_embedding = nn.Embedding(block_size, embedding_dim)\n",
    "        \n",
    "        # Stack of Transformer Blocks\n",
    "        # nn.Sequential applies them one after another\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(embedding_dim, block_size, n_heads) for _ in range(n_layers)]\n",
    "        )\n",
    "        \n",
    "        # Final Layer Normalization\n",
    "        self.ln_f = nn.LayerNorm(embedding_dim)\n",
    "        \n",
    "        # Language Model Head: projects to vocabulary size\n",
    "        # Outputs \"logits\" (unnormalized scores) for each word in vocabulary\n",
    "        self.head = nn.Linear(embedding_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the GPT model.\n",
    "        \n",
    "        Args:\n",
    "            idx: Input token indices [Batch, Time]\n",
    "            targets: Target token indices [Batch, Time] (optional, for training)\n",
    "            \n",
    "        Returns:\n",
    "            logits: Prediction scores [Batch, Time, vocab_size]\n",
    "            loss: Cross-entropy loss (if targets provided)\n",
    "        \"\"\"\n",
    "        # Get batch size and sequence length\n",
    "        B, T = idx.shape  # Batch size, Sequence length\n",
    "        \n",
    "        # Get token embeddings: [B, T] ‚Üí [B, T, embedding_dim]\n",
    "        tok_emb = self.token_embedding(idx)\n",
    "        \n",
    "        # Get position embeddings: [T] ‚Üí [T, embedding_dim]\n",
    "        # torch.arange(T) creates [0, 1, 2, ..., T-1]\n",
    "        pos_emb = self.position_embedding(torch.arange(T, device=idx.device))\n",
    "        \n",
    "        # Add token and position embeddings\n",
    "        # Each token now knows both its identity and position\n",
    "        x = tok_emb + pos_emb  # [B, T, embedding_dim]\n",
    "        \n",
    "        # Pass through transformer blocks\n",
    "        x = self.blocks(x)  # [B, T, embedding_dim]\n",
    "        \n",
    "        # Final layer normalization\n",
    "        x = self.ln_f(x)  # [B, T, embedding_dim]\n",
    "        \n",
    "        # Project to vocabulary size\n",
    "        logits = self.head(x)  # [B, T, vocab_size]\n",
    "        \n",
    "        # Calculate loss if targets are provided\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            # Reshape for cross_entropy: need [N, C] and [N]\n",
    "            B, T, C = logits.shape\n",
    "            logits_flat = logits.view(B * T, C)  # Flatten to [B*T, vocab_size]\n",
    "            targets_flat = targets.view(B * T)   # Flatten to [B*T]\n",
    "            \n",
    "            # Cross-entropy loss: measures how well predictions match targets\n",
    "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
    "        \n",
    "        # Return logits and loss\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \"\"\"\n",
    "        Generate new tokens autoregressively.\n",
    "        \n",
    "        Args:\n",
    "            idx: Starting tokens [Batch, Time]\n",
    "            max_new_tokens: Number of new tokens to generate\n",
    "            \n",
    "        Returns:\n",
    "            Extended sequence [Batch, Time + max_new_tokens]\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop to last block_size tokens (context window limit)\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            \n",
    "            # Get predictions\n",
    "            logits, _ = self(idx_cond)  # [B, T, vocab_size]\n",
    "            \n",
    "            # Focus on the last token's prediction\n",
    "            logits = logits[:, -1, :]  # [B, vocab_size]\n",
    "            \n",
    "            # Convert to probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # [B, vocab_size]\n",
    "            \n",
    "            # Sample from the probability distribution\n",
    "            # multinomial samples indices based on their probabilities\n",
    "            next_idx = torch.multinomial(probs, 1)  # [B, 1]\n",
    "            \n",
    "            # Append the new token to the sequence\n",
    "            idx = torch.cat((idx, next_idx), dim=1)  # [B, T+1]\n",
    "        \n",
    "        # Return the extended sequence\n",
    "        return idx\n",
    "\n",
    "# Create the model\n",
    "model = TinyGPT()\n",
    "\n",
    "# Count parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"TinyGPT Model created!\")\n",
    "print(f\"Total parameters: {num_params:,}\")\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644c9198",
   "metadata": {},
   "source": [
    "## Part 10: Training the Model üèãÔ∏è\n",
    "\n",
    "Now we train our GPT model! The training loop:\n",
    "1. Get a batch of (input, target) pairs\n",
    "2. Forward pass: compute predictions and loss\n",
    "3. Backward pass: compute gradients\n",
    "4. Optimizer step: update model weights\n",
    "\n",
    "We're using the **AdamW optimizer**, which is the standard for transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c3e452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epochs: 1500\n",
      "Learning rate: 0.001\n",
      "----------------------------------------\n",
      "Step    0 | Loss: 3.7184\n",
      "Step  300 | Loss: 0.3292\n",
      "Step  600 | Loss: 0.2192\n",
      "Step  900 | Loss: 0.2919\n",
      "Step 1200 | Loss: 0.2376\n",
      "----------------------------------------\n",
      "Training complete! Final loss: 0.2620\n"
     ]
    }
   ],
   "source": [
    "# Create optimizer\n",
    "# AdamW is Adam with proper weight decay (L2 regularization)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "# Print training configuration\n",
    "print(\"Starting training...\")\n",
    "print(f\"Epochs: {epochs}\")\n",
    "print(f\"Learning rate: {lr}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Training loop\n",
    "for step in range(epochs):\n",
    "    # Get a batch of training data\n",
    "    xb, yb = get_batch()\n",
    "    \n",
    "    # Forward pass: compute predictions and loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    \n",
    "    # Zero out gradients from previous step\n",
    "    # (PyTorch accumulates gradients by default)\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Backward pass: compute gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # Optimizer step: update model weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print progress every 300 steps\n",
    "    if step % 300 == 0:\n",
    "        print(f\"Step {step:4d} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Training complete\n",
    "print(\"-\" * 40)\n",
    "print(f\"Training complete! Final loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0480d22c",
   "metadata": {},
   "source": [
    "## Part 11: Generating Text! üé®\n",
    "\n",
    "Now the fun part - let's make our model generate text!\n",
    "\n",
    "We'll give it a starting word, and it will predict the next words one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f06daed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting word: 'mary'\n",
      "\n",
      "Generating text...\n",
      "\n",
      "==================================================\n",
      "Generated Text:\n",
      "==================================================\n",
      "mary went <END> mary went mary went <END> everywhere that mary went <END> the lamb was\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Set model to evaluation mode\n",
    "# (disables dropout and other training-specific behaviors)\n",
    "model.eval()\n",
    "\n",
    "# Start with a single word\n",
    "start_word = \"mary\"\n",
    "print(f\"Starting word: '{start_word}'\")\n",
    "print(\"\\nGenerating text...\\n\")\n",
    "\n",
    "# Convert starting word to tensor\n",
    "context = torch.tensor([[word2idx[start_word]]], dtype=torch.long)\n",
    "\n",
    "# Generate new tokens\n",
    "with torch.no_grad():  # No need to track gradients during generation\n",
    "    generated = model.generate(context, max_new_tokens=15)\n",
    "\n",
    "# Convert indices back to words\n",
    "generated_text = \" \".join([idx2word[int(i)] for i in generated[0]])\n",
    "\n",
    "# Print the generated text\n",
    "print(\"=\" * 50)\n",
    "print(\"Generated Text:\")\n",
    "print(\"=\" * 50)\n",
    "print(generated_text)\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8813b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating from different starting words:\n",
      "\n",
      "'mary' ‚Üí mary went mary went <END> everywhere that mary went <END> the lamb was\n",
      "\n",
      "'the' ‚Üí the lamb was sure to go <END> it followed her to school one\n",
      "\n",
      "'it' ‚Üí it made the children laugh and play <END> it made the children laugh\n",
      "\n",
      "'little' ‚Üí little lamb <END> its fleece was white as snow <END> and everywhere that\n",
      "\n",
      "'lamb' ‚Üí lamb <END> little lamb little lamb <END> mary had a little lamb <END>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's try with different starting words!\n",
    "def generate_from_word(start_word, max_tokens=12):\n",
    "    \"\"\"Generate text starting from a given word.\"\"\"\n",
    "    if start_word not in word2idx:\n",
    "        print(f\"'{start_word}' not in vocabulary!\")\n",
    "        print(f\"Available words: {list(word2idx.keys())}\")\n",
    "        return\n",
    "    \n",
    "    # Create context tensor\n",
    "    context = torch.tensor([[word2idx[start_word]]], dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        generated = model.generate(context, max_new_tokens=max_tokens)\n",
    "    \n",
    "    # Decode generated indices to words\n",
    "    text = \" \".join([idx2word[int(i)] for i in generated[0]])\n",
    "    return text\n",
    "\n",
    "# Try different starting words from the nursery rhyme\n",
    "start_words = [\"mary\", \"the\", \"it\", \"little\", \"lamb\"]\n",
    "\n",
    "# Generate and print results\n",
    "print(\"Generating from different starting words:\\n\")\n",
    "for word in start_words:\n",
    "    result = generate_from_word(word)\n",
    "    if result:\n",
    "        print(f\"'{word}' ‚Üí {result}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f4dbf5",
   "metadata": {},
   "source": [
    "## Part 12: Understanding What We Built üìö\n",
    "\n",
    "### Summary\n",
    "\n",
    "Congratulations! You've built a working GPT model from scratch! Here's what each component does:\n",
    "\n",
    "| Component | Purpose |\n",
    "|-----------|--------|\n",
    "| **Token Embedding** | Converts word IDs to dense vectors that capture word meaning |\n",
    "| **Position Embedding** | Adds positional information so the model knows word order |\n",
    "| **Self-Attention** | Allows tokens to \"look at\" other tokens and gather information |\n",
    "| **Multi-Head Attention** | Runs multiple attention heads to capture different patterns |\n",
    "| **Feed-Forward Network** | Processes each token independently with non-linearity |\n",
    "| **Residual Connections** | Helps gradient flow and training stability |\n",
    "| **Layer Normalization** | Normalizes activations for stable training |\n",
    "| **Language Model Head** | Converts final representations to word predictions |\n",
    "\n",
    "### Key Differences from Production GPT\n",
    "\n",
    "Our TinyGPT is small for educational purposes. Real GPT models have:\n",
    "- **More layers**: GPT-3 has 96 layers, we have 2\n",
    "- **Larger embeddings**: GPT-3 uses 12,288 dimensions, we use 32\n",
    "- **More attention heads**: GPT-3 has 96 heads, we have 2\n",
    "- **Bigger vocabulary**: GPT-3 has ~50,000 tokens, we have ~40 words\n",
    "- **More training data**: GPT-3 trained on hundreds of billions of words!\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "To improve this model, you could:\n",
    "1. Use a larger corpus (download books, Wikipedia, etc.)\n",
    "2. Increase model size (more layers, larger embeddings)\n",
    "3. Use subword tokenization (BPE) instead of word-level\n",
    "4. Add dropout for regularization\n",
    "5. Train on a GPU with larger batch sizes\n",
    "6. Implement learning rate scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a82c727a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Model Statistics\n",
      "==================================================\n",
      "Vocabulary size: 35 words\n",
      "Context window: 6 tokens\n",
      "Embedding dimension: 32\n",
      "Attention heads: 2\n",
      "Transformer layers: 2\n",
      "Total parameters: 27,747\n",
      "==================================================\n",
      "\n",
      "üéâ You've built a GPT from scratch! üéâ\n"
     ]
    }
   ],
   "source": [
    "# Final model statistics\n",
    "print(\"=\" * 50)\n",
    "print(\"Model Statistics\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Vocabulary size: {vocab_size} words\")\n",
    "print(f\"Context window: {block_size} tokens\")\n",
    "print(f\"Embedding dimension: {embedding_dim}\")\n",
    "print(f\"Attention heads: {n_heads}\")\n",
    "print(f\"Transformer layers: {n_layers}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nüéâ You've built a GPT from scratch! üéâ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
