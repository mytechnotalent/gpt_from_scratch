{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3abc0e0",
   "metadata": {},
   "source": [
    "# GPT From Scratch\n",
    "\n",
    "## What is GPT?\n",
    "\n",
    "**GPT** stands for **Generative Pre-trained Transformer**. It's the technology behind ChatGPT and many other AI chatbots. But what does that mean?\n",
    "\n",
    "- **Generative**: It can *generate* (create) new text, like writing a story or answering a question\n",
    "- **Pre-trained**: It learned from reading billions of words before you use it\n",
    "- **Transformer**: The type of neural network architecture it uses (we'll build this!)\n",
    "\n",
    "## What Will You Learn?\n",
    "\n",
    "In this notebook, we'll build a **tiny GPT model from scratch**! By the end, you'll understand:\n",
    "\n",
    "1. **Tokenization**: How computers turn words into numbers\n",
    "2. **Embeddings**: How those numbers become meaningful representations\n",
    "3. **Attention**: How the model decides which words are important\n",
    "4. **Neural Networks**: The basic building blocks of AI\n",
    "5. **Training**: How the model learns from examples\n",
    "\n",
    "We'll use a simple nursery rhyme (\"Mary Had a Little Lamb\") so you can see exactly what's happening at every step!\n",
    "\n",
    "#### Author: [Kevin Thomas](mailto:ket189@pitt.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c22f2f9",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Imports\n",
    "\n",
    "Before we start building our AI, we need to import some tools. Think of this like getting your supplies ready before a science project!\n",
    "\n",
    "### What are these libraries?\n",
    "\n",
    "| Library | What it does | Real-world analogy |\n",
    "|---------|--------------|-------------------|\n",
    "| `torch` | The main AI library (PyTorch) | Your toolbox |\n",
    "| `torch.nn` | Pre-built neural network pieces | LEGO blocks |\n",
    "| `torch.nn.functional` | Math operations for AI | Calculator |\n",
    "\n",
    "**PyTorch** was created by Facebook's AI Research lab and is one of the most popular libraries for building AI. It handles all the complicated math for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d5dfadcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "CHECKING YOUR COMPUTER'S AI CAPABILITIES\n",
      "==================================================\n",
      "\n",
      "PyTorch version: 2.9.1\n",
      "\n",
      "NVIDIA GPU available (CUDA): False\n",
      "Apple GPU available (MPS): True\n",
      "\n",
      "âœ… Using Apple Silicon GPU (MPS)\n",
      "\n",
      "ğŸ–¥ï¸  Your AI will run on: mps\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# IMPORTING OUR AI TOOLS\n",
    "# ============================================================================\n",
    "# These three lines bring in all the tools we need to build our GPT model.\n",
    "# It's like importing LEGO sets - each one has different pieces we'll use!\n",
    "\n",
    "# torch: The main PyTorch library - handles all the math with \"tensors\" (fancy arrays)\n",
    "import torch\n",
    "\n",
    "# torch.nn: Pre-built neural network building blocks (like layers)\n",
    "import torch.nn as nn\n",
    "\n",
    "# torch.nn.functional: Math functions we'll use (like softmax for probabilities)\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ============================================================================\n",
    "# CHECKING OUR COMPUTER'S CAPABILITIES\n",
    "# ============================================================================\n",
    "# Neural networks need to do LOTS of math. Let's see what our computer can do!\n",
    "\n",
    "# Print the version of PyTorch we're using\n",
    "print(\"=\" * 50)\n",
    "print(\"CHECKING YOUR COMPUTER'S AI CAPABILITIES\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Check if we have a GPU (Graphics Processing Unit)\n",
    "# GPUs are MUCH faster at AI math than regular CPUs!\n",
    "print(f\"\\nNVIDIA GPU available (CUDA): {torch.cuda.is_available()}\")\n",
    "print(f\"Apple GPU available (MPS): {torch.backends.mps.is_available()}\")\n",
    "\n",
    "# Pick the best available device\n",
    "# Priority: NVIDIA GPU > Apple GPU > CPU\n",
    "if torch.cuda.is_available():\n",
    "    # NVIDIA GPU - fastest option!\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"\\nâœ… Using NVIDIA GPU: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    # Apple Silicon GPU - fast on Mac!\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"\\nâœ… Using Apple Silicon GPU (MPS)\")\n",
    "else:\n",
    "    # CPU - works on any computer, just slower\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"\\nâœ… Using CPU (this will work, just slower)\")\n",
    "\n",
    "# Show the final device choice\n",
    "print(f\"\\nğŸ–¥ï¸  Your AI will run on: {device}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75705062",
   "metadata": {},
   "source": [
    "## Part 2: Preparing the Training Data\n",
    "\n",
    "### 2.1 What is Training Data?\n",
    "\n",
    "**Training data** is the text that our AI will learn from. Think of it like a textbook for the AI!\n",
    "\n",
    "**The Big Idea:** The AI will read this text over and over, learning patterns like:\n",
    "- \"mary\" is often followed by \"had\" or \"went\"\n",
    "- \"little\" is often followed by \"lamb\"\n",
    "- Sentences often end with certain words\n",
    "\n",
    "### Real GPT vs Our Tiny GPT\n",
    "\n",
    "| | Real GPT (like ChatGPT) | Our Tiny GPT |\n",
    "|---|---|---|\n",
    "| **Training Data** | Billions of web pages, books, articles | One nursery rhyme |\n",
    "| **Words Learned** | 50,000+ word pieces | ~25 words |\n",
    "| **Training Time** | Months on supercomputers | Seconds on your laptop |\n",
    "\n",
    "### 2.2 Creating Our Corpus\n",
    "\n",
    "A **corpus** is just a fancy word for \"a collection of text.\" Our corpus is the nursery rhyme, \"Mary Had a Little Lamb\", however simple enough to understand, but shows all the concepts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c564aa11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence with END token:\n",
      "  'mary had a little lamb <END>'\n",
      "\n",
      "============================================================\n",
      "OUR COMPLETE TRAINING TEXT:\n",
      "============================================================\n",
      "mary had a little lamb <END> little lamb little lamb <END> mary had a little lamb <END> its fleece was white as snow <END> and everywhere that mary went <END> mary went mary went <END> everywhere that mary went <END> the lamb was sure to go <END> it followed her to school one day <END> school one day school one day <END> it followed her to school one day <END> which was against the rules <END> it made the children laugh and play <END> laugh and play laugh and play <END> it made the children laugh and play <END> to see a lamb at school <END>\n",
      "\n",
      "ğŸ“Š Total characters: 546\n",
      "ğŸ“Š Total words: 106\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CREATING OUR TRAINING DATA (THE CORPUS)\n",
    "# ============================================================================\n",
    "# This is the text our AI will learn from. We're using, \"Mary Had a Little Lamb\",\n",
    "# because it's simple, repetitive and familiar.\n",
    "\n",
    "# Our training corpus where each line is one sentence from the nursery rhyme.\n",
    "# The AI will learn patterns from these sentences!\n",
    "corpus = [\n",
    "    \"mary had a little lamb\",              # Sentence 1\n",
    "    \"little lamb little lamb\",             # Sentence 2\n",
    "    \"mary had a little lamb\",              # Sentence 3\n",
    "    \"its fleece was white as snow\",        # Sentence 4\n",
    "    \"and everywhere that mary went\",       # Sentence 5\n",
    "    \"mary went mary went\",                 # Sentence 6\n",
    "    \"everywhere that mary went\",           # Sentence 7\n",
    "    \"the lamb was sure to go\",             # Sentence 8\n",
    "    \"it followed her to school one day\",   # Sentence 9\n",
    "    \"school one day school one day\",       # Sentence 10\n",
    "    \"it followed her to school one day\",   # Sentence 11\n",
    "    \"which was against the rules\",         # Sentence 12\n",
    "    \"it made the children laugh and play\", # Sentence 13\n",
    "    \"laugh and play laugh and play\",       # Sentence 14\n",
    "    \"it made the children laugh and play\", # Sentence 15\n",
    "    \"to see a lamb at school\"              # Sentence 16\n",
    "]\n",
    "\n",
    "# ============================================================================\n",
    "# ADDING A SPECIAL \"END\" TOKEN\n",
    "# ============================================================================\n",
    "# We add \"<END>\" to mark where each sentence stops.\n",
    "# This teaches the AI to know when a thought is complete!\n",
    "# Without this, it might just ramble forever.\n",
    "\n",
    "# Add \" <END>\" to the end of every sentence\n",
    "corpus = [sentence + \" <END>\" for sentence in corpus]\n",
    "\n",
    "# Let's see what one sentence looks like now\n",
    "print(\"Example sentence with END token:\")\n",
    "print(f\"  '{corpus[0]}'\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# COMBINING ALL SENTENCES INTO ONE BIG TEXT\n",
    "# ============================================================================\n",
    "# We join all sentences together with spaces between them.\n",
    "# This creates one long string of text for training.\n",
    "\n",
    "# Join all sentences with a space between each one\n",
    "text = \" \".join(corpus)\n",
    "\n",
    "# Show the complete training text\n",
    "print(\"=\" * 60)\n",
    "print(\"OUR COMPLETE TRAINING TEXT:\")\n",
    "print(\"=\" * 60)\n",
    "print(text)\n",
    "print()\n",
    "print(f\"ğŸ“Š Total characters: {len(text)}\")\n",
    "print(f\"ğŸ“Š Total words: {len(text.split())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff71f9c",
   "metadata": {},
   "source": [
    "### 2.2 Building the Vocabulary (Teaching the AI Words)\n",
    "\n",
    "Before our AI can read text, we need to convert words into numbers. This is called **tokenization**.\n",
    "\n",
    "**Why numbers?** Computers can't understand words directly as they only understand numbers! So we need to give each word a unique number (like an ID or index).\n",
    "\n",
    "**Here's our 3-step plan:**\n",
    "1. **Find all unique words** in our text (our \"vocabulary\")\n",
    "2. **Assign each word a unique number** (like giving each word an ID badge)\n",
    "3. **Create dictionaries** so we can easily convert between words â†” numbers\n",
    "\n",
    "**Real-world example:**\n",
    "```\n",
    "Word:   \"mary\" â†’ Number: 0\n",
    "Word:   \"had\"  â†’ Number: 1  \n",
    "Word:   \"a\"    â†’ Number: 2\n",
    "...and so on\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "08668120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ALL UNIQUE WORDS IN OUR TEXT:\n",
      "============================================================\n",
      "['one', 'against', 'that', 'was', 'followed', 'day', '<END>', 'everywhere', 'its', 'white', 'which', 'snow', 'as', 'a', 'it', 'the', 'play', 'to', 'made', 'little', 'school', 'laugh', 'went', 'her', 'see', 'rules', 'fleece', 'lamb', 'had', 'and', 'at', 'go', 'mary', 'children', 'sure']\n",
      "\n",
      "ğŸ“Š We found 35 unique words!\n",
      "   This means our vocabulary size = 35\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 1: FIND ALL UNIQUE WORDS IN OUR TEXT\n",
    "# ============================================================================\n",
    "# We use set() to automatically remove duplicate words.\n",
    "# For example, \"mary\" appears many times but set() keeps only ONE copy.\n",
    "# Then we convert back to a list so we can work with it.\n",
    "\n",
    "# Split text into words, then remove duplicates using set()\n",
    "words = list(set(text.split()))\n",
    "\n",
    "# Let's see what unique words we found!\n",
    "print(\"=\" * 60)\n",
    "print(\"ALL UNIQUE WORDS IN OUR TEXT:\")\n",
    "print(\"=\" * 60)\n",
    "print(words)\n",
    "print()\n",
    "\n",
    "# Count how many unique words we have - this is our \"vocabulary size\"\n",
    "vocab_size = len(words)\n",
    "print(f\"ğŸ“Š We found {vocab_size} unique words!\")\n",
    "print(f\"   This means our vocabulary size = {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "267b48c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DICTIONARY 1: word2idx (Word â†’ Number)\n",
      "============================================================\n",
      "Use this to ENCODE words into numbers the AI can process\n",
      "\n",
      "   'one' â†’ 0\n",
      "   'against' â†’ 1\n",
      "   'that' â†’ 2\n",
      "   'was' â†’ 3\n",
      "   'followed' â†’ 4\n",
      "   'day' â†’ 5\n",
      "   '<END>' â†’ 6\n",
      "   'everywhere' â†’ 7\n",
      "   'its' â†’ 8\n",
      "   'white' â†’ 9\n",
      "   'which' â†’ 10\n",
      "   'snow' â†’ 11\n",
      "   'as' â†’ 12\n",
      "   'a' â†’ 13\n",
      "   'it' â†’ 14\n",
      "   'the' â†’ 15\n",
      "   'play' â†’ 16\n",
      "   'to' â†’ 17\n",
      "   'made' â†’ 18\n",
      "   'little' â†’ 19\n",
      "   'school' â†’ 20\n",
      "   'laugh' â†’ 21\n",
      "   'went' â†’ 22\n",
      "   'her' â†’ 23\n",
      "   'see' â†’ 24\n",
      "   'rules' â†’ 25\n",
      "   'fleece' â†’ 26\n",
      "   'lamb' â†’ 27\n",
      "   'had' â†’ 28\n",
      "   'and' â†’ 29\n",
      "   'at' â†’ 30\n",
      "   'go' â†’ 31\n",
      "   'mary' â†’ 32\n",
      "   'children' â†’ 33\n",
      "   'sure' â†’ 34\n",
      "\n",
      "============================================================\n",
      "DICTIONARY 2: idx2word (Number â†’ Word)\n",
      "============================================================\n",
      "Use this to DECODE numbers back into readable words\n",
      "\n",
      "   0 â†’ 'one'\n",
      "   1 â†’ 'against'\n",
      "   2 â†’ 'that'\n",
      "   3 â†’ 'was'\n",
      "   4 â†’ 'followed'\n",
      "   5 â†’ 'day'\n",
      "   6 â†’ '<END>'\n",
      "   7 â†’ 'everywhere'\n",
      "   8 â†’ 'its'\n",
      "   9 â†’ 'white'\n",
      "   10 â†’ 'which'\n",
      "   11 â†’ 'snow'\n",
      "   12 â†’ 'as'\n",
      "   13 â†’ 'a'\n",
      "   14 â†’ 'it'\n",
      "   15 â†’ 'the'\n",
      "   16 â†’ 'play'\n",
      "   17 â†’ 'to'\n",
      "   18 â†’ 'made'\n",
      "   19 â†’ 'little'\n",
      "   20 â†’ 'school'\n",
      "   21 â†’ 'laugh'\n",
      "   22 â†’ 'went'\n",
      "   23 â†’ 'her'\n",
      "   24 â†’ 'see'\n",
      "   25 â†’ 'rules'\n",
      "   26 â†’ 'fleece'\n",
      "   27 â†’ 'lamb'\n",
      "   28 â†’ 'had'\n",
      "   29 â†’ 'and'\n",
      "   30 â†’ 'at'\n",
      "   31 â†’ 'go'\n",
      "   32 â†’ 'mary'\n",
      "   33 â†’ 'children'\n",
      "   34 â†’ 'sure'\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: CREATE WORD â†” NUMBER DICTIONARIES\n",
    "# ============================================================================\n",
    "# We need TWO dictionaries:\n",
    "#   1. word2idx: looks up a word, gives us its number (for ENCODING)\n",
    "#   2. idx2word: looks up a number, gives us its word (for DECODING)\n",
    "\n",
    "# Dictionary 1: Word â†’ Number (for encoding text into numbers)\n",
    "# Example: word2idx[\"mary\"] might give us 5\n",
    "word2idx = {word: idx for idx, word in enumerate(words)}\n",
    "\n",
    "# Display the two dictionaries clearly\n",
    "print(\"=\" * 60)\n",
    "print(\"DICTIONARY 1: word2idx (Word â†’ Number)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Use this to ENCODE words into numbers the AI can process\")\n",
    "print()\n",
    "for word, idx in word2idx.items():\n",
    "    print(f\"   '{word}' â†’ {idx}\")\n",
    "\n",
    "# Dictionary 2: Number â†’ Word (for decoding numbers back to text)\n",
    "# Example: idx2word[5] might give us \"mary\"\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "# Display the second dictionary clearly\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"DICTIONARY 2: idx2word (Number â†’ Word)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Use this to DECODE numbers back into readable words\")\n",
    "print()\n",
    "for idx, word in idx2word.items():\n",
    "    print(f\"   {idx} â†’ '{word}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "531ff8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "OUR TEXT CONVERTED TO NUMBERS (DATA TENSOR)\n",
      "============================================================\n",
      "Shape: torch.Size([106]) (meaning we have 106 tokens)\n",
      "\n",
      "First 20 tokens (as numbers):\n",
      "   [32, 28, 13, 19, 27, 6, 19, 27, 19, 27, 6, 32, 28, 13, 19, 27, 6, 8, 26, 3]\n",
      "\n",
      "Those same 20 tokens decoded back to words:\n",
      "   mary had a little lamb <END> little lamb little lamb <END> mary had a little lamb <END> its fleece was\n",
      "\n",
      "âœ… Perfect! We can now convert text â†’ numbers â†’ text!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 3: CONVERT OUR ENTIRE TEXT INTO NUMBERS\n",
    "# ============================================================================\n",
    "# Now we use word2idx to convert every word in our text to its number.\n",
    "# This creates a \"data tensor\" - a list of numbers representing our text.\n",
    "\n",
    "# Convert each word to its index number\n",
    "data = torch.tensor([word2idx[word] for word in text.split()], dtype=torch.long)\n",
    "\n",
    "# Let's see what we created!\n",
    "print(\"=\" * 60)\n",
    "print(\"OUR TEXT CONVERTED TO NUMBERS (DATA TENSOR)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Shape: {data.shape} (meaning we have {len(data)} tokens)\")\n",
    "print()\n",
    "\n",
    "# Show the first 20 tokens as numbers\n",
    "print(\"First 20 tokens (as numbers):\")\n",
    "print(f\"   {data[:20].tolist()}\")\n",
    "print()\n",
    "\n",
    "# Decode those numbers back to words to verify it worked!\n",
    "decoded_words = [idx2word[int(i)] for i in data[:20]]\n",
    "print(\"Those same 20 tokens decoded back to words:\")\n",
    "print(f\"   {' '.join(decoded_words)}\")\n",
    "print()\n",
    "print(\"âœ… Perfect! We can now convert text â†’ numbers â†’ text!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ba575c",
   "metadata": {},
   "source": [
    "## Part 3: Hyperparameters (The Settings That Control Our AI)\n",
    "\n",
    "**Hyperparameters** are like the settings on a video game as they control how the game (or in this case, our AI) behaves. These are values WE choose before training starts.\n",
    "\n",
    "Think of building a brain:\n",
    "- How many brain cells should we use?\n",
    "- How should they connect?\n",
    "- How fast should it learn?\n",
    "\n",
    "Here are our key settings:\n",
    "\n",
    "| Parameter | What It Means | Our Value | Simple Explanation |\n",
    "|-----------|---------------|-----------|---------------------|\n",
    "| `block_size` | Context window | 6 | AI can \"remember\" 6 words at a time |\n",
    "| `embedding_dim` | Word vector size | 32 | Each word becomes 32 numbers |\n",
    "| `n_heads` | Attention heads | 2 | AI looks at text 2 different ways |\n",
    "| `n_layers` | Transformer layers | 2 | Stack 2 processing blocks together |\n",
    "| `lr` | Learning rate | 0.001 | How big each learning step is |\n",
    "| `epochs` | Training rounds | 1500 | Train for 1500 rounds |\n",
    "\n",
    "**Analogy:** If the AI is a student:\n",
    "- `block_size` = How many words they can read at once\n",
    "- `embedding_dim` = How detailed their understanding is\n",
    "- `n_heads` = How many different perspectives they consider\n",
    "- `n_layers` = How many times they re-read and think\n",
    "- `lr` = How quickly they adjust their understanding\n",
    "- `epochs` = How many times they practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "aa490b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "HYPERPARAMETERS CONFIGURED!\n",
      "============================================================\n",
      "\n",
      "ğŸ“ MODEL ARCHITECTURE:\n",
      "   â€¢ Context window (block_size): 6 tokens\n",
      "   â€¢ Embedding dimension: 32 numbers per word\n",
      "   â€¢ Attention heads: 2\n",
      "   â€¢ Transformer layers: 2\n",
      "\n",
      "ğŸ“ TRAINING SETTINGS:\n",
      "   â€¢ Learning rate: 0.001\n",
      "   â€¢ Training epochs: 1500\n",
      "\n",
      "âœ… Ready to build our model!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MODEL ARCHITECTURE HYPERPARAMETERS\n",
    "# ============================================================================\n",
    "# These control the SIZE and STRUCTURE of our neural network\n",
    "\n",
    "# Context window: how many tokens the model can \"see\" at once\n",
    "# Like reading 6 words at a time through a small window\n",
    "block_size = 6\n",
    "\n",
    "# Embedding dimension: how many numbers represent each word\n",
    "# Bigger = more detailed representation, but slower training\n",
    "embedding_dim = 32\n",
    "\n",
    "# Number of attention heads: parallel ways to analyze relationships\n",
    "# More heads = more perspectives on the data\n",
    "n_heads = 2\n",
    "\n",
    "# Number of transformer layers: stacked processing blocks\n",
    "# More layers = deeper understanding, but more computation\n",
    "n_layers = 2\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING HYPERPARAMETERS\n",
    "# ============================================================================\n",
    "# These control HOW the model learns\n",
    "\n",
    "# Learning rate: how big each learning step is (0.001)\n",
    "# Too high = overshoots and never learns\n",
    "# Too low = takes forever to learn\n",
    "lr = 1e-3\n",
    "\n",
    "# Epochs: how many times we go through training\n",
    "# More epochs = more practice = better learning (up to a point)\n",
    "epochs = 1500\n",
    "\n",
    "# ============================================================================\n",
    "# DISPLAY ALL OUR SETTINGS\n",
    "# ============================================================================\n",
    "# Let's summarize all our hyperparameters clearly\n",
    "print(\"=\" * 60)\n",
    "print(\"HYPERPARAMETERS CONFIGURED!\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"ğŸ“ MODEL ARCHITECTURE:\")\n",
    "print(f\"   â€¢ Context window (block_size): {block_size} tokens\")\n",
    "print(f\"   â€¢ Embedding dimension: {embedding_dim} numbers per word\")\n",
    "print(f\"   â€¢ Attention heads: {n_heads}\")\n",
    "print(f\"   â€¢ Transformer layers: {n_layers}\")\n",
    "print()\n",
    "print(\"ğŸ“ TRAINING SETTINGS:\")\n",
    "print(f\"   â€¢ Learning rate: {lr}\")\n",
    "print(f\"   â€¢ Training epochs: {epochs}\")\n",
    "print()\n",
    "print(\"âœ… Ready to build our model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d0e93b",
   "metadata": {},
   "source": [
    "### 3.1 Understanding the Embedding Matrix (The Word Dictionary)\n",
    "\n",
    "**What is an embedding matrix?** It's a big table that converts words into lists of numbers. Think of it like a secret code book!\n",
    "\n",
    "**Imagine this:** Every word in our vocabulary gets its own \"fingerprint\" made of 32 numbers. These numbers describe the word in a way the computer can understand.\n",
    "\n",
    "**Visual Example - Our Embedding Matrix:**\n",
    "\n",
    "```\n",
    "                    32 Columns (one for each \"feature\")\n",
    "                    â†“   â†“   â†“   â†“   â†“   â†“       â†“\n",
    "              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "Token 0 (mary) â†’â”‚ 0.23 -0.45 0.12 0.56 ... 0.67 â”‚  â† \"mary\"'s fingerprint\n",
    "Token 1 (had)  â†’â”‚-0.11  0.89-0.34 0.21 ... 0.21 â”‚  â† \"had\"'s fingerprint\n",
    "Token 2 (a)    â†’â”‚ 0.56  0.02 0.78-0.33 ...-0.45 â”‚  â† \"a\"'s fingerprint\n",
    "Token 3 (lamb) â†’â”‚-0.22  0.33 0.44 0.11 ... 0.88 â”‚  â† \"lamb\"'s fingerprint\n",
    "      ...      â”‚  ...   ...  ...  ... ...  ... â”‚\n",
    "Token 26(<END>)â†’â”‚-0.33  0.44 0.11 0.67 ... 0.55 â”‚  â† \"<END>\"'s fingerprint\n",
    "              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                 â†‘\n",
    "                 27 Rows (one for each word in vocabulary)\n",
    "```\n",
    "\n",
    "**Key Points:**\n",
    "- **Rows** = `vocab_size` (27 words) â†’ Each row is ONE word's number-fingerprint\n",
    "- **Columns** = `embedding_dim` (32 numbers) â†’ Each column is a \"feature\" dimension\n",
    "\n",
    "**The magic:** At first, these numbers are RANDOM! But during training, the AI adjusts them so:\n",
    "- Similar words (like \"lamb\" and \"fleece\") end up with similar number patterns\n",
    "- Different words (like \"school\" and \"snow\") have different patterns\n",
    "\n",
    "**Analogy:** Imagine each word is a person, and the 32 numbers describe them:\n",
    "- Number 1 might relate to \"is it an animal?\"\n",
    "- Number 2 might relate to \"is it a verb?\"\n",
    "- Number 3 might relate to \"is it happy?\"\n",
    "- ...and so on (though we don't actually know what each number means!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "8921ba37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "OUR EMBEDDING MATRIX\n",
      "============================================================\n",
      "Shape: torch.Size([35, 32])\n",
      "  â€¢ Rows: 35 (one row per word in vocabulary)\n",
      "  â€¢ Columns: 32 (each word becomes 32 numbers)\n",
      "\n",
      "ğŸ“Š Total numbers to learn: 35 Ã— 32 = 1,120\n",
      "\n",
      "============================================================\n",
      "WORD AT ROW 0: 'one'\n",
      "============================================================\n",
      "Its 32-number embedding (currently random):\n",
      "tensor([ 0.6332, -0.0753, -1.8653,  0.8703,  1.1888, -2.7935, -0.9989,  0.4554,\n",
      "        -1.0302,  0.5497,  0.8914, -0.7058,  1.4873, -1.7920,  0.2586, -0.1602,\n",
      "        -0.9965,  0.1511,  1.9502,  1.3443,  0.2510, -0.1353,  1.7262,  0.5959,\n",
      "        -0.7752, -1.4495, -0.5191, -0.2409, -0.7671,  2.0413,  1.0716, -1.1712])\n",
      "\n",
      "============================================================\n",
      "WORD AT ROW 5: 'day'\n",
      "============================================================\n",
      "Its 32-number embedding (currently random):\n",
      "tensor([-0.3783,  0.7111, -1.7393,  0.5526, -0.3928,  1.4691,  0.0308,  0.0430,\n",
      "         0.1392,  2.3944,  0.9666, -0.2336,  1.2540,  0.1864,  0.1668,  0.0821,\n",
      "         0.0717,  0.3131, -1.2393, -0.2884,  0.0065,  0.7896, -0.5960,  0.4357,\n",
      "         1.7102,  0.5282, -0.1130,  0.6262,  0.3687, -1.7577,  0.0103, -0.9602])\n",
      "\n",
      "ğŸ’¡ These numbers are random NOW, but during training,\n",
      "   they'll adjust so similar words have similar numbers!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LET'S SEE THE EMBEDDING MATRIX IN ACTION!\n",
    "# ============================================================================\n",
    "# We'll create an embedding layer and look at what it contains.\n",
    "\n",
    "# Create an embedding layer: vocab_size rows Ã— embedding_dim columns\n",
    "sample_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "# Show the shape of our embedding matrix\n",
    "print(\"=\" * 60)\n",
    "print(\"OUR EMBEDDING MATRIX\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Shape: {sample_embedding.weight.shape}\")\n",
    "print(f\"  â€¢ Rows: {vocab_size} (one row per word in vocabulary)\")\n",
    "print(f\"  â€¢ Columns: {embedding_dim} (each word becomes 32 numbers)\")\n",
    "print()\n",
    "print(f\"ğŸ“Š Total numbers to learn: {vocab_size} Ã— {embedding_dim} = {vocab_size * embedding_dim:,}\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# LOOK UP SPECIFIC WORDS IN THE EMBEDDING MATRIX\n",
    "# ============================================================================\n",
    "# Let's see what \"fingerprint\" each word has!\n",
    "\n",
    "# Token 0's word\n",
    "token_0_word = idx2word[0]\n",
    "print(\"=\" * 60)\n",
    "print(f\"WORD AT ROW 0: '{token_0_word}'\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Its 32-number embedding (currently random):\")\n",
    "print(sample_embedding.weight[0].data)\n",
    "print()\n",
    "\n",
    "# Token 5's word\n",
    "token_5_word = idx2word[5]\n",
    "print(\"=\" * 60)\n",
    "print(f\"WORD AT ROW 5: '{token_5_word}'\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Its 32-number embedding (currently random):\")\n",
    "print(sample_embedding.weight[5].data)\n",
    "print()\n",
    "\n",
    "# The embeddings are random at first\n",
    "print(\"ğŸ’¡ These numbers are random NOW, but during training,\")\n",
    "print(\"   they'll adjust so similar words have similar numbers!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b088ced3",
   "metadata": {},
   "source": [
    "### 3.1.1 Neural Network Fundamentals: A Quick Primer\n",
    "\n",
    "Before we go further, let's understand the **basic building blocks of neural networks**. This is super important for understanding how GPT learns!\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  What is a Neuron?\n",
    "\n",
    "A **neuron** is the smallest unit of a brain (real or artificial). In AI, it's a simple math formula.\n",
    "\n",
    "A single neuron can take **multiple inputs** (like 2, 3, or even 1000!), but it always produces **one output**.\n",
    "\n",
    "```\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚                    A SINGLE NEURON                          â”‚\n",
    "        â”‚                                                             â”‚\n",
    "        â”‚   2 INPUTS           WEIGHTS        PROCESSING              â”‚\n",
    "        â”‚      â”‚                  â”‚               â”‚                   â”‚\n",
    "        â”‚      â–¼                  â–¼               â–¼                   â”‚\n",
    "        â”‚                                                             â”‚\n",
    "        â”‚   â”Œâ”€â”€â”€â”                             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚\n",
    "        â”‚   â”‚0.8â”‚â”€â”€â”€â”€â”€ Ã— 0.6 â”€â”€â”€â”€â”€â”€â”          â”‚         â”‚             â”‚\n",
    "        â”‚   â””â”€â”€â”€â”˜                  â”‚          â”‚         â”‚             â”‚\n",
    "        â”‚   Input 1                â–¼          â”‚         â”‚   â”Œâ”€â”€â”€â”€â”€â”   â”‚\n",
    "        â”‚                       â”Œâ”€â”€â”€â”€â”€â”       â”‚  ReLU   â”‚â”€â”€â–ºâ”‚ 0.66â”‚   â”‚\n",
    "        â”‚   â”Œâ”€â”€â”€â”               â”‚ SUM â”‚â”€â”€â”€â”€â”€â”€â–ºâ”‚         â”‚   â””â”€â”€â”€â”€â”€â”˜   â”‚\n",
    "        â”‚   â”‚0.2â”‚â”€â”€â”€â”€â”€ Ã— 0.4 â”€â”€â–ºâ”‚     â”‚       â”‚         â”‚   Output!   â”‚\n",
    "        â”‚   â””â”€â”€â”€â”˜               â”‚     â”‚       â”‚         â”‚             â”‚\n",
    "        â”‚   Input 2             â”‚ +0.1â”‚â†bias  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚\n",
    "        â”‚                       â””â”€â”€â”€â”€â”€â”˜                               â”‚\n",
    "        â”‚                          â–²                                  â”‚\n",
    "        â”‚                          â”‚                                  â”‚\n",
    "        â”‚                   Bias is added                             â”‚\n",
    "        â”‚                   INSIDE the neuron                         â”‚\n",
    "        â”‚                   (not a 3rd input!)                        â”‚\n",
    "        â”‚                                                             â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "The math: (0.8 Ã— 0.6) + (0.2 Ã— 0.4) + 0.1 = 0.48 + 0.08 + 0.1 = 0.66\n",
    "         \\_________/   \\_________/   \\_/\n",
    "          Input 1       Input 2     Bias\n",
    "          weighted      weighted    (constant)\n",
    "```\n",
    "\n",
    "**Key Point:** The neuron has 2 inputs, but the bias is NOT a third input!\n",
    "- **Inputs** come from outside (data from other neurons or raw data)\n",
    "- **Bias** is a constant stored INSIDE the neuron (like its personal adjustment)\n",
    "\n",
    "---\n",
    "\n",
    "**Parts of a Neuron:**\n",
    "\n",
    "| Part | What It Is | Analogy |\n",
    "|------|-----------|---------|\n",
    "| **Inputs (x)** | Numbers fed into the neuron | Questions on a test |\n",
    "| **Weights (w)** | How important each input is | How much each question is worth |\n",
    "| **Bias (b)** | A constant adjustment inside the neuron | Bonus points on the test |\n",
    "| **Activation** | A special function applied at the end | The grading curve |\n",
    "\n",
    "**The Formula:**\n",
    "$$\\text{output} = \\text{activation}(w_1 \\cdot x_1 + w_2 \\cdot x_2 + b)$$\n",
    "\n",
    "**Why the activation function?** Without it, the neuron is just basic multiplication and addition. The activation function adds \"curves\" that let the network learn complex patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "22b3115d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXAMPLE: A Single Neuron Predicting Movie Enjoyment\n",
      "============================================================\n",
      "\n",
      "INPUTS (what we know about the movie):\n",
      "  xâ‚ = 0.8 (action score - high means lots of action)\n",
      "  xâ‚‚ = 0.2 (romance score - low means little romance)\n",
      "\n",
      "WEIGHTS (how important each feature is):\n",
      "  wâ‚ = 0.6 (positive: this person LIKES action)\n",
      "  wâ‚‚ = -0.4 (negative: this person DISLIKES romance)\n",
      "\n",
      "BIAS (starting tendency):\n",
      "  b = 0.1 (slightly positive: they generally like movies)\n",
      "\n",
      "============================================================\n",
      "THE MATH: Weighted Sum\n",
      "============================================================\n",
      "\n",
      "Formula: weighted_sum = wâ‚Â·xâ‚ + wâ‚‚Â·xâ‚‚ + b\n",
      "\n",
      "  = (0.6) Ã— (0.8) + (-0.4) Ã— (0.2) + 0.1\n",
      "  = 0.48 + -0.08000000000000002 + 0.1\n",
      "  = 0.5\n",
      "\n",
      "============================================================\n",
      "THE MATH: Activation Function (ReLU)\n",
      "============================================================\n",
      "\n",
      "ReLU means: if negative â†’ 0, if positive â†’ keep it\n",
      "  ReLU(0.5) = max(0, 0.5) = 0.5\n",
      "\n",
      "============================================================\n",
      "ğŸ¯ FINAL OUTPUT: 0.50\n",
      "============================================================\n",
      "\n",
      "Interpretation: They'll probably LOVE this movie! ğŸ¬â¤ï¸\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EXAMPLE: Building a Single Neuron from Scratch\n",
    "# ============================================================================\n",
    "# Let's predict if someone will like a movie based on two scores!\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EXAMPLE: A Single Neuron Predicting Movie Enjoyment\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# --- STEP 1: Define our inputs ---\n",
    "\n",
    "# These are the two features about the movie (scale 0-1)\n",
    "x1 = 0.8  # Action score: 0.8 means 80% action-packed\n",
    "x2 = 0.2  # Romance score: 0.2 means only 20% romance\n",
    "\n",
    "print(\"INPUTS (what we know about the movie):\")\n",
    "print(f\"  xâ‚ = {x1} (action score - high means lots of action)\")\n",
    "print(f\"  xâ‚‚ = {x2} (romance score - low means little romance)\")\n",
    "print()\n",
    "\n",
    "# --- STEP 2: Define our weights (learnable!) ---\n",
    "\n",
    "# These represent what the viewer likes (positive = likes, negative = dislikes)\n",
    "w1 = 0.6   # Weight for action: positive = they like action\n",
    "w2 = -0.4  # Weight for romance: NEGATIVE = they dislike romance!\n",
    "\n",
    "print(\"WEIGHTS (how important each feature is):\")\n",
    "print(f\"  wâ‚ = {w1} (positive: this person LIKES action)\")\n",
    "print(f\"  wâ‚‚ = {w2} (negative: this person DISLIKES romance)\")\n",
    "print()\n",
    "\n",
    "# --- STEP 3: Define the bias (starting point) ---\n",
    "\n",
    "# This is the starting tendency before considering inputs\n",
    "b = 0.1  # They generally like movies (+0.1 bonus)\n",
    "\n",
    "print(\"BIAS (starting tendency):\")\n",
    "print(f\"  b = {b} (slightly positive: they generally like movies)\")\n",
    "print()\n",
    "\n",
    "# --- STEP 4: Calculate the weighted sum ---\n",
    "\n",
    "# Formula: wâ‚Â·xâ‚ + wâ‚‚Â·xâ‚‚ + b\n",
    "weighted_sum = (w1 * x1) + (w2 * x2) + b\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"THE MATH: Weighted Sum\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"Formula: weighted_sum = wâ‚Â·xâ‚ + wâ‚‚Â·xâ‚‚ + b\")\n",
    "print()\n",
    "print(f\"  = ({w1}) Ã— ({x1}) + ({w2}) Ã— ({x2}) + {b}\")\n",
    "print(f\"  = {w1*x1} + {w2*x2} + {b}\")\n",
    "print(f\"  = {weighted_sum}\")\n",
    "print()\n",
    "\n",
    "# --- STEP 5: Apply activation function (ReLU) ---\n",
    "\n",
    "# ReLU = \"Rectified Linear Unit\" = max(0, x)\n",
    "# If negative, make it 0. Otherwise, keep it.\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"ReLU activation: returns max(0, x).\"\"\"\n",
    "    return max(0, x)\n",
    "\n",
    "# Apply ReLU to the weighted sum\n",
    "output = relu(weighted_sum)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"THE MATH: Activation Function (ReLU)\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"ReLU means: if negative â†’ 0, if positive â†’ keep it\")\n",
    "print(f\"  ReLU({weighted_sum}) = max(0, {weighted_sum}) = {output}\")\n",
    "print()\n",
    "\n",
    "# --- FINAL RESULT ---\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"ğŸ¯ FINAL OUTPUT: {output:.2f}\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "if output > 0.4:\n",
    "    print(\"Interpretation: They'll probably LOVE this movie! ğŸ¬â¤ï¸\")\n",
    "elif output > 0.2:\n",
    "    print(\"Interpretation: They'll probably enjoy this movie! ğŸ¬ğŸ‘\")\n",
    "else:\n",
    "    print(\"Interpretation: They might not enjoy this movie ğŸ¬ğŸ˜\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3a05e3",
   "metadata": {},
   "source": [
    "### 3.1.2 A Simple Neural Network (Connecting Neurons Together)\n",
    "\n",
    "One neuron isn't very smart. But when we **connect many neurons together**, they can learn amazing things!\n",
    "\n",
    "Here's a tiny neural network with **3 layers**:\n",
    "\n",
    "```\n",
    "                    SIMPLE NEURAL NETWORK\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚                                                              â”‚\n",
    "    â”‚   INPUT LAYER      HIDDEN LAYER         OUTPUT LAYER         â”‚\n",
    "    â”‚   (what we         (does the            (gives us            â”‚\n",
    "    â”‚    know)           thinking)            the answer)          â”‚\n",
    "    â”‚                                                              â”‚\n",
    "    â”‚       xâ‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º[hâ‚]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚\n",
    "    â”‚        â”‚ â•²        â•±    â•²             â•²                       â”‚\n",
    "    â”‚        â”‚   â•²    â•±        â•²            â•²                      â”‚\n",
    "    â”‚        â”‚     â•³            â•²            â–º[out]â”€â”€â”€â”€â–º Å·         â”‚\n",
    "    â”‚        â”‚   â•±    â•²          â•²          â•±  (answer)            â”‚\n",
    "    â”‚        â”‚ â•±        â•²         â•²        â•±                       â”‚\n",
    "    â”‚       xâ‚‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º[hâ‚‚]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚\n",
    "    â”‚                                                              â”‚\n",
    "    â”‚   (2 values)     (2 neurons)        (1 neuron)               â”‚\n",
    "    â”‚                                                              â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**What's happening:**\n",
    "1. **Input Layer**: We feed in 2 numbers (like action score and romance score)\n",
    "2. **Hidden Layer**: 2 neurons process those numbers (the \"thinking\" happens here)\n",
    "3. **Output Layer**: 1 neuron combines everything into a final answer\n",
    "\n",
    "**Each arrow = a WEIGHT (a number the AI can change)**\n",
    "\n",
    "**Counting the learnable parameters:**\n",
    "| Connection | Calculation | Result |\n",
    "|-----------|-------------|--------|\n",
    "| Inputs â†’ Hidden | 2 inputs Ã— 2 neurons = 4 weights | 4 weights |\n",
    "| Hidden biases | 2 neurons Ã— 1 bias each | 2 biases |\n",
    "| Hidden â†’ Output | 2 neurons Ã— 1 output = 2 weights | 2 weights |\n",
    "| Output bias | 1 neuron Ã— 1 bias | 1 bias |\n",
    "| **TOTAL** | | **9 learnable parameters** |\n",
    "\n",
    "**Why \"hidden\"?** We can see the inputs and outputs, but the hidden layer is like the brain's inner workings - hidden from view!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "9fd19a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "BUILDING A SIMPLE NEURAL NETWORK FROM SCRATCH\n",
      "======================================================================\n",
      "\n",
      "ğŸ“¥ STEP 1: OUR INPUTS\n",
      "   xâ‚ = 0.8 (action score)\n",
      "   xâ‚‚ = 0.2 (romance score)\n",
      "\n",
      "ğŸ”· STEP 2: HIDDEN LAYER PARAMETERS (6 total)\n",
      "   Neuron hâ‚: wâ‚=0.5, wâ‚‚=0.3, bias=0.1\n",
      "   Neuron hâ‚‚: wâ‚=-0.2, wâ‚‚=0.8, bias=-0.1\n",
      "\n",
      "ğŸ”· STEP 3: OUTPUT LAYER PARAMETERS (3 total)\n",
      "   Output neuron: w_h1=0.7, w_h2=0.4, bias=0.05\n",
      "\n",
      "ğŸ“Š TOTAL LEARNABLE PARAMETERS: 6 + 3 = 9\n",
      "\n",
      "======================================================================\n",
      "FORWARD PASS: Computing the Output\n",
      "======================================================================\n",
      "\n",
      "ğŸ§® HIDDEN NEURON 1 (hâ‚):\n",
      "   Before ReLU (z_h1):\n",
      "   = (wâ‚ Ã— xâ‚) + (wâ‚‚ Ã— xâ‚‚) + bias\n",
      "   = (0.5 Ã— 0.8) + (0.3 Ã— 0.2) + 0.1\n",
      "   = 0.4 + 0.06 + 0.1\n",
      "   = 0.56\n",
      "   After ReLU: hâ‚ = max(0, 0.56) = 0.56\n",
      "\n",
      "ğŸ§® HIDDEN NEURON 2 (hâ‚‚):\n",
      "   Before ReLU (z_h2):\n",
      "   = (wâ‚ Ã— xâ‚) + (wâ‚‚ Ã— xâ‚‚) + bias\n",
      "   = (-0.2 Ã— 0.8) + (0.8 Ã— 0.2) + -0.1\n",
      "   = -0.16000000000000003 + 0.16000000000000003 + -0.1\n",
      "   = -0.1\n",
      "   After ReLU: hâ‚‚ = max(0, -0.1) = 0\n",
      "\n",
      "ğŸ§® OUTPUT NEURON:\n",
      "   Before ReLU (z_out):\n",
      "   = (w_h1 Ã— hâ‚) + (w_h2 Ã— hâ‚‚) + bias\n",
      "   = (0.7 Ã— 0.56) + (0.4 Ã— 0) + 0.05\n",
      "   = 0.3920 + 0.0000 + 0.05\n",
      "   = 0.4420\n",
      "   After ReLU: output = max(0, 0.4420) = 0.4420\n",
      "\n",
      "======================================================================\n",
      "ğŸ¯ FINAL PREDICTION: 0.4420\n",
      "======================================================================\n",
      "\n",
      "ğŸ’¡ This number is the network's answer!\n",
      "   In the next section, we'll see how to measure if it's right or wrong.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# BUILDING A SIMPLE NEURAL NETWORK FROM SCRATCH\n",
    "# ============================================================================\n",
    "# Let's build the 3-layer network from the diagram above!\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"BUILDING A SIMPLE NEURAL NETWORK FROM SCRATCH\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# --- STEP 1: Define our inputs ---\n",
    "\n",
    "# Same movie prediction example: action score and romance score\n",
    "x1 = 0.8  # Action score (80% action)\n",
    "x2 = 0.2  # Romance score (20% romance)\n",
    "\n",
    "print(\"ğŸ“¥ STEP 1: OUR INPUTS\")\n",
    "print(f\"   xâ‚ = {x1} (action score)\")\n",
    "print(f\"   xâ‚‚ = {x2} (romance score)\")\n",
    "print()\n",
    "\n",
    "# --- STEP 2: Define hidden layer weights and biases ---\n",
    "\n",
    "# Weights going TO hidden neuron 1 (hâ‚)\n",
    "w_h1_x1 = 0.5   # How much xâ‚ affects hâ‚\n",
    "w_h1_x2 = 0.3   # How much xâ‚‚ affects hâ‚\n",
    "b_h1 = 0.1      # Bias for hâ‚\n",
    "\n",
    "# Weights going TO hidden neuron 2 (hâ‚‚)\n",
    "w_h2_x1 = -0.2  # How much xâ‚ affects hâ‚‚ (negative = opposite effect!)\n",
    "w_h2_x2 = 0.8   # How much xâ‚‚ affects hâ‚‚\n",
    "b_h2 = -0.1     # Bias for hâ‚‚\n",
    "\n",
    "print(\"ğŸ”· STEP 2: HIDDEN LAYER PARAMETERS (6 total)\")\n",
    "print(f\"   Neuron hâ‚: wâ‚={w_h1_x1}, wâ‚‚={w_h1_x2}, bias={b_h1}\")\n",
    "print(f\"   Neuron hâ‚‚: wâ‚={w_h2_x1}, wâ‚‚={w_h2_x2}, bias={b_h2}\")\n",
    "print()\n",
    "\n",
    "# --- STEP 3: Define output layer weights and biases ---\n",
    "\n",
    "# Weights going TO output neuron\n",
    "w_out_h1 = 0.7   # How much hâ‚ affects the output\n",
    "w_out_h2 = 0.4   # How much hâ‚‚ affects the output\n",
    "b_out = 0.05     # Bias for the output\n",
    "\n",
    "print(\"ğŸ”· STEP 3: OUTPUT LAYER PARAMETERS (3 total)\")\n",
    "print(f\"   Output neuron: w_h1={w_out_h1}, w_h2={w_out_h2}, bias={b_out}\")\n",
    "print()\n",
    "print(f\"ğŸ“Š TOTAL LEARNABLE PARAMETERS: 6 + 3 = 9\")\n",
    "print()\n",
    "\n",
    "# --- STEP 4: Forward pass - hidden layer ---\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FORWARD PASS: Computing the Output\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Compute hidden neuron 1's value\n",
    "z_h1 = (w_h1_x1 * x1) + (w_h1_x2 * x2) + b_h1\n",
    "\n",
    "print(\"ğŸ§® HIDDEN NEURON 1 (hâ‚):\")\n",
    "print(f\"   Before ReLU (z_h1):\")\n",
    "print(f\"   = (wâ‚ Ã— xâ‚) + (wâ‚‚ Ã— xâ‚‚) + bias\")\n",
    "print(f\"   = ({w_h1_x1} Ã— {x1}) + ({w_h1_x2} Ã— {x2}) + {b_h1}\")\n",
    "print(f\"   = {w_h1_x1*x1} + {w_h1_x2*x2} + {b_h1}\")\n",
    "print(f\"   = {z_h1}\")\n",
    "\n",
    "# Apply ReLU\n",
    "h1 = max(0, z_h1)\n",
    "print(f\"   After ReLU: hâ‚ = max(0, {z_h1}) = {h1}\")\n",
    "print()\n",
    "\n",
    "# Compute hidden neuron 2's value\n",
    "z_h2 = (w_h2_x1 * x1) + (w_h2_x2 * x2) + b_h2\n",
    "\n",
    "print(\"ğŸ§® HIDDEN NEURON 2 (hâ‚‚):\")\n",
    "print(f\"   Before ReLU (z_h2):\")\n",
    "print(f\"   = (wâ‚ Ã— xâ‚) + (wâ‚‚ Ã— xâ‚‚) + bias\")\n",
    "print(f\"   = ({w_h2_x1} Ã— {x1}) + ({w_h2_x2} Ã— {x2}) + {b_h2}\")\n",
    "print(f\"   = {w_h2_x1*x1} + {w_h2_x2*x2} + {b_h2}\")\n",
    "print(f\"   = {z_h2}\")\n",
    "\n",
    "# Apply ReLU\n",
    "h2 = max(0, z_h2)\n",
    "print(f\"   After ReLU: hâ‚‚ = max(0, {z_h2}) = {h2}\")\n",
    "print()\n",
    "\n",
    "# --- STEP 5: Forward pass - output layer ---\n",
    "\n",
    "# Compute output neuron's value\n",
    "z_out = (w_out_h1 * h1) + (w_out_h2 * h2) + b_out\n",
    "\n",
    "print(\"ğŸ§® OUTPUT NEURON:\")\n",
    "print(f\"   Before ReLU (z_out):\")\n",
    "print(f\"   = (w_h1 Ã— hâ‚) + (w_h2 Ã— hâ‚‚) + bias\")\n",
    "print(f\"   = ({w_out_h1} Ã— {h1}) + ({w_out_h2} Ã— {h2}) + {b_out}\")\n",
    "print(f\"   = {w_out_h1*h1:.4f} + {w_out_h2*h2:.4f} + {b_out}\")\n",
    "print(f\"   = {z_out:.4f}\")\n",
    "\n",
    "# Apply ReLU to get final output\n",
    "output = max(0, z_out)\n",
    "print(f\"   After ReLU: output = max(0, {z_out:.4f}) = {output:.4f}\")\n",
    "print()\n",
    "\n",
    "# --- FINAL RESULT ---\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"ğŸ¯ FINAL PREDICTION: {output:.4f}\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"ğŸ’¡ This number is the network's answer!\")\n",
    "print(\"   In the next section, we'll see how to measure if it's right or wrong.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d35317e",
   "metadata": {},
   "source": [
    "### 3.1.3 The Loss Function: Measuring How Wrong We Are\n",
    "\n",
    "Our network made a prediction. But how do we know if it's **good or bad**?\n",
    "\n",
    "We use a **loss function** - a formula that measures the **difference between our prediction and the correct answer**.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ Think of it Like a Test Score\n",
    "\n",
    "| What We Want | What We Got | How Wrong? |\n",
    "|-------------|-------------|------------|\n",
    "| 100 points | 100 points | 0 (perfect!) |\n",
    "| 100 points | 90 points | 10 (pretty good) |\n",
    "| 100 points | 50 points | 50 (needs work) |\n",
    "| 100 points | 0 points | 100 (very wrong) |\n",
    "\n",
    "**The loss is like the \"wrongness score\"** - we want it to be as LOW as possible!\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§® Mean Squared Error (MSE)\n",
    "\n",
    "The most common loss function is **Mean Squared Error**:\n",
    "\n",
    "$$\\text{Loss} = (\\text{true value} - \\text{prediction})^2$$\n",
    "\n",
    "**Why square it?**\n",
    "1. Makes all errors positive (no negative wrongness!)\n",
    "2. Punishes big errors more than small ones (being off by 10 is WAY worse than being off by 2)\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "True value (what we wanted):     y = 1.0\n",
    "Our prediction:                  Å· = 0.6\n",
    "                                    \n",
    "Error:                           1.0 - 0.6 = 0.4\n",
    "Squared Error (Loss):            0.4Â² = 0.16\n",
    "```\n",
    "\n",
    "**Our goal during training:** Adjust the weights to make this loss number SMALLER!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "3bde2935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "COMPUTING THE LOSS (How Wrong Are We?)\n",
      "======================================================================\n",
      "\n",
      "ğŸ“Š COMPARING PREDICTION TO TRUTH:\n",
      "   What we wanted (y_true): 1.0\n",
      "   What we got (y_pred):    0.5560\n",
      "   Difference:              0.4440\n",
      "\n",
      "ğŸ§® LOSS CALCULATION (Mean Squared Error):\n",
      "\n",
      "   Step 1: Find the difference\n",
      "           error = y_true - y_pred\n",
      "                 = 1.0 - 0.5560\n",
      "                 = 0.4440\n",
      "\n",
      "   Step 2: Square it\n",
      "           loss = errorÂ²\n",
      "                = (0.4440)Â²\n",
      "                = 0.1971\n",
      "\n",
      "======================================================================\n",
      "ğŸ“‰ FINAL LOSS: 0.1971\n",
      "======================================================================\n",
      "\n",
      "ğŸ’¡ What this means:\n",
      "   â€¢ Our prediction was off by 0.4440\n",
      "   â€¢ The squared error (loss) is 0.1971\n",
      "\n",
      "ğŸ¯ OUR GOAL: Adjust the weights to make this loss SMALLER!\n",
      "   â†’ Smaller loss = better predictions\n",
      "   â†’ The next section shows HOW to adjust the weights!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# COMPUTING THE LOSS: How Wrong Is Our Network?\n",
    "# ============================================================================\n",
    "# Let's measure how far off our prediction is from what we wanted.\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"COMPUTING THE LOSS (How Wrong Are We?)\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# --- The prediction and the truth ---\n",
    "\n",
    "# Our network predicted this value (from the forward pass above)\n",
    "y_pred = 0.5560  # What our network outputted\n",
    "\n",
    "# This is what we WANTED the network to say (the \"ground truth\")\n",
    "y_true = 1.0  # We wanted it to predict \"definitely will like the movie\"\n",
    "\n",
    "print(\"ğŸ“Š COMPARING PREDICTION TO TRUTH:\")\n",
    "print(f\"   What we wanted (y_true): {y_true}\")\n",
    "print(f\"   What we got (y_pred):    {y_pred:.4f}\")\n",
    "print(f\"   Difference:              {y_true - y_pred:.4f}\")\n",
    "print()\n",
    "\n",
    "# --- Step-by-step loss calculation ---\n",
    "\n",
    "# Using Mean Squared Error: Loss = (y_true - y_pred)Â²\n",
    "\n",
    "# Step 1: Find the difference (error)\n",
    "error = y_true - y_pred\n",
    "\n",
    "# Step 2: Square it (makes it positive and punishes big errors)\n",
    "loss = error ** 2\n",
    "\n",
    "print(\"ğŸ§® LOSS CALCULATION (Mean Squared Error):\")\n",
    "print()\n",
    "print(\"   Step 1: Find the difference\")\n",
    "print(f\"           error = y_true - y_pred\")\n",
    "print(f\"                 = {y_true} - {y_pred:.4f}\")\n",
    "print(f\"                 = {error:.4f}\")\n",
    "print()\n",
    "print(\"   Step 2: Square it\")\n",
    "print(f\"           loss = errorÂ²\")\n",
    "print(f\"                = ({error:.4f})Â²\")\n",
    "print(f\"                = {loss:.4f}\")\n",
    "print()\n",
    "\n",
    "# --- What does this loss mean? ---\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"ğŸ“‰ FINAL LOSS: {loss:.4f}\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"ğŸ’¡ What this means:\")\n",
    "print(f\"   â€¢ Our prediction was off by {abs(error):.4f}\")\n",
    "print(f\"   â€¢ The squared error (loss) is {loss:.4f}\")\n",
    "print()\n",
    "print(\"ğŸ¯ OUR GOAL: Adjust the weights to make this loss SMALLER!\")\n",
    "print(\"   â†’ Smaller loss = better predictions\")\n",
    "print(\"   â†’ The next section shows HOW to adjust the weights!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8863ab",
   "metadata": {},
   "source": [
    "### 3.1.4 Backpropagation: How the Network Learns\n",
    "\n",
    "Now the big question: **How do we adjust the weights to make the loss smaller?**\n",
    "\n",
    "The answer is **backpropagation** (short for \"backward propagation of errors\").\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ The Key Idea\n",
    "\n",
    "Imagine you're shooting arrows at a target:\n",
    "- If you miss to the LEFT, you adjust your aim to the RIGHT\n",
    "- If you miss to the RIGHT, you adjust your aim to the LEFT\n",
    "- The AMOUNT you adjust depends on HOW FAR you missed\n",
    "\n",
    "Neural networks do the same thing with math!\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ The Chain Rule (Don't Panic!)\n",
    "\n",
    "The **chain rule** is a rule from calculus that lets us figure out how much each weight contributed to the error.\n",
    "\n",
    "**Simple analogy:** If you're baking cookies and they taste bad:\n",
    "- Was it too much sugar? Or too little flour? Or wrong temperature?\n",
    "- The chain rule helps us figure out WHICH ingredient to adjust and by HOW MUCH\n",
    "\n",
    "**The Math (simplified):**\n",
    "$$\\frac{\\partial \\text{Loss}}{\\partial \\text{weight}} = \\text{how much this weight affected the loss}$$\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”„ Forward vs Backward Pass\n",
    "\n",
    "```\n",
    "FORWARD PASS (computing the output):\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º\n",
    "Input â†’ Weights â†’ Hidden â†’ Weights â†’ Output â†’ Loss\n",
    "                                                 â”‚\n",
    "                                                 â”‚\n",
    "BACKWARD PASS (computing gradients):             â”‚\n",
    "â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "âˆ‚L/âˆ‚wâ‚ â† âˆ‚L/âˆ‚hâ‚ â† âˆ‚L/âˆ‚out â† âˆ‚L/âˆ‚Å· â† Loss\n",
    "\n",
    "First we go FORWARD to get the prediction.\n",
    "Then we go BACKWARD to figure out how to fix it!\n",
    "```\n",
    "\n",
    "**The gradient (âˆ‚L/âˆ‚w) tells us:**\n",
    "- **Sign**: Should we increase or decrease this weight?\n",
    "- **Magnitude**: By how much?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "9fd2afcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "BACKPROPAGATION: Computing Gradients Step by Step\n",
      "======================================================================\n",
      "\n",
      "ğŸ“Š VALUES FROM FORWARD PASS:\n",
      "   y_true (wanted):  1.0\n",
      "   y_pred (got):     0.556\n",
      "   h1 (hidden 1):    0.56\n",
      "   h2 (hidden 2):    0.0\n",
      "\n",
      "======================================================================\n",
      "STEP 1: Derivative of Loss with respect to Prediction\n",
      "======================================================================\n",
      "\n",
      "Our loss function is: L = (y_true - y_pred)Â²\n",
      "\n",
      "Using calculus (power rule + chain rule):\n",
      "   âˆ‚L/âˆ‚y_pred = -2 Ã— (y_true - y_pred)\n",
      "\n",
      "Let's plug in our numbers:\n",
      "   âˆ‚L/âˆ‚y_pred = -2 Ã— (1.0 - 0.556)\n",
      "             = -2 Ã— 0.4440\n",
      "             = -0.8880\n",
      "\n",
      "======================================================================\n",
      "STEP 2: Derivative through the ReLU Activation\n",
      "======================================================================\n",
      "\n",
      "ReLU activation:\n",
      "   â€¢ If input > 0: output = input (derivative = 1)\n",
      "   â€¢ If input â‰¤ 0: output = 0 (derivative = 0)\n",
      "\n",
      "Since z_out was positive, ReLU derivative = 1\n",
      "   âˆ‚y_pred/âˆ‚z_out = 1.0\n",
      "\n",
      "======================================================================\n",
      "STEP 3: Derivative of z_out with respect to weight w_out_h1\n",
      "======================================================================\n",
      "\n",
      "The formula for z_out is:\n",
      "   z_out = w_out_h1 Ã— h1 + w_out_h2 Ã— h2 + bias\n",
      "\n",
      "Taking derivative with respect to w_out_h1:\n",
      "   âˆ‚z_out/âˆ‚w_out_h1 = h1  (because h1 is multiplied by w_out_h1)\n",
      "\n",
      "   âˆ‚z_out/âˆ‚w_out_h1 = 0.56\n",
      "\n",
      "======================================================================\n",
      "STEP 4: THE CHAIN RULE - Multiply Everything Together!\n",
      "======================================================================\n",
      "\n",
      "The chain rule says:\n",
      "   âˆ‚L/âˆ‚w = (âˆ‚L/âˆ‚y_pred) Ã— (âˆ‚y_pred/âˆ‚z_out) Ã— (âˆ‚z_out/âˆ‚w)\n",
      "\n",
      "Calculation:\n",
      "   âˆ‚L/âˆ‚w_out_h1 = (-0.8880) Ã— (1.0) Ã— (0.56)\n",
      "               = -0.4973\n",
      "\n",
      "======================================================================\n",
      "ğŸ¯ GRADIENT for w_out_h1: -0.4973\n",
      "======================================================================\n",
      "\n",
      "What does this mean?\n",
      "   â€¢ The gradient is NEGATIVE (-0.4973)\n",
      "   â€¢ Negative gradient means: increasing this weight would DECREASE the loss\n",
      "   â€¢ So we SHOULD increase this weight!\n",
      "\n",
      "ğŸ’¡ Remember: We want to go DOWNHILL (toward lower loss)\n",
      "   The gradient points uphill, so we go the OPPOSITE direction!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# BACKPROPAGATION: Computing Gradients Step by Step\n",
    "# ============================================================================\n",
    "# Let's figure out how to adjust each weight to reduce the loss!\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"BACKPROPAGATION: Computing Gradients Step by Step\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# --- Recap: Values from our forward pass ---\n",
    "\n",
    "y_true = 1.0      # What we WANTED\n",
    "y_pred = 0.5560   # What we ACTUALLY got\n",
    "h1 = 0.56         # Hidden neuron 1's output (after ReLU)\n",
    "h2 = 0.0          # Hidden neuron 2's output (ReLU made it 0)\n",
    "x1, x2 = 0.8, 0.2 # Our original inputs\n",
    "w_out_h1 = 0.7    # Weight we want to update\n",
    "\n",
    "print(\"ğŸ“Š VALUES FROM FORWARD PASS:\")\n",
    "print(f\"   y_true (wanted):  {y_true}\")\n",
    "print(f\"   y_pred (got):     {y_pred}\")\n",
    "print(f\"   h1 (hidden 1):    {h1}\")\n",
    "print(f\"   h2 (hidden 2):    {h2}\")\n",
    "print()\n",
    "\n",
    "# --- STEP 1: How does loss change when prediction changes? ---\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STEP 1: Derivative of Loss with respect to Prediction\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"Our loss function is: L = (y_true - y_pred)Â²\")\n",
    "print()\n",
    "print(\"Using calculus (power rule + chain rule):\")\n",
    "print(\"   âˆ‚L/âˆ‚y_pred = -2 Ã— (y_true - y_pred)\")\n",
    "print()\n",
    "\n",
    "# Calculate the derivative\n",
    "dL_dy_pred = -2 * (y_true - y_pred)\n",
    "\n",
    "print(\"Let's plug in our numbers:\")\n",
    "print(f\"   âˆ‚L/âˆ‚y_pred = -2 Ã— ({y_true} - {y_pred})\")\n",
    "print(f\"             = -2 Ã— {y_true - y_pred:.4f}\")\n",
    "print(f\"             = {dL_dy_pred:.4f}\")\n",
    "print()\n",
    "\n",
    "# --- STEP 2: How does prediction change when z_out changes? ---\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STEP 2: Derivative through the ReLU Activation\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"ReLU activation:\")\n",
    "print(\"   â€¢ If input > 0: output = input (derivative = 1)\")\n",
    "print(\"   â€¢ If input â‰¤ 0: output = 0 (derivative = 0)\")\n",
    "print()\n",
    "print(\"Since z_out was positive, ReLU derivative = 1\")\n",
    "\n",
    "dy_dz = 1.0\n",
    "print(f\"   âˆ‚y_pred/âˆ‚z_out = {dy_dz}\")\n",
    "print()\n",
    "\n",
    "# --- STEP 3: How does z_out change when weight changes? ---\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STEP 3: Derivative of z_out with respect to weight w_out_h1\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"The formula for z_out is:\")\n",
    "print(\"   z_out = w_out_h1 Ã— h1 + w_out_h2 Ã— h2 + bias\")\n",
    "print()\n",
    "print(\"Taking derivative with respect to w_out_h1:\")\n",
    "print(\"   âˆ‚z_out/âˆ‚w_out_h1 = h1  (because h1 is multiplied by w_out_h1)\")\n",
    "print()\n",
    "\n",
    "dz_dw_h1 = h1\n",
    "print(f\"   âˆ‚z_out/âˆ‚w_out_h1 = {dz_dw_h1}\")\n",
    "print()\n",
    "\n",
    "# --- STEP 4: Chain rule - multiply all derivatives! ---\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STEP 4: THE CHAIN RULE - Multiply Everything Together!\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"The chain rule says:\")\n",
    "print(\"   âˆ‚L/âˆ‚w = (âˆ‚L/âˆ‚y_pred) Ã— (âˆ‚y_pred/âˆ‚z_out) Ã— (âˆ‚z_out/âˆ‚w)\")\n",
    "print()\n",
    "\n",
    "# Calculate the gradient\n",
    "dL_dw_out_h1 = dL_dy_pred * dy_dz * dz_dw_h1\n",
    "\n",
    "print(\"Calculation:\")\n",
    "print(f\"   âˆ‚L/âˆ‚w_out_h1 = ({dL_dy_pred:.4f}) Ã— ({dy_dz}) Ã— ({dz_dw_h1})\")\n",
    "print(f\"               = {dL_dw_out_h1:.4f}\")\n",
    "print()\n",
    "\n",
    "# --- What does this gradient tell us? ---\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"ğŸ¯ GRADIENT for w_out_h1: {dL_dw_out_h1:.4f}\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"What does this mean?\")\n",
    "print(f\"   â€¢ The gradient is NEGATIVE ({dL_dw_out_h1:.4f})\")\n",
    "print(\"   â€¢ Negative gradient means: increasing this weight would DECREASE the loss\")\n",
    "print(\"   â€¢ So we SHOULD increase this weight!\")\n",
    "print()\n",
    "print(\"ğŸ’¡ Remember: We want to go DOWNHILL (toward lower loss)\")\n",
    "print(\"   The gradient points uphill, so we go the OPPOSITE direction!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6343cc6",
   "metadata": {},
   "source": [
    "### 3.1.4.1 Understanding Gradients: Why Do We SUBTRACT?\n",
    "\n",
    "This is super confusing at first, so let's make it crystal clear!\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”ï¸ The Hill Analogy\n",
    "\n",
    "Imagine you're lost in a foggy mountain and want to find the lowest valley:\n",
    "\n",
    "```\n",
    "                        THE MOUNTAIN OF LOSS\n",
    "    \n",
    "    Loss â†‘                    You are here! ğŸ§‘\n",
    "         â”‚                          â—\n",
    "      4  â”‚                        â•±   â•²\n",
    "         â”‚                      â•±       â•²\n",
    "      3  â”‚                    â•±           â•²    Gradient says \"go RIGHT\"\n",
    "         â”‚                  â•±               â•²   (points uphill)\n",
    "      2  â”‚                â•±                   â•²\n",
    "         â”‚              â•±                       â•²\n",
    "      1  â”‚            â•±                           â•²\n",
    "         â”‚          â•±         THE VALLEY           â•²\n",
    "      0  â”‚        â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€(goal!)â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â–º Weight\n",
    "         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "               1       2       3       4       5\n",
    "                           â†‘\n",
    "                    Optimal weight = 3\n",
    "                    (lowest point!)\n",
    "```\n",
    "\n",
    "**Key insight:**\n",
    "- The **gradient** tells you which way is UPHILL (toward higher loss)\n",
    "- We want to go DOWNHILL (toward lower loss)\n",
    "- So we go the **OPPOSITE** direction â†’ we SUBTRACT the gradient!\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Š Gradient vs Negative Gradient\n",
    "\n",
    "| Term | Direction | What We Want? |\n",
    "|------|-----------|---------------|\n",
    "| **Gradient** | Points UPHILL (higher loss) | âŒ No! |\n",
    "| **Negative Gradient** | Points DOWNHILL (lower loss) | âœ… Yes! |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¢ The Update Rule\n",
    "\n",
    "$$\\text{new weight} = \\text{old weight} - \\text{learning rate} \\times \\text{gradient}$$\n",
    "\n",
    "**Why subtract?**\n",
    "- If gradient is POSITIVE â†’ weight is too high â†’ subtract to make it smaller\n",
    "- If gradient is NEGATIVE â†’ weight is too low â†’ subtracting a negative = adding â†’ makes it bigger\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Current weight: w = 1.0\n",
    "Gradient: âˆ‚L/âˆ‚w = -4 (negative = weight should go UP)\n",
    "Learning rate: 0.1\n",
    "\n",
    "New weight = 1.0 - (0.1 Ã— -4)\n",
    "           = 1.0 - (-0.4)\n",
    "           = 1.0 + 0.4\n",
    "           = 1.4 âœ“ Weight increased!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "24c9e5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "GRADIENT vs NEGATIVE GRADIENT: A Hands-On Example\n",
      "======================================================================\n",
      "\n",
      "ğŸ“Š Our Loss Function: L(w) = (w - 3)Â²\n",
      "   This is shaped like a bowl ğŸ¥£\n",
      "   The bottom of the bowl (minimum) is at w = 3\n",
      "\n",
      "======================================================================\n",
      "ğŸ“ CURRENT POSITION: weight = 1.0\n",
      "======================================================================\n",
      "\n",
      "Loss at this weight:\n",
      "   L(1.0) = (1.0 - 3)Â² = (-2.0)Â² = 4.0\n",
      "\n",
      "======================================================================\n",
      "ğŸ“ COMPUTING THE GRADIENT\n",
      "======================================================================\n",
      "\n",
      "Gradient formula: dL/dw = 2(w - 3)\n",
      "At w = 1.0:\n",
      "   dL/dw = 2 Ã— (1.0 - 3)\n",
      "         = 2 Ã— -2.0\n",
      "         = -4.0\n",
      "\n",
      "======================================================================\n",
      "ğŸ” WHAT DOES THIS GRADIENT MEAN?\n",
      "======================================================================\n",
      "\n",
      "The gradient is -4.0 (NEGATIVE)\n",
      "\n",
      "A NEGATIVE gradient means:\n",
      "   â€¢ The slope is going DOWN as we move RIGHT\n",
      "   â€¢ UPHILL (toward higher loss) is to the LEFT\n",
      "   â€¢ DOWNHILL (toward lower loss) is to the RIGHT\n",
      "\n",
      "We want to go DOWNHILL, so we should move RIGHT (increase w)!\n",
      "\n",
      "======================================================================\n",
      "ğŸ”„ GRADIENT ASCENT vs GRADIENT DESCENT\n",
      "======================================================================\n",
      "\n",
      "âŒ GRADIENT ASCENT (Adding the gradient - WRONG!):\n",
      "   new_w = old_w + learning_rate Ã— gradient\n",
      "         = 1.0 + 0.1 Ã— (-4.0)\n",
      "         = 1.0 + -0.4\n",
      "         = 0.6\n",
      "   New loss: 5.76 â† WORSE! We went UPHILL ğŸ“ˆ\n",
      "\n",
      "âœ… GRADIENT DESCENT (Subtracting the gradient - CORRECT!):\n",
      "   new_w = old_w - learning_rate Ã— gradient\n",
      "         = 1.0 - 0.1 Ã— (-4.0)\n",
      "         = 1.0 - (-0.4)\n",
      "         = 1.4\n",
      "   New loss: 2.56 â† BETTER! We went DOWNHILL ğŸ“‰\n",
      "\n",
      "======================================================================\n",
      "ğŸ’¡ KEY TAKEAWAY\n",
      "======================================================================\n",
      "\n",
      "To minimize loss (go downhill), we SUBTRACT the gradient!\n",
      "\n",
      "   new_weight = old_weight - learning_rate Ã— gradient\n",
      "\n",
      "This is called GRADIENT DESCENT because we 'descend' down the hill.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# GRADIENT vs NEGATIVE GRADIENT: A Hands-On Example\n",
    "# ============================================================================\n",
    "# Let's see gradients in action with a simple example!\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"GRADIENT vs NEGATIVE GRADIENT: A Hands-On Example\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# --- Our simple loss function ---\n",
    "\n",
    "# Loss = (weight - 3)Â² - this loss is 0 when weight = 3\n",
    "\n",
    "def loss_function(w):\n",
    "    \"\"\"Compute loss: (w - 3)Â². Minimum at w = 3.\"\"\"\n",
    "    return (w - 3) ** 2\n",
    "\n",
    "def gradient(w):\n",
    "    \"\"\"Compute gradient: 2(w - 3). Tells us which way is uphill.\"\"\"\n",
    "    return 2 * (w - 3)\n",
    "\n",
    "print(\"ğŸ“Š Our Loss Function: L(w) = (w - 3)Â²\")\n",
    "print(\"   This is shaped like a bowl ğŸ¥£\")\n",
    "print(\"   The bottom of the bowl (minimum) is at w = 3\")\n",
    "print()\n",
    "\n",
    "# --- Current position ---\n",
    "\n",
    "w_current = 1.0  # Start at weight = 1\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"ğŸ“ CURRENT POSITION: weight = {w_current}\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(f\"Loss at this weight:\")\n",
    "print(f\"   L({w_current}) = ({w_current} - 3)Â² = ({w_current - 3})Â² = {loss_function(w_current)}\")\n",
    "print()\n",
    "\n",
    "# --- Compute the gradient ---\n",
    "\n",
    "grad = gradient(w_current)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ“ COMPUTING THE GRADIENT\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(f\"Gradient formula: dL/dw = 2(w - 3)\")\n",
    "print(f\"At w = {w_current}:\")\n",
    "print(f\"   dL/dw = 2 Ã— ({w_current} - 3)\")\n",
    "print(f\"         = 2 Ã— {w_current - 3}\")\n",
    "print(f\"         = {grad}\")\n",
    "print()\n",
    "\n",
    "# --- Interpret the gradient ---\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ” WHAT DOES THIS GRADIENT MEAN?\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(f\"The gradient is {grad} (NEGATIVE)\")\n",
    "print()\n",
    "print(\"A NEGATIVE gradient means:\")\n",
    "print(\"   â€¢ The slope is going DOWN as we move RIGHT\")\n",
    "print(\"   â€¢ UPHILL (toward higher loss) is to the LEFT\")\n",
    "print(\"   â€¢ DOWNHILL (toward lower loss) is to the RIGHT\")\n",
    "print()\n",
    "print(\"We want to go DOWNHILL, so we should move RIGHT (increase w)!\")\n",
    "print()\n",
    "\n",
    "# --- Gradient ascent vs descent ---\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ”„ GRADIENT ASCENT vs GRADIENT DESCENT\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# WRONG: Gradient Ascent\n",
    "w_ascent = w_current + learning_rate * grad\n",
    "loss_ascent = loss_function(w_ascent)\n",
    "\n",
    "print(\"âŒ GRADIENT ASCENT (Adding the gradient - WRONG!):\")\n",
    "print(f\"   new_w = old_w + learning_rate Ã— gradient\")\n",
    "print(f\"         = {w_current} + {learning_rate} Ã— ({grad})\")\n",
    "print(f\"         = {w_current} + {learning_rate * grad}\")\n",
    "print(f\"         = {w_ascent}\")\n",
    "print(f\"   New loss: {loss_ascent:.2f} â† WORSE! We went UPHILL ğŸ“ˆ\")\n",
    "print()\n",
    "\n",
    "# CORRECT: Gradient Descent\n",
    "w_descent = w_current - learning_rate * grad\n",
    "loss_descent = loss_function(w_descent)\n",
    "\n",
    "print(\"âœ… GRADIENT DESCENT (Subtracting the gradient - CORRECT!):\")\n",
    "print(f\"   new_w = old_w - learning_rate Ã— gradient\")\n",
    "print(f\"         = {w_current} - {learning_rate} Ã— ({grad})\")\n",
    "print(f\"         = {w_current} - ({learning_rate * grad})\")\n",
    "print(f\"         = {w_descent}\")\n",
    "print(f\"   New loss: {loss_descent:.2f} â† BETTER! We went DOWNHILL ğŸ“‰\")\n",
    "print()\n",
    "\n",
    "# --- Summary ---\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ’¡ KEY TAKEAWAY\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"To minimize loss (go downhill), we SUBTRACT the gradient!\")\n",
    "print()\n",
    "print(\"   new_weight = old_weight - learning_rate Ã— gradient\")\n",
    "print()\n",
    "print(\"This is called GRADIENT DESCENT because we 'descend' down the hill.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "a8d00908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "WATCHING GRADIENT DESCENT IN ACTION\n",
      "======================================================================\n",
      "\n",
      "ğŸ¯ GOAL: Find the weight that minimizes L(w) = (w - 3)Â²\n",
      "         The optimal weight is 3.0 (where loss = 0)\n",
      "\n",
      "ğŸ“ Starting position: w = 0.0\n",
      "ğŸ“ Learning rate: 0.1\n",
      "\n",
      "======================================================================\n",
      "STEP-BY-STEP PROGRESS\n",
      "======================================================================\n",
      "\n",
      "Step   Weight       Loss         Gradient     Action              \n",
      "----------------------------------------------------------------------\n",
      "0      0.0000       9.0000       -6.0000      â†’ increase w        \n",
      "1      0.6000       5.7600       -4.8000      â†’ increase w        \n",
      "2      1.0800       3.6864       -3.8400      â†’ increase w        \n",
      "3      1.4640       2.3593       -3.0720      â†’ increase w        \n",
      "4      1.7712       1.5099       -2.4576      â†’ increase w        \n",
      "5      2.0170       0.9664       -1.9661      â†’ increase w        \n",
      "6      2.2136       0.6185       -1.5729      â†’ increase w        \n",
      "7      2.3709       0.3958       -1.2583      â†’ increase w        \n",
      "FINAL  2.4967       0.2533      \n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š WHAT HAPPENED:\n",
      "======================================================================\n",
      "\n",
      "   â€¢ We started at w = 0.0 (loss = 9.0)\n",
      "   â€¢ After 8 steps, we're at w = 2.4967 (loss = 0.2533)\n",
      "   â€¢ The optimal is w = 3.0 (loss = 0.0)\n",
      "\n",
      "Notice how:\n",
      "   1. The weight gets closer to 3 each step\n",
      "   2. The loss gets smaller each step\n",
      "   3. The gradient gets smaller as we approach the minimum\n",
      "   4. We never overshoot because learning rate is small enough\n",
      "\n",
      "ğŸ‰ This is how neural networks learn!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# WATCHING GRADIENT DESCENT IN ACTION (Multiple Steps!)\n",
    "# ============================================================================\n",
    "# Let's see how the weight gets closer to optimal over many steps.\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"WATCHING GRADIENT DESCENT IN ACTION\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# --- Setup ---\n",
    "\n",
    "w = 0.0               # Start at weight = 0 (far from optimal)\n",
    "learning_rate = 0.1   # How big each step is\n",
    "optimal_w = 3.0       # The best weight (where loss = 0)\n",
    "\n",
    "print(f\"ğŸ¯ GOAL: Find the weight that minimizes L(w) = (w - 3)Â²\")\n",
    "print(f\"         The optimal weight is 3.0 (where loss = 0)\")\n",
    "print()\n",
    "print(f\"ğŸ“ Starting position: w = {w}\")\n",
    "print(f\"ğŸ“ Learning rate: {learning_rate}\")\n",
    "print()\n",
    "\n",
    "# --- Run gradient descent for 8 steps ---\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STEP-BY-STEP PROGRESS\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(f\"{'Step':<6} {'Weight':<12} {'Loss':<12} {'Gradient':<12} {'Action':<20}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for step in range(8):\n",
    "    # Calculate current loss and gradient\n",
    "    current_loss = loss_function(w)\n",
    "    grad = gradient(w)\n",
    "    \n",
    "    # Determine direction\n",
    "    if grad < 0:\n",
    "        action = \"â†’ increase w\"\n",
    "    elif grad > 0:\n",
    "        action = \"â† decrease w\"\n",
    "    else:\n",
    "        action = \"â–  at minimum!\"\n",
    "    \n",
    "    # Print current state\n",
    "    print(f\"{step:<6} {w:<12.4f} {current_loss:<12.4f} {grad:<12.4f} {action:<20}\")\n",
    "    \n",
    "    # Update weight using gradient descent\n",
    "    w = w - learning_rate * grad\n",
    "\n",
    "# Print final state\n",
    "print(f\"{'FINAL':<6} {w:<12.4f} {loss_function(w):<12.4f}\")\n",
    "print()\n",
    "\n",
    "# --- Summary ---\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ“Š WHAT HAPPENED:\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(f\"   â€¢ We started at w = 0.0 (loss = 9.0)\")\n",
    "print(f\"   â€¢ After 8 steps, we're at w = {w:.4f} (loss = {loss_function(w):.4f})\")\n",
    "print(f\"   â€¢ The optimal is w = 3.0 (loss = 0.0)\")\n",
    "print()\n",
    "print(\"Notice how:\")\n",
    "print(\"   1. The weight gets closer to 3 each step\")\n",
    "print(\"   2. The loss gets smaller each step\")\n",
    "print(\"   3. The gradient gets smaller as we approach the minimum\")\n",
    "print(\"   4. We never overshoot because learning rate is small enough\")\n",
    "print()\n",
    "print(\"ğŸ‰ This is how neural networks learn!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebdbc90",
   "metadata": {},
   "source": [
    "### 3.1.4.2 Summary: The Two Directions\n",
    "\n",
    "Let's make sure we're crystal clear on the difference:\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Š Gradient Ascent vs Gradient Descent\n",
    "\n",
    "| Method | Formula | Direction | When to Use |\n",
    "|--------|---------|-----------|-------------|\n",
    "| **Gradient Ascent** | $w_{new} = w + \\alpha \\cdot gradient$ | UPHILL (maximize) | Finding highest point |\n",
    "| **Gradient Descent** | $w_{new} = w - \\alpha \\cdot gradient$ | DOWNHILL (minimize) | **Minimizing loss âœ“** |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§® Why the Subtraction Works Perfectly\n",
    "\n",
    "The magic of subtracting the gradient:\n",
    "\n",
    "```\n",
    "If gradient > 0:\n",
    "   â†’ \"Increasing w makes loss go UP\"\n",
    "   â†’ We want loss to go DOWN\n",
    "   â†’ So DECREASE w\n",
    "   â†’ w_new = w_old - (positive number) \n",
    "   â†’ Weight gets SMALLER âœ“\n",
    "\n",
    "If gradient < 0:\n",
    "   â†’ \"Increasing w makes loss go DOWN\"\n",
    "   â†’ That's good! Let's increase w\n",
    "   â†’ So INCREASE w\n",
    "   â†’ w_new = w_old - (negative number)\n",
    "   â†’ w_new = w_old + (positive number)\n",
    "   â†’ Weight gets BIGGER âœ“\n",
    "```\n",
    "\n",
    "**The subtraction automatically does the right thing!**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Real World Analogy\n",
    "\n",
    "Think of a ball rolling downhill:\n",
    "- **Gravity** always pulls the ball toward the lowest point\n",
    "- The **negative gradient** is like gravity for neural networks\n",
    "- It always pulls weights toward lower loss\n",
    "\n",
    "```\n",
    "      â—  â† Ball (our weight)\n",
    "     â•± â•²\n",
    "    â•±   â•²\n",
    "   â•±     â•²  Gravity pulls DOWN\n",
    "  â•±       â•²   (negative gradient direction)\n",
    " â•±    â†“    â•²\n",
    "â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—  â† Lowest point (minimum loss)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "48c308ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "GRADIENT DESCENT: Updating the Weights\n",
      "======================================================================\n",
      "\n",
      "ğŸ“Š CURRENT VALUES:\n",
      "   Weight:        w_out_h1 = 0.7\n",
      "   Gradient:      âˆ‚L/âˆ‚w = -0.4973\n",
      "   Learning rate: Î± = 0.1\n",
      "\n",
      "======================================================================\n",
      "APPLYING THE UPDATE RULE\n",
      "======================================================================\n",
      "\n",
      "Formula: w_new = w_old - Î± Ã— gradient\n",
      "\n",
      "Step 1: Multiply learning rate by gradient\n",
      "        Î± Ã— gradient = 0.1 Ã— (-0.4973)\n",
      "                     = -0.0497\n",
      "\n",
      "Step 2: Subtract from old weight\n",
      "        w_new = 0.7 - (-0.0497)\n",
      "              = 0.7497\n",
      "\n",
      "======================================================================\n",
      "âœ… WEIGHT UPDATED: 0.7000 â†’ 0.7497\n",
      "======================================================================\n",
      "\n",
      "The weight INCREASED by 0.0497\n",
      "\n",
      "Why did this happen?\n",
      "   â€¢ The gradient was NEGATIVE (-0.4973)\n",
      "   â€¢ Negative gradient means: increasing w would DECREASE loss\n",
      "   â€¢ So the update rule increased the weight (correct!)\n",
      "\n",
      "======================================================================\n",
      "ğŸ” VERIFICATION: Does the new weight reduce loss?\n",
      "======================================================================\n",
      "\n",
      "With OLD weight:\n",
      "   Prediction = 0.4420\n",
      "   Loss = (1.0 - 0.4420)Â² = 0.3114\n",
      "\n",
      "With NEW weight:\n",
      "   Prediction = 0.4698\n",
      "   Loss = (1.0 - 0.4698)Â² = 0.2811\n",
      "\n",
      "======================================================================\n",
      "ğŸ‰ LOSS REDUCED BY 0.0303!\n",
      "======================================================================\n",
      "\n",
      "The gradient descent update worked perfectly!\n",
      "By adjusting the weight in the direction of lower loss,\n",
      "we made our prediction closer to the true value.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# GRADIENT DESCENT: Actually Updating the Weights!\n",
    "# ============================================================================\n",
    "# Now let's use what we learned to update a weight and verify it helps!\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"GRADIENT DESCENT: Updating the Weights\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# --- Recall our values from backpropagation ---\n",
    "\n",
    "w_out_h1 = 0.7          # Current weight value\n",
    "dL_dw_out_h1 = -0.4973  # Gradient we computed\n",
    "learning_rate = 0.1     # How big of a step to take\n",
    "\n",
    "print(\"ğŸ“Š CURRENT VALUES:\")\n",
    "print(f\"   Weight:        w_out_h1 = {w_out_h1}\")\n",
    "print(f\"   Gradient:      âˆ‚L/âˆ‚w = {dL_dw_out_h1:.4f}\")\n",
    "print(f\"   Learning rate: Î± = {learning_rate}\")\n",
    "print()\n",
    "\n",
    "# --- Apply the update rule ---\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"APPLYING THE UPDATE RULE\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"Formula: w_new = w_old - Î± Ã— gradient\")\n",
    "print()\n",
    "\n",
    "# Step-by-step calculation\n",
    "step1 = learning_rate * dL_dw_out_h1\n",
    "w_out_h1_new = w_out_h1 - step1\n",
    "\n",
    "print(\"Step 1: Multiply learning rate by gradient\")\n",
    "print(f\"        Î± Ã— gradient = {learning_rate} Ã— ({dL_dw_out_h1:.4f})\")\n",
    "print(f\"                     = {step1:.4f}\")\n",
    "print()\n",
    "print(\"Step 2: Subtract from old weight\")\n",
    "print(f\"        w_new = {w_out_h1} - ({step1:.4f})\")\n",
    "print(f\"              = {w_out_h1_new:.4f}\")\n",
    "print()\n",
    "\n",
    "# --- Interpret the result ---\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"âœ… WEIGHT UPDATED: {w_out_h1:.4f} â†’ {w_out_h1_new:.4f}\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Did the weight increase or decrease?\n",
    "if w_out_h1_new > w_out_h1:\n",
    "    change = \"INCREASED\"\n",
    "else:\n",
    "    change = \"DECREASED\"\n",
    "\n",
    "print(f\"The weight {change} by {abs(w_out_h1_new - w_out_h1):.4f}\")\n",
    "print()\n",
    "print(\"Why did this happen?\")\n",
    "print(f\"   â€¢ The gradient was NEGATIVE ({dL_dw_out_h1:.4f})\")\n",
    "print(f\"   â€¢ Negative gradient means: increasing w would DECREASE loss\")\n",
    "print(f\"   â€¢ So the update rule increased the weight (correct!)\")\n",
    "print()\n",
    "\n",
    "# --- Verify: Does the new weight actually reduce loss? ---\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ” VERIFICATION: Does the new weight reduce loss?\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Values from our network\n",
    "h1_val = 0.56\n",
    "h2_val = 0.0\n",
    "w_out_h2 = 0.4\n",
    "b_out = 0.05\n",
    "y_true = 1.0\n",
    "\n",
    "# Calculate predictions with old and new weights\n",
    "y_pred_old = w_out_h1 * h1_val + w_out_h2 * h2_val + b_out\n",
    "y_pred_new = w_out_h1_new * h1_val + w_out_h2 * h2_val + b_out\n",
    "\n",
    "# Calculate losses\n",
    "loss_old = (y_true - y_pred_old) ** 2\n",
    "loss_new = (y_true - y_pred_new) ** 2\n",
    "\n",
    "print(\"With OLD weight:\")\n",
    "print(f\"   Prediction = {y_pred_old:.4f}\")\n",
    "print(f\"   Loss = ({y_true} - {y_pred_old:.4f})Â² = {loss_old:.4f}\")\n",
    "print()\n",
    "print(\"With NEW weight:\")\n",
    "print(f\"   Prediction = {y_pred_new:.4f}\")\n",
    "print(f\"   Loss = ({y_true} - {y_pred_new:.4f})Â² = {loss_new:.4f}\")\n",
    "print()\n",
    "\n",
    "# Compare\n",
    "loss_reduction = loss_old - loss_new\n",
    "print(\"=\" * 70)\n",
    "print(f\"ğŸ‰ LOSS REDUCED BY {loss_reduction:.4f}!\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"The gradient descent update worked perfectly!\")\n",
    "print(\"By adjusting the weight in the direction of lower loss,\")\n",
    "print(\"we made our prediction closer to the true value.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a954464d",
   "metadata": {},
   "source": [
    "### 3.1.5 The Complete Training Loop (Putting It All Together!)\n",
    "\n",
    "Now you understand all the pieces! Here's how they fit together in the **training loop**:\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”„ The 5 Steps of Training (Repeated Thousands of Times!)\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                     THE NEURAL NETWORK TRAINING LOOP                    â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                         â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\n",
    "â”‚  â”‚                                                                 â”‚    â”‚\n",
    "â”‚  â”‚  STEP 1: FORWARD PASS                                           â”‚    â”‚\n",
    "â”‚  â”‚          Feed input through the network to get a prediction     â”‚    â”‚\n",
    "â”‚  â”‚          Input â†’ Weights â†’ Activation â†’ Output                  â”‚    â”‚\n",
    "â”‚  â”‚                                                                 â”‚    â”‚\n",
    "â”‚  â”‚  STEP 2: COMPUTE LOSS                                           â”‚    â”‚\n",
    "â”‚  â”‚          Measure how wrong the prediction is                    â”‚    â”‚\n",
    "â”‚  â”‚          Loss = (True - Prediction)Â²                            â”‚    â”‚\n",
    "â”‚  â”‚                                                                 â”‚    â”‚\n",
    "â”‚  â”‚  STEP 3: BACKWARD PASS (Backpropagation)                        â”‚    â”‚\n",
    "â”‚  â”‚          Calculate gradients using the chain rule               â”‚    â”‚\n",
    "â”‚  â”‚          \"How much did each weight contribute to the error?\"    â”‚    â”‚\n",
    "â”‚  â”‚                                                                 â”‚    â”‚\n",
    "â”‚  â”‚  STEP 4: UPDATE WEIGHTS (Gradient Descent)                      â”‚    â”‚\n",
    "â”‚  â”‚          Adjust each weight to reduce the loss                  â”‚    â”‚\n",
    "â”‚  â”‚          weight = weight - learning_rate Ã— gradient             â”‚    â”‚\n",
    "â”‚  â”‚                                                                 â”‚    â”‚\n",
    "â”‚  â”‚  STEP 5: REPEAT!                                                â”‚    â”‚\n",
    "â”‚  â”‚          Go back to Step 1 and do it again                      â”‚    â”‚\n",
    "â”‚  â”‚          Each time is called an \"epoch\" or \"step\"               â”‚    â”‚\n",
    "â”‚  â”‚                                                                 â”‚    â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚\n",
    "â”‚                                    â†‘                                    â”‚\n",
    "â”‚                                    â”‚                                    â”‚\n",
    "â”‚                              Repeat 1000s                               â”‚\n",
    "â”‚                               of times!                                 â”‚\n",
    "â”‚                                                                         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ This is EXACTLY How GPT Learns!\n",
    "\n",
    "| Component | What Gets Updated | How |\n",
    "|-----------|------------------|-----|\n",
    "| Embeddings | The 32 numbers for each word | Gradient descent |\n",
    "| Attention weights | Which words to pay attention to | Gradient descent |\n",
    "| Feed-forward weights | How to transform information | Gradient descent |\n",
    "| All biases | All the starting offsets | Gradient descent |\n",
    "\n",
    "After millions of training steps:\n",
    "- Words with similar meanings get similar embeddings\n",
    "- Attention learns to focus on relevant context\n",
    "- The model learns to predict the next word accurately!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "68cdd13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "COMPLETE TRAINING LOOP IN PYTORCH\n",
      "======================================================================\n",
      "\n",
      "ğŸ“¦ Network created!\n",
      "   Total parameters: 9\n",
      "\n",
      "ğŸ“Š TRAINING DATA:\n",
      "   Input:  [[0.800000011920929, 0.20000000298023224]] (action=0.8, romance=0.2)\n",
      "   Target: [[1.0]] (we want output = 1.0)\n",
      "\n",
      "======================================================================\n",
      "TRAINING LOOP (10 epochs)\n",
      "======================================================================\n",
      "\n",
      "Epoch    Prediction      Loss            Status              \n",
      "----------------------------------------------------------\n",
      "0        0.5456          0.2065          Learning...         \n",
      "2        0.7946          0.0422          Getting better!     \n",
      "4        0.9066          0.0087          Almost perfect!     \n",
      "6        0.9579          0.0018          Almost perfect!     \n",
      "8        0.9812          0.0004          Almost perfect!     \n",
      "9        0.9874          0.0002          Almost perfect!     \n",
      "----------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "âœ… TRAINING COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "Final prediction: 0.9874\n",
      "Target was:       1.0\n",
      "Difference:       0.0126\n",
      "\n",
      "ğŸ‰ The network learned to produce the correct output!\n",
      "\n",
      "PyTorch did all the hard work:\n",
      "   â€¢ Forward pass: automatic\n",
      "   â€¢ Loss computation: F.mse_loss()\n",
      "   â€¢ Backward pass: loss.backward()\n",
      "   â€¢ Weight updates: optimizer.step()\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# COMPLETE TRAINING LOOP IN PYTORCH\n",
    "# ============================================================================\n",
    "# Now let's see how PyTorch makes all this easy!\n",
    "# PyTorch does the backpropagation automatically for us.\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"COMPLETE TRAINING LOOP IN PYTORCH\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# --- Define our neural network ---\n",
    "\n",
    "class SimpleNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple 2-layer neural network for learning demonstrations.\n",
    "    \n",
    "    Architecture:\n",
    "        Input (2) â†’ Hidden Layer (2 neurons) â†’ ReLU â†’ Output (1)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(2, 2)   # Hidden layer: 2 inputs â†’ 2 neurons\n",
    "        self.output = nn.Linear(2, 1)   # Output layer: 2 neurons â†’ 1 output\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden(x))      # Apply ReLU activation\n",
    "        x = self.output(x)               # Output (no activation for regression)\n",
    "        return x\n",
    "\n",
    "# --- Create the network and optimizer ---\n",
    "\n",
    "torch.manual_seed(42)  # For reproducibility\n",
    "\n",
    "net = SimpleNetwork()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.1)\n",
    "\n",
    "print(\"ğŸ“¦ Network created!\")\n",
    "print(f\"   Total parameters: {sum(p.numel() for p in net.parameters())}\")\n",
    "print()\n",
    "\n",
    "# --- Prepare training data ---\n",
    "\n",
    "x_train = torch.tensor([[0.8, 0.2]])  # Input: action=0.8, romance=0.2\n",
    "y_train = torch.tensor([[1.0]])       # Target: we want output = 1.0\n",
    "\n",
    "print(\"ğŸ“Š TRAINING DATA:\")\n",
    "print(f\"   Input:  {x_train.tolist()} (action=0.8, romance=0.2)\")\n",
    "print(f\"   Target: {y_train.tolist()} (we want output = 1.0)\")\n",
    "print()\n",
    "\n",
    "# --- The training loop ---\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TRAINING LOOP (10 epochs)\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(f\"{'Epoch':<8} {'Prediction':<15} {'Loss':<15} {'Status':<20}\")\n",
    "print(\"-\" * 58)\n",
    "\n",
    "for epoch in range(10):\n",
    "    # STEP 1: Forward pass - get prediction\n",
    "    y_pred = net(x_train)\n",
    "    \n",
    "    # STEP 2: Compute loss - measure error\n",
    "    loss = F.mse_loss(y_pred, y_train)\n",
    "    \n",
    "    # Determine status\n",
    "    if loss.item() > 0.1:\n",
    "        status = \"Learning...\"\n",
    "    elif loss.item() > 0.01:\n",
    "        status = \"Getting better!\"\n",
    "    else:\n",
    "        status = \"Almost perfect!\"\n",
    "    \n",
    "    # Print progress\n",
    "    if epoch % 2 == 0 or epoch == 9:\n",
    "        print(f\"{epoch:<8} {y_pred.item():<15.4f} {loss.item():<15.4f} {status:<20}\")\n",
    "    \n",
    "    # STEP 3: Backward pass - compute gradients\n",
    "    optimizer.zero_grad()  # Clear old gradients\n",
    "    loss.backward()        # Compute new gradients\n",
    "    \n",
    "    # STEP 4: Update weights\n",
    "    optimizer.step()\n",
    "\n",
    "# --- Final results ---\n",
    "\n",
    "print(\"-\" * 58)\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"âœ… TRAINING COMPLETE!\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(f\"Final prediction: {y_pred.item():.4f}\")\n",
    "print(f\"Target was:       1.0\")\n",
    "print(f\"Difference:       {abs(1.0 - y_pred.item()):.4f}\")\n",
    "print()\n",
    "print(\"ğŸ‰ The network learned to produce the correct output!\")\n",
    "print()\n",
    "print(\"PyTorch did all the hard work:\")\n",
    "print(\"   â€¢ Forward pass: automatic\")\n",
    "print(\"   â€¢ Loss computation: F.mse_loss()\")\n",
    "print(\"   â€¢ Backward pass: loss.backward()\")\n",
    "print(\"   â€¢ Weight updates: optimizer.step()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442d1490",
   "metadata": {},
   "source": [
    "### 3.1.6 Key Takeaways: Neural Network Fundamentals\n",
    "\n",
    "ğŸ“ **Congratulations!** You now understand the core of how neural networks learn!\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‹ Summary Table\n",
    "\n",
    "| Concept | What It Is | Simple Explanation |\n",
    "|---------|-----------|-------------------|\n",
    "| **Neuron** | Basic computing unit | Takes inputs, multiplies by weights, adds bias, applies activation |\n",
    "| **Weights** | Learnable numbers | How important each input is (gets adjusted during training) |\n",
    "| **Bias** | Learnable offset | A starting point that shifts the output up or down |\n",
    "| **Activation** | Non-linear function | Makes the network able to learn complex patterns (ReLU, sigmoid) |\n",
    "| **Forward Pass** | Computing output | Input â†’ through all layers â†’ prediction |\n",
    "| **Loss Function** | Error measurement | \"How wrong is our prediction?\" (MSE, Cross-Entropy) |\n",
    "| **Gradient** | Slope of the loss | \"Which way is uphill?\" |\n",
    "| **Backpropagation** | Finding gradients | Uses chain rule to compute how each weight affects loss |\n",
    "| **Gradient Descent** | Updating weights | `new = old - learning_rate Ã— gradient` |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”— How This Connects to GPT\n",
    "\n",
    "Everything in GPT works the same way!\n",
    "\n",
    "| GPT Component | What It Is | How It Learns |\n",
    "|---------------|-----------|---------------|\n",
    "| **Embedding Matrix** | 27 words Ã— 32 numbers | Random at first â†’ trained via gradient descent |\n",
    "| **Attention Weights** | Q, K, V matrices | Learned to focus on relevant words |\n",
    "| **Feed-Forward Layers** | Just like our SimpleNetwork! | Same neurons, weights, biases |\n",
    "| **All Parameters** | ~30,000+ numbers | All updated by gradient descent |\n",
    "\n",
    "**The only difference:** GPT is bigger and the loss function is \"predict the next word\" instead of movie preferences!\n",
    "\n",
    "Now let's see how embeddings actually learn meaning! ğŸ‘‡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26d2bc8",
   "metadata": {},
   "source": [
    "### 3.2 How Embeddings Learn Meaning (The Magic!)\n",
    "\n",
    "Remember our embedding matrix? Those 32 random numbers for each word?\n",
    "\n",
    "Here's the amazing part: **after training, they're not random anymore!**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ² Before Training vs ğŸ¯ After Training\n",
    "\n",
    "| Stage | What the Numbers Look Like | Meaning |\n",
    "|-------|---------------------------|---------|\n",
    "| **Before** | `[0.42, -0.17, 0.89, ...]` (random) | Meaningless noise |\n",
    "| **After** | `[0.42, -0.17, 0.89, ...]` (trained) | Encodes word relationships! |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  How Does This Happen?\n",
    "\n",
    "1. **The model tries to predict the next word**\n",
    "   - Input: \"mary had a little\"\n",
    "   - Correct answer: \"lamb\"\n",
    "   \n",
    "2. **If wrong, backpropagation adjusts the embeddings**\n",
    "   - \"The embedding for 'lamb' should be more similar to 'little'\"\n",
    "   - Gradients flow back and tweak the 32 numbers\n",
    "   \n",
    "3. **Over thousands of examples, patterns emerge**\n",
    "   - Words appearing in similar contexts get similar embeddings\n",
    "   - Related words cluster together\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ‘ In Our \"Mary Had a Little Lamb\" Model\n",
    "\n",
    "After training, the model might learn:\n",
    "- **\"lamb\" and \"fleece\"** have similar vectors (both describe the lamb)\n",
    "- **\"mary\" and \"went\"** are connected (Mary does the going)\n",
    "- **\"school\" and \"rules\"** cluster together (both about school)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¤¯ The Famous Example (from larger models)\n",
    "\n",
    "With enough training data, embeddings can do this:\n",
    "\n",
    "$$\\text{embedding(\"king\")} - \\text{embedding(\"man\")} + \\text{embedding(\"woman\")} \\approx \\text{embedding(\"queen\")}$$\n",
    "\n",
    "The model learned that \"king\" is to \"man\" as \"queen\" is to \"woman\"!\n",
    "\n",
    "**We never told it this.** It discovered the relationship by predicting words!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63a0799",
   "metadata": {},
   "source": [
    "### 3.3 Understanding Attention Heads (How Words Talk to Each Other)\n",
    "\n",
    "Now comes the **coolest part of GPT**: **attention heads**!\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¤” The Problem\n",
    "\n",
    "We have word embeddings (32 numbers per word). But words in isolation are useless!\n",
    "\n",
    "Consider: \"The lamb was sure to go\"\n",
    "- What does \"it\" refer to in the previous sentence?\n",
    "- What is the lamb going TO?\n",
    "\n",
    "**Words need to share information with each other!**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ’¡ The Solution: Attention\n",
    "\n",
    "**Attention** lets each word look at other words and gather relevant information.\n",
    "\n",
    "Think of it like a student taking notes in class:\n",
    "- You're writing about \"lamb\"\n",
    "- You glance at \"mary\" to remember whose lamb it is\n",
    "- You glance at \"little\" to remember what kind of lamb\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ­ Why MULTIPLE Heads?\n",
    "\n",
    "One attention head might miss important relationships. So we use **multiple heads**!\n",
    "\n",
    "With `n_heads = 2`:\n",
    "\n",
    "| Head | What It Might Learn | Example |\n",
    "|------|---------------------|---------|\n",
    "| Head 1 | \"Who owns this?\" | lamb â†’ looks at â†’ mary |\n",
    "| Head 2 | \"What action happened?\" | lamb â†’ looks at â†’ had |\n",
    "\n",
    "**Each head looks at the sentence differently!** Then we combine their findings.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ‚ï¸ How Do We Split the Embedding?\n",
    "\n",
    "Since `embedding_dim = 32` and `n_heads = 2`:\n",
    "\n",
    "```\n",
    "Original embedding: 32 dimensions\n",
    "                    â†“\n",
    "          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "          â†“                   â†“\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚  Head 1   â”‚       â”‚  Head 2   â”‚\n",
    "    â”‚ 16 dims   â”‚       â”‚ 16 dims   â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n",
    "          â”‚                   â”‚\n",
    "          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                    â†“\n",
    "          Concatenate back â†’ 32 dimensions\n",
    "```\n",
    "\n",
    "Each head works with 16 dimensions, then we glue the results back together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "6db4f0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "HOW THE EMBEDDING IS SPLIT ACROSS ATTENTION HEADS\n",
      "======================================================================\n",
      "\n",
      "ğŸ“Š THE SPLIT:\n",
      "   Total embedding dimensions: 32\n",
      "   Number of attention heads:  2\n",
      "   Dimensions per head:        32 Ã· 2 = 16\n",
      "\n",
      "ğŸ“ VISUAL REPRESENTATION:\n",
      "\n",
      "   Full token embedding (32 dimensions):\n",
      "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "   â”‚  dim0, dim1, dim2, ... dim15 â”‚ dim16, dim17, ... dim31     â”‚\n",
      "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "                   â”‚                               â”‚\n",
      "                   â–¼                               â–¼\n",
      "           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "           â”‚    HEAD 1     â”‚               â”‚    HEAD 2     â”‚\n",
      "           â”‚  (dims 0-15)  â”‚               â”‚ (dims 16-31)  â”‚\n",
      "           â”‚   16 numbers  â”‚               â”‚   16 numbers  â”‚\n",
      "           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "ğŸ’¡ Each head works with 16 dimensions independently,\n",
      "   then we combine their results back into 32 dimensions!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# HOW THE EMBEDDING IS SPLIT ACROSS ATTENTION HEADS\n",
    "# ============================================================================\n",
    "# Let's see exactly how each head gets its portion of the embedding.\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"HOW THE EMBEDDING IS SPLIT ACROSS ATTENTION HEADS\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# --- Calculate head size ---\n",
    "\n",
    "head_size = embedding_dim // n_heads  # 32 Ã· 2 = 16 dimensions per head\n",
    "\n",
    "print(\"ğŸ“Š THE SPLIT:\")\n",
    "print(f\"   Total embedding dimensions: {embedding_dim}\")\n",
    "print(f\"   Number of attention heads:  {n_heads}\")\n",
    "print(f\"   Dimensions per head:        {embedding_dim} Ã· {n_heads} = {head_size}\")\n",
    "print()\n",
    "\n",
    "# --- Visual representation ---\n",
    "\n",
    "print(\"ğŸ“ VISUAL REPRESENTATION:\")\n",
    "print()\n",
    "print(\"   Full token embedding (32 dimensions):\")\n",
    "print(\"   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "print(\"   â”‚  dim0, dim1, dim2, ... dim15 â”‚ dim16, dim17, ... dim31     â”‚\")\n",
    "print(\"   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
    "print(\"                   â”‚                               â”‚\")\n",
    "print(\"                   â–¼                               â–¼\")\n",
    "print(\"           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "print(\"           â”‚    HEAD 1     â”‚               â”‚    HEAD 2     â”‚\")\n",
    "print(\"           â”‚  (dims 0-15)  â”‚               â”‚ (dims 16-31)  â”‚\")\n",
    "print(\"           â”‚   16 numbers  â”‚               â”‚   16 numbers  â”‚\")\n",
    "print(\"           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
    "print()\n",
    "print(f\"ğŸ’¡ Each head works with {head_size} dimensions independently,\")\n",
    "print(f\"   then we combine their results back into {embedding_dim} dimensions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e6bcf4",
   "metadata": {},
   "source": [
    "### 3.3.1 Concrete Example: Watch Attention Heads in Action!\n",
    "\n",
    "Let's trace through exactly what happens with real words from our vocabulary!\n",
    "\n",
    "We'll use: **\"mary\"**, **\"had\"**, **\"a\"**\n",
    "\n",
    "You'll see:\n",
    "1. Each word gets a 32-number embedding\n",
    "2. The embedding is split between 2 heads (16 numbers each)\n",
    "3. Each head computes attention differently\n",
    "4. The results are combined back together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "2a3f7fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STEP 1: SELECT TOKENS FROM OUR VOCABULARY\n",
      "======================================================================\n",
      "\n",
      "ğŸ“ We're using these 3 words:\n",
      "   'mary' â†’ Token ID: 32\n",
      "   'had' â†’ Token ID: 28\n",
      "   'a'   â†’ Token ID: 13\n",
      "\n",
      "======================================================================\n",
      "STEP 2: CONVERT WORDS TO EMBEDDINGS (32 numbers each)\n",
      "======================================================================\n",
      "\n",
      "ğŸ“Š Embedding matrix lookup:\n",
      "   Input: 3 token IDs â†’ [32, 28, 13]\n",
      "   Output shape: torch.Size([3, 32]) (3 words Ã— 32 numbers each)\n",
      "\n",
      "   'mary' embedding (first 6 of 32):\n",
      "      [-1.225, 0.963, -1.579, 0.672, -0.060, 0.070, ...]\n",
      "\n",
      "   'had' embedding (first 6 of 32):\n",
      "      [-0.711, -0.387, 0.958, -0.823, -2.391, 0.322, ...]\n",
      "\n",
      "   'a' embedding (first 6 of 32):\n",
      "      [0.684, -1.325, -0.516, 0.600, -0.470, -0.609, ...]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 1: SELECT THREE WORDS FROM OUR VOCABULARY\n",
    "# ============================================================================\n",
    "# Let's trace through attention with real words!\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STEP 1: SELECT TOKENS FROM OUR VOCABULARY\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# --- Pick 3 words and get their token IDs ---\n",
    "\n",
    "word1, word2, word3 = \"mary\", \"had\", \"a\"\n",
    "\n",
    "id1 = word2idx[word1]\n",
    "id2 = word2idx[word2]\n",
    "id3 = word2idx[word3]\n",
    "\n",
    "print(f\"ğŸ“ We're using these 3 words:\")\n",
    "print(f\"   '{word1}' â†’ Token ID: {id1}\")\n",
    "print(f\"   '{word2}' â†’ Token ID: {id2}\")\n",
    "print(f\"   '{word3}'   â†’ Token ID: {id3}\")\n",
    "print()\n",
    "\n",
    "# --- Get embeddings for each word (32 numbers each) ---\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STEP 2: CONVERT WORDS TO EMBEDDINGS (32 numbers each)\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "torch.manual_seed(42)  # For reproducibility\n",
    "demo_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "token_ids = torch.tensor([id1, id2, id3])\n",
    "embeddings = demo_embedding(token_ids)  # Shape: [3, 32]\n",
    "\n",
    "print(f\"ğŸ“Š Embedding matrix lookup:\")\n",
    "print(f\"   Input: 3 token IDs â†’ {token_ids.tolist()}\")\n",
    "print(f\"   Output shape: {embeddings.shape} (3 words Ã— 32 numbers each)\")\n",
    "print()\n",
    "\n",
    "# Show a preview of each embedding\n",
    "for i, word in enumerate([word1, word2, word3]):\n",
    "    emb_preview = embeddings[i].data[:6].tolist()\n",
    "    print(f\"   '{word}' embedding (first 6 of 32):\")\n",
    "    print(f\"      [{emb_preview[0]:.3f}, {emb_preview[1]:.3f}, {emb_preview[2]:.3f}, {emb_preview[3]:.3f}, {emb_preview[4]:.3f}, {emb_preview[5]:.3f}, ...]\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "572b3dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STEP 3: SPLIT EACH EMBEDDING INTO 2 HEADS\n",
      "======================================================================\n",
      "\n",
      "ğŸ“Š Splitting 32 dimensions into 2 heads:\n",
      "   Head 1 gets dimensions 0-15  (16 numbers)\n",
      "   Head 2 gets dimensions 16-31 (16 numbers)\n",
      "\n",
      "'mary' (Token 32):\n",
      "   Full embedding: [-1.22, 0.96, ... -0.00]  (32 nums)\n",
      "      â†“ split â†“\n",
      "   Head 1 gets:    [-1.22, 0.96, ... 0.27]  (16 nums)\n",
      "   Head 2 gets:    [-0.35, -0.12, ... -0.00]  (16 nums)\n",
      "\n",
      "'had' (Token 28):\n",
      "   Full embedding: [-0.71, -0.39, ... -0.96]  (32 nums)\n",
      "      â†“ split â†“\n",
      "   Head 1 gets:    [-0.71, -0.39, ... -0.42]  (16 nums)\n",
      "   Head 2 gets:    [1.86, -1.08, ... -0.96]  (16 nums)\n",
      "\n",
      "'a' (Token 13):\n",
      "   Full embedding: [0.68, -1.32, ... -0.75]  (32 nums)\n",
      "      â†“ split â†“\n",
      "   Head 1 gets:    [0.68, -1.32, ... 0.69]  (16 nums)\n",
      "   Head 2 gets:    [-0.49, 1.14, ... -0.75]  (16 nums)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 3: SPLIT EACH EMBEDDING BETWEEN THE TWO HEADS\n",
    "# ============================================================================\n",
    "# Dims 0-15 go to Head 1, dims 16-31 go to Head 2\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STEP 3: SPLIT EACH EMBEDDING INTO 2 HEADS\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# --- Calculate head size ---\n",
    "\n",
    "head_size = embedding_dim // n_heads  # 32 Ã· 2 = 16 dimensions per head\n",
    "\n",
    "print(f\"ğŸ“Š Splitting {embedding_dim} dimensions into {n_heads} heads:\")\n",
    "print(f\"   Head 1 gets dimensions 0-15  ({head_size} numbers)\")\n",
    "print(f\"   Head 2 gets dimensions 16-31 ({head_size} numbers)\")\n",
    "print()\n",
    "\n",
    "# --- Show how each word's embedding is split ---\n",
    "\n",
    "for i, word in enumerate([word1, word2, word3]):\n",
    "    full_emb = embeddings[i].data\n",
    "    head1_portion = full_emb[:head_size]   # Dimensions 0-15\n",
    "    head2_portion = full_emb[head_size:]   # Dimensions 16-31\n",
    "    \n",
    "    print(f\"'{word}' (Token {token_ids[i].item()}):\")\n",
    "    print(f\"   Full embedding: [{full_emb[0]:.2f}, {full_emb[1]:.2f}, ... {full_emb[31]:.2f}]  (32 nums)\")\n",
    "    print(f\"      â†“ split â†“\")\n",
    "    print(f\"   Head 1 gets:    [{head1_portion[0]:.2f}, {head1_portion[1]:.2f}, ... {head1_portion[15]:.2f}]  (16 nums)\")\n",
    "    print(f\"   Head 2 gets:    [{head2_portion[0]:.2f}, {head2_portion[1]:.2f}, ... {head2_portion[15]:.2f}]  (16 nums)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "6342e20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STEP 4: EACH HEAD COMPUTES Q, K, V INDEPENDENTLY\n",
      "======================================================================\n",
      "\n",
      "ğŸ§  What are Q, K, V?\n",
      "   Q (Query):  'What am I looking for?'\n",
      "   K (Key):    'What do I have to offer?'\n",
      "   V (Value):  'What information should I share?'\n",
      "\n",
      "ğŸ“Š Each head has its own learnable matrices:\n",
      "   Head 1: Q matrix 16Ã—16, K matrix 16Ã—16, V matrix 16Ã—16\n",
      "   Head 2: Q matrix 16Ã—16, K matrix 16Ã—16, V matrix 16Ã—16\n",
      "\n",
      "ğŸ“¥ Input to each head:\n",
      "   Head 1 input: torch.Size([3, 16]) (3 tokens Ã— 16 dims)\n",
      "   Head 2 input: torch.Size([3, 16]) (3 tokens Ã— 16 dims)\n",
      "\n",
      "ğŸ“¤ Output Q, K, V (each is 3 tokens Ã— 16 dims):\n",
      "   Head 1: Q1[3, 16], K1[3, 16], V1[3, 16]\n",
      "   Head 2: Q2[3, 16], K2[3, 16], V2[3, 16]\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 4: EACH HEAD COMPUTES ATTENTION INDEPENDENTLY\n",
    "# ============================================================================\n",
    "# Each head has its own Q, K, V matrices (learnable weights!)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STEP 4: EACH HEAD COMPUTES Q, K, V INDEPENDENTLY\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "print(\"ğŸ§  What are Q, K, V?\")\n",
    "print(\"   Q (Query):  'What am I looking for?'\")\n",
    "print(\"   K (Key):    'What do I have to offer?'\")\n",
    "print(\"   V (Value):  'What information should I share?'\")\n",
    "print()\n",
    "\n",
    "# --- Create Q, K, V matrices for each head ---\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Head 1 matrices\n",
    "head1_query = nn.Linear(head_size, head_size, bias=False)\n",
    "head1_key = nn.Linear(head_size, head_size, bias=False)\n",
    "head1_value = nn.Linear(head_size, head_size, bias=False)\n",
    "\n",
    "# Head 2 matrices (different learned weights!)\n",
    "head2_query = nn.Linear(head_size, head_size, bias=False)\n",
    "head2_key = nn.Linear(head_size, head_size, bias=False)\n",
    "head2_value = nn.Linear(head_size, head_size, bias=False)\n",
    "\n",
    "print(\"ğŸ“Š Each head has its own learnable matrices:\")\n",
    "print(f\"   Head 1: Q matrix {head_size}Ã—{head_size}, K matrix {head_size}Ã—{head_size}, V matrix {head_size}Ã—{head_size}\")\n",
    "print(f\"   Head 2: Q matrix {head_size}Ã—{head_size}, K matrix {head_size}Ã—{head_size}, V matrix {head_size}Ã—{head_size}\")\n",
    "print()\n",
    "\n",
    "# --- Split embeddings for each head ---\n",
    "\n",
    "head1_embeddings = embeddings[:, :head_size]   # [3, 16] - dims 0-15\n",
    "head2_embeddings = embeddings[:, head_size:]   # [3, 16] - dims 16-31\n",
    "\n",
    "print(\"ğŸ“¥ Input to each head:\")\n",
    "print(f\"   Head 1 input: {head1_embeddings.shape} (3 tokens Ã— 16 dims)\")\n",
    "print(f\"   Head 2 input: {head2_embeddings.shape} (3 tokens Ã— 16 dims)\")\n",
    "print()\n",
    "\n",
    "# --- Compute Q, K, V for each head ---\n",
    "\n",
    "Q1 = head1_query(head1_embeddings)\n",
    "K1 = head1_key(head1_embeddings)\n",
    "V1 = head1_value(head1_embeddings)\n",
    "\n",
    "Q2 = head2_query(head2_embeddings)\n",
    "K2 = head2_key(head2_embeddings)\n",
    "V2 = head2_value(head2_embeddings)\n",
    "\n",
    "print(\"ğŸ“¤ Output Q, K, V (each is 3 tokens Ã— 16 dims):\")\n",
    "print(f\"   Head 1: Q1{list(Q1.shape)}, K1{list(K1.shape)}, V1{list(V1.shape)}\")\n",
    "print(f\"   Head 2: Q2{list(Q2.shape)}, K2{list(K2.shape)}, V2{list(V2.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "01fd5e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STEP 5: COMPUTE ATTENTION SCORES\n",
      "======================================================================\n",
      "\n",
      "ğŸ§® The attention formula:\n",
      "   Attention Score = (Query Ã— Key^T) / âˆš(head_size)\n",
      "\n",
      "   What this means:\n",
      "   â€¢ Each word's Query asks: 'What am I looking for?'\n",
      "   â€¢ Each word's Key answers: 'Here's what I have to offer'\n",
      "   â€¢ The score tells us how well they match!\n",
      "\n",
      "======================================================================\n",
      "ğŸ”µ HEAD 1 ATTENTION WEIGHTS\n",
      "======================================================================\n",
      "\n",
      "Read as: How much does each word (row) attend to other words (columns)?\n",
      "\n",
      "                    Attends to:\n",
      "                          mary        had          a\n",
      "   'mary'  â†’        0.228      0.322      0.450\n",
      "   'had'  â†’        0.404      0.254      0.341\n",
      "   'a'  â†’        0.344      0.354      0.302\n",
      "\n",
      "======================================================================\n",
      "ğŸŸ¢ HEAD 2 ATTENTION WEIGHTS\n",
      "======================================================================\n",
      "\n",
      "Read as: How much does each word (row) attend to other words (columns)?\n",
      "\n",
      "                    Attends to:\n",
      "                          mary        had          a\n",
      "   'mary'  â†’        0.241      0.415      0.344\n",
      "   'had'  â†’        0.304      0.244      0.453\n",
      "   'a'  â†’        0.526      0.241      0.233\n",
      "\n",
      "======================================================================\n",
      "ğŸ’¡ KEY OBSERVATION:\n",
      "======================================================================\n",
      "\n",
      "Notice how Head 1 and Head 2 have DIFFERENT attention patterns!\n",
      "This is because they have different learned weights.\n",
      "Each head learns to focus on different types of relationships.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 5: COMPUTE ATTENTION SCORES\n",
    "# ============================================================================\n",
    "# How much should each word look at others? This is the CORE of attention!\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STEP 5: COMPUTE ATTENTION SCORES\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "print(\"ğŸ§® The attention formula:\")\n",
    "print(\"   Attention Score = (Query Ã— Key^T) / âˆš(head_size)\")\n",
    "print()\n",
    "print(\"   What this means:\")\n",
    "print(\"   â€¢ Each word's Query asks: 'What am I looking for?'\")\n",
    "print(\"   â€¢ Each word's Key answers: 'Here's what I have to offer'\")\n",
    "print(\"   â€¢ The score tells us how well they match!\")\n",
    "print()\n",
    "\n",
    "# --- Compute attention scores ---\n",
    "\n",
    "# Q (3Ã—16) @ K.T (16Ã—3) = (3Ã—3) score matrix\n",
    "attn_scores_head1 = (Q1 @ K1.T) / (head_size ** 0.5)\n",
    "attn_scores_head2 = (Q2 @ K2.T) / (head_size ** 0.5)\n",
    "\n",
    "# Convert scores to probabilities (0-1, sum to 1)\n",
    "attn_weights_head1 = F.softmax(attn_scores_head1, dim=-1)\n",
    "attn_weights_head2 = F.softmax(attn_scores_head2, dim=-1)\n",
    "\n",
    "# --- Display attention weights ---\n",
    "\n",
    "words = [word1, word2, word3]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ”µ HEAD 1 ATTENTION WEIGHTS\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"Read as: How much does each word (row) attend to other words (columns)?\")\n",
    "print()\n",
    "print(f\"                    Attends to:\")\n",
    "print(f\"                    {word1:>10} {word2:>10} {word3:>10}\")\n",
    "for i, word in enumerate(words):\n",
    "    weights = [f\"{attn_weights_head1[i, j].item():.3f}\" for j in range(3)]\n",
    "    print(f\"   '{word}'  â†’   {weights[0]:>10} {weights[1]:>10} {weights[2]:>10}\")\n",
    "print()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸŸ¢ HEAD 2 ATTENTION WEIGHTS\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"Read as: How much does each word (row) attend to other words (columns)?\")\n",
    "print()\n",
    "print(f\"                    Attends to:\")\n",
    "print(f\"                    {word1:>10} {word2:>10} {word3:>10}\")\n",
    "for i, word in enumerate(words):\n",
    "    weights = [f\"{attn_weights_head2[i, j].item():.3f}\" for j in range(3)]\n",
    "    print(f\"   '{word}'  â†’   {weights[0]:>10} {weights[1]:>10} {weights[2]:>10}\")\n",
    "print()\n",
    "\n",
    "# --- Key observation ---\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ’¡ KEY OBSERVATION:\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"Notice how Head 1 and Head 2 have DIFFERENT attention patterns!\")\n",
    "print(\"This is because they have different learned weights.\")\n",
    "print(\"Each head learns to focus on different types of relationships.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "29e60724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STEP 6: APPLY ATTENTION AND CONCATENATE OUTPUTS\n",
      "======================================================================\n",
      "\n",
      "ğŸ§® How attention works:\n",
      "   Output = Attention_Weights Ã— Values\n",
      "\n",
      "   Each word's output is a weighted average of ALL words' values,\n",
      "   weighted by how much attention it pays to each word!\n",
      "\n",
      "ğŸ“Š Each head produces a 16-dimensional output per token:\n",
      "   Head 1 output: [3, 16] (3 tokens Ã— 16 dims)\n",
      "   Head 2 output: [3, 16] (3 tokens Ã— 16 dims)\n",
      "\n",
      "======================================================================\n",
      "CONCATENATING HEADS BACK TOGETHER\n",
      "======================================================================\n",
      "\n",
      "   Head 1 output (16 dims) + Head 2 output (16 dims)\n",
      "                     â†“\n",
      "            Combined output (32 dims)\n",
      "\n",
      "âœ¨ FINAL COMBINED OUTPUT: [3, 32]\n",
      "\n",
      "We started with 32 dimensions per token, and after all that\n",
      "processing through 2 attention heads, we're back to 32 dimensions!\n",
      "\n",
      "But now each token's embedding has been ENRICHED with information\n",
      "from other tokens that it paid attention to! ğŸ“\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 6: APPLY ATTENTION AND CONCATENATE OUTPUTS\n",
    "# ============================================================================\n",
    "# Each head gathers information, then we combine the results!\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STEP 6: APPLY ATTENTION AND CONCATENATE OUTPUTS\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "print(\"ğŸ§® How attention works:\")\n",
    "print(\"   Output = Attention_Weights Ã— Values\")\n",
    "print()\n",
    "print(\"   Each word's output is a weighted average of ALL words' values,\")\n",
    "print(\"   weighted by how much attention it pays to each word!\")\n",
    "print()\n",
    "\n",
    "# --- Apply attention weights to values ---\n",
    "\n",
    "# Attention weights (3Ã—3) @ Values (3Ã—16) = Output (3Ã—16)\n",
    "output_head1 = attn_weights_head1 @ V1\n",
    "output_head2 = attn_weights_head2 @ V2\n",
    "\n",
    "print(\"ğŸ“Š Each head produces a 16-dimensional output per token:\")\n",
    "print(f\"   Head 1 output: {list(output_head1.shape)} (3 tokens Ã— 16 dims)\")\n",
    "print(f\"   Head 2 output: {list(output_head2.shape)} (3 tokens Ã— 16 dims)\")\n",
    "print()\n",
    "\n",
    "# --- Concatenate the two heads back together ---\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CONCATENATING HEADS BACK TOGETHER\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "combined_output = torch.cat([output_head1, output_head2], dim=-1)  # [3, 32]\n",
    "\n",
    "print(\"   Head 1 output (16 dims) + Head 2 output (16 dims)\")\n",
    "print(\"                     â†“\")\n",
    "print(\"            Combined output (32 dims)\")\n",
    "print()\n",
    "print(f\"âœ¨ FINAL COMBINED OUTPUT: {list(combined_output.shape)}\")\n",
    "print()\n",
    "print(\"We started with 32 dimensions per token, and after all that\")\n",
    "print(\"processing through 2 attention heads, we're back to 32 dimensions!\")\n",
    "print()\n",
    "print(\"But now each token's embedding has been ENRICHED with information\")\n",
    "print(\"from other tokens that it paid attention to! ğŸ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf0f441",
   "metadata": {},
   "source": [
    "### 3.3.2 Visual Summary: What Just Happened?\n",
    "\n",
    "Let's recap what happened with our tokens \"mary\", \"had\", \"a\":\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    MULTI-HEAD ATTENTION SUMMARY                    â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                    â”‚\n",
    "â”‚  INPUT: 3 tokens, each with 32 numbers                             â”‚\n",
    "â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                          â”‚\n",
    "â”‚  \"mary\" â†’ [32 numbers]                                             â”‚\n",
    "â”‚  \"had\"  â†’ [32 numbers]                                             â”‚\n",
    "â”‚  \"a\"    â†’ [32 numbers]                                             â”‚\n",
    "â”‚                     â”‚                                              â”‚\n",
    "â”‚                     â–¼ SPLIT                                        â”‚\n",
    "â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                    â”‚\n",
    "â”‚           â–¼                   â–¼                                    â”‚\n",
    "â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚\n",
    "â”‚     â”‚  HEAD 1   â”‚       â”‚  HEAD 2   â”‚                              â”‚\n",
    "â”‚     â”‚(16 nums)  â”‚       â”‚(16 nums)  â”‚                              â”‚\n",
    "â”‚     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                              â”‚\n",
    "â”‚     â”‚ Q, K, V   â”‚       â”‚ Q, K, V   â”‚                              â”‚\n",
    "â”‚     â”‚ Attention â”‚       â”‚ Attention â”‚                              â”‚\n",
    "â”‚     â”‚ Scores    â”‚       â”‚ Scores    â”‚                              â”‚\n",
    "â”‚     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                              â”‚\n",
    "â”‚           â”‚                   â”‚                                    â”‚\n",
    "â”‚           â–¼                   â–¼                                    â”‚\n",
    "â”‚     [16 numbers]        [16 numbers]                               â”‚\n",
    "â”‚           â”‚                   â”‚                                    â”‚\n",
    "â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â”‚\n",
    "â”‚                     â–¼ CONCATENATE                                  â”‚\n",
    "â”‚                                                                    â”‚\n",
    "â”‚  OUTPUT: 3 tokens, each STILL with 32 numbers                      â”‚\n",
    "â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                          â”‚\n",
    "â”‚  \"mary\" â†’ [32 numbers] (NOW ENRICHED!)                             â”‚\n",
    "â”‚  \"had\"  â†’ [32 numbers] (NOW ENRICHED!)                             â”‚\n",
    "â”‚  \"a\"    â†’ [32 numbers] (NOW ENRICHED!)                             â”‚\n",
    "â”‚                                                                    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**ğŸ“ The Magic:**\n",
    "- Each token's new 32-number embedding now contains information from ALL other tokens!\n",
    "- The attention weights determined how much information to gather from each word\n",
    "- Head 1 and Head 2 each looked for different patterns\n",
    "- We combined their findings for a richer representation!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ff5d4b",
   "metadata": {},
   "source": [
    "### 3.4 Understanding Transformer Layers (Stacking Blocks of Intelligence!)\n",
    "\n",
    "One attention step isn't enough. We **stack multiple layers** for deeper understanding!\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ—ï¸ What is a Transformer Layer?\n",
    "\n",
    "A **transformer layer** (also called a \"block\") has TWO parts:\n",
    "\n",
    "| Part | What It Does |\n",
    "|------|-------------|\n",
    "| 1. **Multi-Head Attention** | Words look at each other and share info |\n",
    "| 2. **Feed-Forward Network** | Each word \"thinks\" independently |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š Why Stack Multiple Layers?\n",
    "\n",
    "Think of it like reading comprehension:\n",
    "\n",
    "| Layer | What It Learns | Analogy |\n",
    "|-------|---------------|---------|\n",
    "| Layer 1 | Basic patterns: \"lamb often follows little\" | Recognizing words |\n",
    "| Layer 2 | Complex patterns: \"mary â†’ lamb\" connection | Understanding sentences |\n",
    "| Layer 3+ | Abstract patterns: grammar, context | Deep comprehension |\n",
    "\n",
    "**More layers = deeper understanding!**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”„ Our Model's Architecture (n_layers = 2)\n",
    "\n",
    "```\n",
    "Input Embeddings (32 numbers per token)\n",
    "         â”‚\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚     TRANSFORMER BLOCK 1             â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\n",
    "â”‚  â”‚ Multi-Head Attention        â”‚    â”‚ â† Words share information\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\n",
    "â”‚  â”‚ Feed-Forward Network        â”‚    â”‚ â† Each word \"thinks\"\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚     TRANSFORMER BLOCK 2             â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\n",
    "â”‚  â”‚ Multi-Head Attention        â”‚    â”‚ â† Words share MORE info\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\n",
    "â”‚  â”‚ Feed-Forward Network        â”‚    â”‚ â† Each word \"thinks\" more\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "      Output â†’ Predict next word!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Š Real GPT Models\n",
    "\n",
    "| Model | Number of Layers |\n",
    "|-------|-----------------|\n",
    "| Our TinyGPT | 2 layers |\n",
    "| GPT-2 Small | 12 layers |\n",
    "| GPT-2 XL | 48 layers |\n",
    "| GPT-3 | 96 layers |\n",
    "\n",
    "More layers = more capacity to learn = more computation needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "9b82bbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "COUNTING PARAMETERS IN OUR MODEL\n",
      "======================================================================\n",
      "\n",
      "ğŸ“Š EMBEDDING LAYERS:\n",
      "   Token embeddings:    35 words Ã— 32 dims = 1,120 parameters\n",
      "   Position embeddings: 6 positions Ã— 32 dims = 192 parameters\n",
      "   Total embeddings:    1,312 parameters\n",
      "\n",
      "ğŸ”µ ATTENTION PARAMETERS (per layer):\n",
      "   Each head has Q, K, V matrices:\n",
      "     Q: 32 Ã— 16 = 512 params\n",
      "     K: 32 Ã— 16 = 512 params\n",
      "     V: 32 Ã— 16 = 512 params\n",
      "   Per head: 1536 params Ã— 2 heads = 3072 params\n",
      "   Output projection: 32 Ã— 32 = 1024 params\n",
      "   Total attention per layer: ~4,096 parameters\n",
      "\n",
      "ğŸŸ¢ FEED-FORWARD PARAMETERS (per layer):\n",
      "   Layer 1: 32 â†’ 128 = 4096 params\n",
      "   Layer 2: 128 â†’ 32 = 4096 params\n",
      "   Total feed-forward per layer: 8,192 parameters\n",
      "\n",
      "======================================================================\n",
      "ğŸ“¦ TOTAL PARAMETER COUNT\n",
      "======================================================================\n",
      "\n",
      "   Embeddings:         1,312 parameters\n",
      "   Transformer blocks: 2 layers Ã— ~12,288 = ~24,576 parameters\n",
      "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "   TOTAL:              ~25,888 parameters\n",
      "\n",
      "ğŸ’¡ KEY INSIGHT:\n",
      "   â€¢ Doubling n_layers â†’ ~2Ã— more transformer parameters\n",
      "   â€¢ Doubling n_heads â†’ SAME parameters, but more diverse attention patterns\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# COUNTING PARAMETERS: How Many Numbers Does Our Model Learn?\n",
    "# ============================================================================\n",
    "# Let's break down exactly where all the learnable parameters are!\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"COUNTING PARAMETERS IN OUR MODEL\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# --- Embedding parameters ---\n",
    "\n",
    "# Token embeddings: one row of 32 numbers for each word in vocabulary\n",
    "token_emb_params = vocab_size * embedding_dim\n",
    "\n",
    "# Position embeddings: one row of 32 numbers for each position (0-5)\n",
    "pos_emb_params = block_size * embedding_dim\n",
    "\n",
    "# Total embedding parameters\n",
    "total_emb_params = token_emb_params + pos_emb_params\n",
    "\n",
    "print(\"ğŸ“Š EMBEDDING LAYERS:\")\n",
    "print(f\"   Token embeddings:    {vocab_size} words Ã— {embedding_dim} dims = {token_emb_params:,} parameters\")\n",
    "print(f\"   Position embeddings: {block_size} positions Ã— {embedding_dim} dims = {pos_emb_params:,} parameters\")\n",
    "print(f\"   Total embeddings:    {total_emb_params:,} parameters\")\n",
    "print()\n",
    "\n",
    "# --- Attention parameters ---\n",
    "\n",
    "head_size = embedding_dim // n_heads  # 16\n",
    "\n",
    "# Q, K, V matrices: 32 â†’ 16 each\n",
    "params_per_head = 3 * (embedding_dim * head_size)\n",
    "\n",
    "# Output projection: combines heads back to 32 dims\n",
    "output_proj_params = embedding_dim * embedding_dim\n",
    "\n",
    "# Total attention per layer\n",
    "attention_params_per_layer = (params_per_head * n_heads) + output_proj_params\n",
    "\n",
    "print(\"ğŸ”µ ATTENTION PARAMETERS (per layer):\")\n",
    "print(f\"   Each head has Q, K, V matrices:\")\n",
    "print(f\"     Q: {embedding_dim} Ã— {head_size} = {embedding_dim * head_size} params\")\n",
    "print(f\"     K: {embedding_dim} Ã— {head_size} = {embedding_dim * head_size} params\")\n",
    "print(f\"     V: {embedding_dim} Ã— {head_size} = {embedding_dim * head_size} params\")\n",
    "print(f\"   Per head: {params_per_head} params Ã— {n_heads} heads = {params_per_head * n_heads} params\")\n",
    "print(f\"   Output projection: {embedding_dim} Ã— {embedding_dim} = {output_proj_params} params\")\n",
    "print(f\"   Total attention per layer: ~{attention_params_per_layer:,} parameters\")\n",
    "print()\n",
    "\n",
    "# --- Feed-forward parameters ---\n",
    "\n",
    "ff_hidden = 4 * embedding_dim  # 128 hidden neurons\n",
    "ff_params_per_layer = (embedding_dim * ff_hidden) + (ff_hidden * embedding_dim)\n",
    "\n",
    "print(\"ğŸŸ¢ FEED-FORWARD PARAMETERS (per layer):\")\n",
    "print(f\"   Layer 1: {embedding_dim} â†’ {ff_hidden} = {embedding_dim * ff_hidden} params\")\n",
    "print(f\"   Layer 2: {ff_hidden} â†’ {embedding_dim} = {ff_hidden * embedding_dim} params\")\n",
    "print(f\"   Total feed-forward per layer: {ff_params_per_layer:,} parameters\")\n",
    "print()\n",
    "\n",
    "# --- Total parameters ---\n",
    "\n",
    "params_per_layer = attention_params_per_layer + ff_params_per_layer\n",
    "total_transformer_params = params_per_layer * n_layers\n",
    "total_params = total_emb_params + total_transformer_params\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ“¦ TOTAL PARAMETER COUNT\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(f\"   Embeddings:         {total_emb_params:,} parameters\")\n",
    "print(f\"   Transformer blocks: {n_layers} layers Ã— ~{params_per_layer:,} = ~{total_transformer_params:,} parameters\")\n",
    "print(f\"   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "print(f\"   TOTAL:              ~{total_params:,} parameters\")\n",
    "print()\n",
    "\n",
    "print(\"ğŸ’¡ KEY INSIGHT:\")\n",
    "print(f\"   â€¢ Doubling n_layers â†’ ~2Ã— more transformer parameters\")\n",
    "print(f\"   â€¢ Doubling n_heads â†’ SAME parameters, but more diverse attention patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377b94c0",
   "metadata": {},
   "source": [
    "### 3.5 The Complete Architecture: All the Pieces Together!\n",
    "\n",
    "Now let's see how everything fits together into one complete model!\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ›ï¸ TinyGPT Architecture Diagram\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                         TinyGPT ARCHITECTURE                         â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                      â”‚\n",
    "â”‚  INPUT: \"mary had a little lamb <END>\"                               â”‚\n",
    "â”‚         â”‚                                                            â”‚\n",
    "â”‚         â–¼                                                            â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚\n",
    "â”‚  â”‚ ğŸ“– TOKEN EMBEDDING TABLE                                   â”‚      â”‚\n",
    "â”‚  â”‚ [vocab_size Ã— embedding_dim] = [27 Ã— 32]                   â”‚      â”‚\n",
    "â”‚  â”‚ Each word â†’ 32 numbers                                     â”‚      â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚\n",
    "â”‚         â”‚                                                            â”‚\n",
    "â”‚         â–¼ (+ added together)                                         â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚\n",
    "â”‚  â”‚ ğŸ“ POSITION EMBEDDING TABLE                                â”‚      â”‚\n",
    "â”‚  â”‚ [block_size Ã— embedding_dim] = [6 Ã— 32]                    â”‚      â”‚\n",
    "â”‚  â”‚ Each position (0,1,2,3,4,5) â†’ 32 numbers                   â”‚      â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚\n",
    "â”‚         â”‚                                                            â”‚\n",
    "â”‚         â–¼                                                            â”‚\n",
    "â”‚  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—      â”‚\n",
    "â”‚  â•‘ ğŸ”· TRANSFORMER BLOCK 1                                     â•‘      â”‚\n",
    "â”‚  â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘      â”‚\n",
    "â”‚  â•‘  â”‚ Multi-Head Attention (2 heads)                       â”‚  â•‘      â”‚\n",
    "â”‚  â•‘  â”‚ Head 1 (16 dims) + Head 2 (16 dims) â†’ 32 dims        â”‚  â•‘      â”‚\n",
    "â”‚  â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘      â”‚\n",
    "â”‚  â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘      â”‚\n",
    "â”‚  â•‘  â”‚ Feed-Forward Network                                 â”‚  â•‘      â”‚\n",
    "â”‚  â•‘  â”‚ 32 â†’ 128 â†’ 32 (expand then contract)                 â”‚  â•‘      â”‚\n",
    "â”‚  â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘      â”‚\n",
    "â”‚  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•      â”‚\n",
    "â”‚         â”‚                                                            â”‚\n",
    "â”‚         â–¼                                                            â”‚\n",
    "â”‚  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—      â”‚\n",
    "â”‚  â•‘ ğŸ”· TRANSFORMER BLOCK 2 (same structure)                    â•‘      â”‚\n",
    "â”‚  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•      â”‚\n",
    "â”‚         â”‚                                                            â”‚\n",
    "â”‚         â–¼                                                            â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚\n",
    "â”‚  â”‚ ğŸ“Š OUTPUT HEAD                                             â”‚      â”‚\n",
    "â”‚  â”‚ [embedding_dim â†’ vocab_size] = [32 â†’ 27]                   â”‚      â”‚\n",
    "â”‚  â”‚ Converts 32 numbers â†’ 27 probabilities (one per word)      â”‚      â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚\n",
    "â”‚         â”‚                                                            â”‚\n",
    "â”‚         â–¼                                                            â”‚\n",
    "â”‚  OUTPUT: \"The next word is probably 'lamb' (85% confidence)\"         â”‚\n",
    "â”‚                                                                      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‹ Quick Reference: Our Hyperparameters\n",
    "\n",
    "| Parameter | Value | What It Controls |\n",
    "|-----------|-------|-----------------|\n",
    "| `vocab_size` | 27 | Number of unique words we can predict |\n",
    "| `embedding_dim` | 32 | Size of each word's number-vector |\n",
    "| `n_heads` | 2 | Parallel attention patterns |\n",
    "| `n_layers` | 2 | Stacked transformer blocks |\n",
    "| `block_size` | 6 | How many words of context at once |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b3be46",
   "metadata": {},
   "source": [
    "## Part 4: Data Loading - Creating Training Batches\n",
    "\n",
    "Now we need to prepare our data for training!\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ The Training Task\n",
    "\n",
    "GPT learns by predicting the **next word**. We show it:\n",
    "- **Input**: A sequence of words\n",
    "- **Target**: The same sequence, but shifted by 1 (the \"next words\")\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ Example\n",
    "\n",
    "If our sentence is: `\"mary had a little lamb <END>\"`\n",
    "\n",
    "| Position | Input (what model sees) | Target (what it should predict) |\n",
    "|----------|------------------------|--------------------------------|\n",
    "| 0 | \"mary\" | \"had\" |\n",
    "| 1 | \"had\" | \"a\" |\n",
    "| 2 | \"a\" | \"little\" |\n",
    "| 3 | \"little\" | \"lamb\" |\n",
    "| 4 | \"lamb\" | \"<END>\" |\n",
    "\n",
    "At each position, the model tries to guess the next word!\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“¦ Batches\n",
    "\n",
    "We don't train on just one sequence at a time - that's slow!\n",
    "\n",
    "Instead, we create **batches** - multiple sequences processed together.\n",
    "\n",
    "| Batch Item | Input (6 words) | Target (6 words) |\n",
    "|------------|-----------------|------------------|\n",
    "| Sequence 1 | \"mary had a little lamb <END>\" | \"had a little lamb <END> little\" |\n",
    "| Sequence 2 | \"lamb little lamb mary had a\" | \"little lamb mary had a little\" |\n",
    "| ... | ... | ... |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "c1d653b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXAMPLE: A Training Batch\n",
      "======================================================================\n",
      "\n",
      "ğŸ“Š Batch shape:\n",
      "   x (input):  [2, 6] (2 sequences Ã— 6 tokens each)\n",
      "   y (target): [2, 6] (2 sequences Ã— 6 tokens each)\n",
      "\n",
      "======================================================================\n",
      "SEQUENCE 1:\n",
      "======================================================================\n",
      "\n",
      "Input tokens (what the model sees):\n",
      "   Token IDs: [2, 32, 22, 6, 32, 22]\n",
      "   Words:     that mary went <END> mary went\n",
      "\n",
      "Target tokens (what the model should predict):\n",
      "   Token IDs: [32, 22, 6, 32, 22, 32]\n",
      "   Words:     mary went <END> mary went mary\n",
      "\n",
      "ğŸ’¡ Notice: The target is the input shifted by 1!\n",
      "   At each position, y tells us what the NEXT word should be.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FUNCTION TO CREATE TRAINING BATCHES\n",
    "# ============================================================================\n",
    "# This function grabs random chunks of our text for training.\n",
    "\n",
    "def get_batch(batch_size=16):\n",
    "    \"\"\"\n",
    "    Create a random batch of training examples.\n",
    "    \n",
    "    Each example is a sequence of tokens and its targets\n",
    "    (the same sequence shifted by 1 position).\n",
    "    \n",
    "    Args:\n",
    "        batch_size: Number of sequences per batch.\n",
    "    \n",
    "    Returns:\n",
    "        x: Input sequences [batch_size, block_size]\n",
    "        y: Target sequences [batch_size, block_size]\n",
    "    \"\"\"\n",
    "    # Pick random starting positions\n",
    "    max_start = len(data) - block_size - 1\n",
    "    ix = torch.randint(max_start, (batch_size,))\n",
    "    \n",
    "    # Create input sequences\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    \n",
    "    # Create target sequences (shifted by 1)\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "# --- Example batch ---\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EXAMPLE: A Training Batch\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Create a tiny batch of 2 sequences\n",
    "x_example, y_example = get_batch(batch_size=2)\n",
    "\n",
    "print(f\"ğŸ“Š Batch shape:\")\n",
    "print(f\"   x (input):  {list(x_example.shape)} ({x_example.shape[0]} sequences Ã— {x_example.shape[1]} tokens each)\")\n",
    "print(f\"   y (target): {list(y_example.shape)} ({y_example.shape[0]} sequences Ã— {y_example.shape[1]} tokens each)\")\n",
    "print()\n",
    "\n",
    "# --- Show the first sequence ---\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SEQUENCE 1:\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"Input tokens (what the model sees):\")\n",
    "print(f\"   Token IDs: {x_example[0].tolist()}\")\n",
    "print(f\"   Words:     {' '.join([idx2word[int(i)] for i in x_example[0]])}\")\n",
    "print()\n",
    "print(\"Target tokens (what the model should predict):\")\n",
    "print(f\"   Token IDs: {y_example[0].tolist()}\")\n",
    "print(f\"   Words:     {' '.join([idx2word[int(i)] for i in y_example[0]])}\")\n",
    "print()\n",
    "\n",
    "print(\"ğŸ’¡ Notice: The target is the input shifted by 1!\")\n",
    "print(\"   At each position, y tells us what the NEXT word should be.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79661dac",
   "metadata": {},
   "source": [
    "## Part 5: Self-Attention - The Heart of Transformers â¤ï¸\n",
    "\n",
    "Now we build the core component of GPT: **Self-Attention**!\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¤” What is Self-Attention?\n",
    "\n",
    "Self-attention lets each word **look at all other words** and decide which ones are most important for predicting the next word.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”‘ The Three Magic Components: Q, K, V\n",
    "\n",
    "Every word computes three things:\n",
    "\n",
    "| Component | What It Means | Analogy |\n",
    "|-----------|--------------|---------|\n",
    "| **Q (Query)** | \"What am I looking for?\" | A question you ask |\n",
    "| **K (Key)** | \"What do I contain?\" | A label describing what you have |\n",
    "| **V (Value)** | \"What info should I share?\" | The actual information |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ How Attention Works\n",
    "\n",
    "For each word, we:\n",
    "1. Compute its Query\n",
    "2. Compare Query to all Keys (get attention scores)\n",
    "3. Use scores to create a weighted average of all Values\n",
    "\n",
    "```\n",
    "                     \"a\"   is looking at other words\n",
    "                      â”‚\n",
    "                      â–¼\n",
    "          Q(\"a\") Ã— K(\"mary\") = score for \"mary\"\n",
    "          Q(\"a\") Ã— K(\"had\")  = score for \"had\"\n",
    "          Q(\"a\") Ã— K(\"a\")    = score for \"a\" (itself)\n",
    "                      â”‚\n",
    "                      â–¼\n",
    "          Apply softmax â†’ [0.3, 0.5, 0.2]  (sum to 1)\n",
    "                      â”‚\n",
    "                      â–¼\n",
    "          Output = 0.3Ã—V(\"mary\") + 0.5Ã—V(\"had\") + 0.2Ã—V(\"a\")\n",
    "```\n",
    "\n",
    "The word \"a\" now contains information from \"mary\", \"had\", and itself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "064b773c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TESTING SELF-ATTENTION HEAD\n",
      "======================================================================\n",
      "\n",
      "ğŸ“¥ Input shape:  [2, 6, 32]\n",
      "   (2 sequences, 6 tokens, 32 dims)\n",
      "\n",
      "ğŸ“¤ Output shape: [2, 6, 16]\n",
      "   (2 sequences, 6 tokens, 16 dims)\n",
      "\n",
      "âœ… The attention head transformed each token while letting them\n",
      "   share information with each other!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SELF-ATTENTION HEAD: The Core of How Words Communicate!\n",
    "# ============================================================================\n",
    "# This is the most important class - it's how words share information.\n",
    "\n",
    "class SelfAttentionHead(nn.Module):\n",
    "    \"\"\"\n",
    "    A single head of self-attention.\n",
    "    \n",
    "    This lets each word look at other words and decide which are important.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim, block_size, head_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Q, K, V projection layers\n",
    "        self.key = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "        self.query = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "        self.value = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "        \n",
    "        # Causal mask: prevents looking at future words\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        # Compute Key and Query\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        wei = q @ k.transpose(-2, -1) / (C ** 0.5)\n",
    "        \n",
    "        # Apply causal mask\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        \n",
    "        # Convert to probabilities\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        \n",
    "        return out\n",
    "\n",
    "# --- Test the attention head ---\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TESTING SELF-ATTENTION HEAD\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "test_head = SelfAttentionHead(embedding_dim, block_size, head_size=16)\n",
    "test_input = torch.randn(2, block_size, embedding_dim)\n",
    "test_output = test_head(test_input)\n",
    "\n",
    "print(f\"ğŸ“¥ Input shape:  {list(test_input.shape)}\")\n",
    "print(f\"   (2 sequences, {block_size} tokens, {embedding_dim} dims)\")\n",
    "print()\n",
    "print(f\"ğŸ“¤ Output shape: {list(test_output.shape)}\")\n",
    "print(f\"   (2 sequences, {block_size} tokens, 16 dims)\")\n",
    "print()\n",
    "print(\"âœ… The attention head transformed each token while letting them\")\n",
    "print(\"   share information with each other!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857acd73",
   "metadata": {},
   "source": [
    "### ğŸ­ The Causal Mask: No Cheating!\n",
    "\n",
    "There's a SUPER important rule in GPT: **you can't look at the future!**\n",
    "\n",
    "When predicting the next word after \"mary had a\", we can't peek at what comes later.\n",
    "\n",
    "The **causal mask** enforces this rule!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "633bc476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "THE CAUSAL MASK (No Peeking at the Future!)\n",
      "======================================================================\n",
      "\n",
      "Causal Mask (1 = CAN attend, 0 = CANNOT attend):\n",
      "\n",
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n",
      "\n",
      "======================================================================\n",
      "INTERPRETATION:\n",
      "======================================================================\n",
      "\n",
      "Each ROW represents a word position.\n",
      "1s show which words that position CAN look at.\n",
      "\n",
      "Position 0 (word 1): [1 0 0 0 0 0] â†’ Can only see itself\n",
      "Position 1 (word 2): [1 1 0 0 0 0] â†’ Can see words 1-2\n",
      "Position 2 (word 3): [1 1 1 0 0 0] â†’ Can see words 1-3\n",
      "Position 3 (word 4): [1 1 1 1 0 0] â†’ Can see words 1-4\n",
      "Position 4 (word 5): [1 1 1 1 1 0] â†’ Can see words 1-5\n",
      "Position 5 (word 6): [1 1 1 1 1 1] â†’ Can see ALL words 1-6\n",
      "\n",
      "ğŸ’¡ This is why GPT predicts LEFT-TO-RIGHT!\n",
      "   Each word only uses information from previous words.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZING THE CAUSAL MASK\n",
    "# ============================================================================\n",
    "# The mask is a triangular matrix that controls what each word can see.\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"THE CAUSAL MASK (No Peeking at the Future!)\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# --- Create the mask ---\n",
    "\n",
    "mask = torch.tril(torch.ones(block_size, block_size))\n",
    "\n",
    "print(\"Causal Mask (1 = CAN attend, 0 = CANNOT attend):\")\n",
    "print()\n",
    "print(mask)\n",
    "print()\n",
    "\n",
    "# --- Interpret the mask ---\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"INTERPRETATION:\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"Each ROW represents a word position.\")\n",
    "print(\"1s show which words that position CAN look at.\")\n",
    "print()\n",
    "print(\"Position 0 (word 1): [1 0 0 0 0 0] â†’ Can only see itself\")\n",
    "print(\"Position 1 (word 2): [1 1 0 0 0 0] â†’ Can see words 1-2\")\n",
    "print(\"Position 2 (word 3): [1 1 1 0 0 0] â†’ Can see words 1-3\")\n",
    "print(\"Position 3 (word 4): [1 1 1 1 0 0] â†’ Can see words 1-4\")\n",
    "print(\"Position 4 (word 5): [1 1 1 1 1 0] â†’ Can see words 1-5\")\n",
    "print(\"Position 5 (word 6): [1 1 1 1 1 1] â†’ Can see ALL words 1-6\")\n",
    "print()\n",
    "print(\"ğŸ’¡ This is why GPT predicts LEFT-TO-RIGHT!\")\n",
    "print(\"   Each word only uses information from previous words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e43ec1",
   "metadata": {},
   "source": [
    "## Part 6: Multi-Head Attention (Multiple Perspectives!)\n",
    "\n",
    "One attention head is good, but **multiple heads are better!**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¤” Why Multiple Heads?\n",
    "\n",
    "Different heads can learn to focus on different things:\n",
    "\n",
    "| Head | What It Might Learn |\n",
    "|------|---------------------|\n",
    "| Head 1 | Subject-verb relationships (\"mary\" â†’ \"went\") |\n",
    "| Head 2 | Adjective-noun relationships (\"little\" â†’ \"lamb\") |\n",
    "| Head 3 | Positional patterns (nearby words) |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”§ How It Works\n",
    "\n",
    "1. **Split** the embedding into chunks (one per head)\n",
    "2. **Run** each head independently  \n",
    "3. **Concatenate** all outputs back together\n",
    "\n",
    "```\n",
    "   Input: 32 dimensions\n",
    "           â†“\n",
    "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   â–¼               â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”\n",
    "â”‚Head 1â”‚       â”‚Head 2â”‚\n",
    "â”‚16 dimâ”‚       â”‚16 dimâ”‚\n",
    "â””â”€â”€â”¬â”€â”€â”€â”˜       â””â”€â”€â”¬â”€â”€â”€â”˜\n",
    "   â–¼               â–¼\n",
    "   16 dims      16 dims\n",
    "   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "           â–¼\n",
    "   Concatenate â†’ 32 dimensions\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "b2654932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TESTING MULTI-HEAD ATTENTION\n",
      "======================================================================\n",
      "\n",
      "ğŸ“¥ Input shape:  [2, 6, 32]\n",
      "ğŸ“¤ Output shape: [2, 6, 32]\n",
      "\n",
      "ğŸ’¡ With 2 heads, each head processes 16 dimensions\n",
      "   All heads' outputs are concatenated and projected back to 32 dims\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MULTI-HEAD ATTENTION: Multiple Attention Heads Working Together\n",
    "# ============================================================================\n",
    "# Multiple attention heads let the model look at relationships in different ways.\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multiple attention heads running in parallel.\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim, block_size, num_heads):\n",
    "        super().__init__()\n",
    "        \n",
    "        head_size = embedding_dim // num_heads\n",
    "        \n",
    "        # Create multiple attention heads\n",
    "        self.heads = nn.ModuleList([\n",
    "            SelfAttentionHead(embedding_dim, block_size, head_size) \n",
    "            for _ in range(num_heads)\n",
    "        ])\n",
    "        \n",
    "        # Output projection\n",
    "        self.proj = nn.Linear(num_heads * head_size, embedding_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Run all heads and concatenate\n",
    "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        return self.proj(out)\n",
    "\n",
    "# --- Test multi-head attention ---\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TESTING MULTI-HEAD ATTENTION\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "test_mha = MultiHeadAttention(embedding_dim, block_size, n_heads)\n",
    "test_output = test_mha(test_input)\n",
    "\n",
    "print(f\"ğŸ“¥ Input shape:  {list(test_input.shape)}\")\n",
    "print(f\"ğŸ“¤ Output shape: {list(test_output.shape)}\")\n",
    "print()\n",
    "print(f\"ğŸ’¡ With {n_heads} heads, each head processes {embedding_dim // n_heads} dimensions\")\n",
    "print(\"   All heads' outputs are concatenated and projected back to 32 dims\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fda5fe3",
   "metadata": {},
   "source": [
    "## Part 7: Feed-Forward Network (Independent Thinking!)\n",
    "\n",
    "After words share information through attention, each word needs to **\"think\"** about what it learned.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  What is the Feed-Forward Network?\n",
    "\n",
    "It's a simple neural network (just like our SimpleNetwork!) that processes each word INDEPENDENTLY.\n",
    "\n",
    "**Structure:**\n",
    "1. **Expand**: 32 dims â†’ 128 dims (more room to think!)\n",
    "2. **ReLU**: Add non-linearity\n",
    "3. **Compress**: 128 dims â†’ 32 dims (back to original)\n",
    "\n",
    "```\n",
    "Token embedding\n",
    "(32 numbers)\n",
    "     â”‚\n",
    "     â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Linear       â”‚\n",
    "â”‚  32 â†’ 128      â”‚ â† Expand\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "     â”‚\n",
    "     â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚     ReLU       â”‚ â† Add curves\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "     â”‚\n",
    "     â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Linear       â”‚\n",
    "â”‚  128 â†’ 32      â”‚ â† Compress back\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "     â”‚\n",
    "     â–¼\n",
    "Token embedding\n",
    "(32 numbers, transformed!)\n",
    "```\n",
    "\n",
    "**Key insight**: The same feed-forward network is applied to EVERY token separately!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "df5cdffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TESTING FEED-FORWARD NETWORK\n",
      "======================================================================\n",
      "\n",
      "ğŸ“¥ Input shape:  [2, 6, 32]\n",
      "ğŸ“¤ Output shape: [2, 6, 32]\n",
      "\n",
      "ğŸ’¡ Inside the network:\n",
      "   32 dims â†’ 128 dims (expand) â†’ ReLU â†’ 32 dims (compress)\n",
      "   Intermediate dimension: 128\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FEED-FORWARD NETWORK: Independent Thinking for Each Word\n",
    "# ============================================================================\n",
    "# A simple 2-layer network applied to each token independently.\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Feed-forward network: 32 â†’ 128 â†’ 32 with ReLU.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),   # Expand: 32 â†’ 128\n",
    "            nn.ReLU(),                        # Activation\n",
    "            nn.Linear(4 * n_embd, n_embd)    # Compress: 128 â†’ 32\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# --- Test feed-forward network ---\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TESTING FEED-FORWARD NETWORK\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "test_ff = FeedForward(embedding_dim)\n",
    "test_output = test_ff(test_input)\n",
    "\n",
    "print(f\"ğŸ“¥ Input shape:  {list(test_input.shape)}\")\n",
    "print(f\"ğŸ“¤ Output shape: {list(test_output.shape)}\")\n",
    "print()\n",
    "print(f\"ğŸ’¡ Inside the network:\")\n",
    "print(f\"   32 dims â†’ 128 dims (expand) â†’ ReLU â†’ 32 dims (compress)\")\n",
    "print(f\"   Intermediate dimension: {4 * embedding_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cbfb35",
   "metadata": {},
   "source": [
    "## Part 8: Transformer Block (Putting Attention + FeedForward Together!)\n",
    "\n",
    "Now we combine multi-head attention and feed-forward into a single **Transformer Block**.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§± What's in a Block?\n",
    "\n",
    "```\n",
    "          INPUT (32 dims per word)\n",
    "             â”‚\n",
    "             â–¼\n",
    "     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "     â”‚  Layer Norm   â”‚  â† Normalize for stability\n",
    "     â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "             â”‚\n",
    "             â–¼\n",
    "     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "     â”‚  Multi-Head   â”‚  â† Words share information\n",
    "     â”‚   Attention   â”‚\n",
    "     â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "             â”‚\n",
    "     â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”\n",
    "     â”‚    + INPUT    â”‚  â† Residual connection (add original)\n",
    "     â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "             â”‚\n",
    "             â–¼\n",
    "     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "     â”‚  Layer Norm   â”‚  â† Normalize again\n",
    "     â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "             â”‚\n",
    "             â–¼\n",
    "     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "     â”‚ Feed-Forward  â”‚  â† Each word \"thinks\"\n",
    "     â”‚   Network     â”‚\n",
    "     â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "             â”‚\n",
    "     â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”\n",
    "     â”‚    + INPUT    â”‚  â† Residual connection again\n",
    "     â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "             â”‚\n",
    "             â–¼\n",
    "          OUTPUT (32 dims per word)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”— What's a \"Residual Connection\"?\n",
    "\n",
    "It's just adding the input back to the output: `output = input + layer(input)`\n",
    "\n",
    "**Why?** It helps gradients flow during training. Without residuals, deep networks are hard to train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "46477462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TESTING THE TRANSFORMER BLOCK\n",
      "============================================================\n",
      "\n",
      "ğŸ“¥ Input shape:  torch.Size([2, 6, 32])\n",
      "ğŸ“¤ Output shape: torch.Size([2, 6, 32])\n",
      "\n",
      "âœ… The Transformer block preserves the shape!\n",
      "   - Same batch size (2)\n",
      "   - Same sequence length (6 words)\n",
      "   - Same embedding dimension (32 features per word)\n",
      "\n",
      "But the OUTPUT contains richer information because:\n",
      "   1. Each word has 'heard' from previous words (attention)\n",
      "   2. Each word has 'thought' about the information (feed-forward)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TRANSFORMER BLOCK: Combines Attention + Feed-Forward\n",
    "# ============================================================================\n",
    "# This is the basic building block of GPT!\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    A single Transformer block.\n",
    "    \n",
    "    Components:\n",
    "        1. Multi-head attention: words share information\n",
    "        2. Feed-forward network: each word processes info\n",
    "        3. Layer normalization: keeps numbers stable\n",
    "        4. Residual connections: helps training\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim, block_size, n_heads):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Attention and feed-forward layers\n",
    "        self.sa = MultiHeadAttention(embedding_dim, block_size, n_heads)\n",
    "        self.ffwd = FeedForward(embedding_dim)\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.ln1 = nn.LayerNorm(embedding_dim)\n",
    "        self.ln2 = nn.LayerNorm(embedding_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Attention with residual connection\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        \n",
    "        # Feed-forward with residual connection\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "# --- Test the transformer block ---\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TESTING THE TRANSFORMER BLOCK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_block = Block(embedding_dim, block_size, n_heads)\n",
    "test_output = test_block(test_input)\n",
    "\n",
    "print(f\"\\nğŸ“¥ Input shape:  {test_input.shape}\")\n",
    "print(f\"ğŸ“¤ Output shape: {test_output.shape}\")\n",
    "print(\"\\nâœ… The Transformer block preserves the shape!\")\n",
    "print(\"   - Same batch size (2)\")\n",
    "print(\"   - Same sequence length (6 words)\")\n",
    "print(\"   - Same embedding dimension (32 features per word)\")\n",
    "print(\"\\nBut the OUTPUT contains richer information because:\")\n",
    "print(\"   1. Each word has 'heard' from previous words (attention)\")\n",
    "print(\"   2. Each word has 'thought' about the information (feed-forward)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6689bb3",
   "metadata": {},
   "source": [
    "## Part 9: The Complete GPT Model ğŸ‰\n",
    "\n",
    "Now we put EVERYTHING together into a complete GPT model!\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ—ï¸ Full Architecture Overview\n",
    "\n",
    "```\n",
    "     Token IDs (e.g., [3, 1, 17, 23, 5, 0])\n",
    "              â”‚\n",
    "              â–¼\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚ Token Embedding â”‚  â† Look up 32 numbers per word\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "              â”‚\n",
    "              + (add together)\n",
    "              â”‚\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚Position Embeddingâ”‚  â† Look up 32 numbers per position\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "              â”‚\n",
    "              â–¼\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚Transformer Block 1â”‚  â† Attention + Feed-Forward\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "              â”‚\n",
    "              â–¼\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚Transformer Block 2â”‚  â† Attention + Feed-Forward again!\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "              â”‚\n",
    "              â–¼\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚   Layer Norm    â”‚  â† Final normalization\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "              â”‚\n",
    "              â–¼\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚ Output Linear   â”‚  â† Convert 32 numbers â†’ 27 numbers\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜   (one score per possible next word)\n",
    "              â”‚\n",
    "              â–¼\n",
    "        LOGITS (27 scores)\n",
    "        \"Which word comes next?\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§® What Are Logits?\n",
    "\n",
    "**Logits** are the raw scores the model outputs for each possible next word.\n",
    "\n",
    "Example for a 5-word vocabulary:\n",
    "```\n",
    "logits = [2.3, -0.5, 1.1, 4.2, 0.8]\n",
    "          â”‚     â”‚     â”‚     â”‚     â”‚\n",
    "          a     b     c     d     e\n",
    "\n",
    "The model thinks \"d\" is most likely (highest score: 4.2)!\n",
    "```\n",
    "\n",
    "We convert logits to probabilities using **softmax** during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "2ca79fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CREATING TINYGPT MODEL\n",
      "============================================================\n",
      "\n",
      "âœ… TinyGPT Model Created!\n",
      "ğŸ“Š Total parameters: 27,747\n",
      "\n",
      "ğŸ—ï¸ Model Architecture:\n",
      "----------------------------------------\n",
      "TinyGPT(\n",
      "  (token_embedding): Embedding(35, 32)\n",
      "  (position_embedding): Embedding(6, 32)\n",
      "  (blocks): Sequential(\n",
      "    (0): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-1): 2 x SelfAttentionHead(\n",
      "            (key): Linear(in_features=32, out_features=16, bias=False)\n",
      "            (query): Linear(in_features=32, out_features=16, bias=False)\n",
      "            (value): Linear(in_features=32, out_features=16, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "      )\n",
      "      (ffwd): FeedForward(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (1): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-1): 2 x SelfAttentionHead(\n",
      "            (key): Linear(in_features=32, out_features=16, bias=False)\n",
      "            (query): Linear(in_features=32, out_features=16, bias=False)\n",
      "            (value): Linear(in_features=32, out_features=16, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "      )\n",
      "      (ffwd): FeedForward(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "  (head): Linear(in_features=32, out_features=35, bias=True)\n",
      ")\n",
      "----------------------------------------\n",
      "\n",
      "This tiny model has the same architecture as ChatGPT,\n",
      "just with much smaller dimensions!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TINYGPT: THE COMPLETE GPT MODEL\n",
    "# ============================================================================\n",
    "# This is where everything comes together!\n",
    "# Architecture: Token Embed â†’ Position Embed â†’ Transformer Blocks â†’ Output\n",
    "\n",
    "class TinyGPT(nn.Module):\n",
    "    \"\"\"\n",
    "    A tiny GPT model that can learn to predict the next word.\n",
    "    \n",
    "    Architecture:\n",
    "        1. Token Embedding: word IDs â†’ 32-dim vectors\n",
    "        2. Position Embedding: positions â†’ 32-dim vectors  \n",
    "        3. Transformer Blocks: attention + feed-forward\n",
    "        4. Output Head: 32 dims â†’ vocabulary scores\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Token embedding: word ID â†’ 32 numbers\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Position embedding: position â†’ 32 numbers\n",
    "        self.position_embedding = nn.Embedding(block_size, embedding_dim)\n",
    "        \n",
    "        # Stack of transformer blocks\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(embedding_dim, block_size, n_heads) for _ in range(n_layers)]\n",
    "        )\n",
    "        \n",
    "        # Final layer normalization\n",
    "        self.ln_f = nn.LayerNorm(embedding_dim)\n",
    "        \n",
    "        # Output projection: 32 â†’ vocab_size\n",
    "        self.head = nn.Linear(embedding_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"\n",
    "        Forward pass: word IDs â†’ prediction scores.\n",
    "        \n",
    "        Args:\n",
    "            idx: Word IDs [Batch, Sequence]\n",
    "            targets: Correct next words (for training)\n",
    "        \n",
    "        Returns:\n",
    "            logits: Prediction scores [Batch, Sequence, vocab_size]\n",
    "            loss: How wrong we are (only if targets provided)\n",
    "        \"\"\"\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        # Step 1: Get token embeddings\n",
    "        tok_emb = self.token_embedding(idx)\n",
    "        \n",
    "        # Step 2: Get position embeddings\n",
    "        pos_emb = self.position_embedding(torch.arange(T, device=idx.device))\n",
    "        \n",
    "        # Step 3: Combine embeddings\n",
    "        x = tok_emb + pos_emb\n",
    "        \n",
    "        # Step 4: Pass through transformer blocks\n",
    "        x = self.blocks(x)\n",
    "        \n",
    "        # Step 5: Final normalization\n",
    "        x = self.ln_f(x)\n",
    "        \n",
    "        # Step 6: Get prediction scores\n",
    "        logits = self.head(x)\n",
    "        \n",
    "        # Step 7: Calculate loss if training\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits_flat = logits.view(B * T, C)\n",
    "            targets_flat = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \"\"\"\n",
    "        Generate new words one at a time.\n",
    "        \n",
    "        Args:\n",
    "            idx: Starting word IDs [Batch, Sequence]\n",
    "            max_new_tokens: How many new words to generate\n",
    "        \n",
    "        Returns:\n",
    "            Extended sequence with generated words\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Limit context to block_size\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            \n",
    "            # Get predictions\n",
    "            logits, _ = self(idx_cond)\n",
    "            \n",
    "            # Focus on last position\n",
    "            logits = logits[:, -1, :]\n",
    "            \n",
    "            # Convert to probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            \n",
    "            # Sample next word\n",
    "            next_idx = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Add to sequence\n",
    "            idx = torch.cat((idx, next_idx), dim=1)\n",
    "        \n",
    "        return idx\n",
    "\n",
    "# --- Create the model ---\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CREATING TINYGPT MODEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "model = TinyGPT()\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"\\nâœ… TinyGPT Model Created!\")\n",
    "print(f\"ğŸ“Š Total parameters: {num_params:,}\")\n",
    "print(f\"\\nğŸ—ï¸ Model Architecture:\")\n",
    "print(\"-\" * 40)\n",
    "print(model)\n",
    "print(\"-\" * 40)\n",
    "print(\"\\nThis tiny model has the same architecture as ChatGPT,\")\n",
    "print(\"just with much smaller dimensions!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644c9198",
   "metadata": {},
   "source": [
    "## Part 10: Training the Model ğŸ‹ï¸\n",
    "\n",
    "Now we teach our GPT to predict words! Training is a loop:\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”„ The Training Loop (What Happens Each Step)\n",
    "\n",
    "```\n",
    "      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "      â”‚                 TRAINING LOOP                    â”‚\n",
    "      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                           â”‚\n",
    "                           â–¼\n",
    "      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "      â”‚ 1. GET A BATCH                                    â”‚\n",
    "      â”‚    Pick random chunks from our training data      â”‚\n",
    "      â”‚    Input:  \"mary had a little lamb whose\"        â”‚\n",
    "      â”‚    Target: \"had a little lamb whose fleece\"      â”‚\n",
    "      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                           â”‚\n",
    "                           â–¼\n",
    "      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "      â”‚ 2. FORWARD PASS                                   â”‚\n",
    "      â”‚    Run input through the model                    â”‚\n",
    "      â”‚    Get predictions + calculate loss              â”‚\n",
    "      â”‚    \"How wrong are we?\"                           â”‚\n",
    "      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                           â”‚\n",
    "                           â–¼\n",
    "      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "      â”‚ 3. BACKWARD PASS                                  â”‚\n",
    "      â”‚    Calculate gradients using backpropagation      â”‚\n",
    "      â”‚    \"Which way should each weight move?\"          â”‚\n",
    "      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                           â”‚\n",
    "                           â–¼\n",
    "      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "      â”‚ 4. UPDATE WEIGHTS                                 â”‚\n",
    "      â”‚    Adjust weights in the direction that           â”‚\n",
    "      â”‚    reduces the loss (gradient descent)            â”‚\n",
    "      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                           â”‚\n",
    "                           â–¼\n",
    "                   Repeat 1500 times!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ AdamW Optimizer\n",
    "\n",
    "We use **AdamW**, the standard optimizer for transformers. It's smarter than basic gradient descent:\n",
    "- **Momentum**: Keeps moving in the same direction (like a ball rolling downhill)\n",
    "- **Adaptive learning rates**: Different learning rates for different parameters\n",
    "- **Weight decay**: Prevents weights from getting too large (regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "c0c3e452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ‹ï¸ STARTING TRAINING\n",
      "============================================================\n",
      "ğŸ“Š Epochs (training steps): 1500\n",
      "ğŸ“ˆ Learning rate: 0.001\n",
      "ğŸ“¦ Batch size: 4 sequences per step\n",
      "ğŸ”¢ Sequence length: 6 words\n",
      "------------------------------------------------------------\n",
      "Step | Loss (lower is better)\n",
      "------------------------------------------------------------\n",
      "   0  | 3.7665\n",
      " 300  | 0.4061\n",
      " 600  | 0.3361\n",
      " 900  | 0.2343\n",
      "1200  | 0.2704\n",
      "------------------------------------------------------------\n",
      "âœ… Training complete!\n",
      "ğŸ“‰ Final loss: 0.2198\n",
      "\n",
      "The model has now learned patterns from Mary Had a Little Lamb!\n",
      "Let's see what it can generate...\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TRAINING LOOP: TEACH THE MODEL!\n",
    "# ============================================================================\n",
    "# This is where learning happens! We repeat these steps 1500 times:\n",
    "#   1. Get a batch of training examples\n",
    "#   2. Make predictions (forward pass)\n",
    "#   3. Calculate how wrong we are (loss)\n",
    "#   4. Calculate gradients (backward pass)\n",
    "#   5. Update weights (optimizer step)\n",
    "\n",
    "# --- Create the optimizer ---\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ‹ï¸ STARTING TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ğŸ“Š Epochs (training steps): {epochs}\")\n",
    "print(f\"ğŸ“ˆ Learning rate: {lr}\")\n",
    "print(f\"ğŸ“¦ Batch size: 4 sequences per step\")\n",
    "print(f\"ğŸ”¢ Sequence length: {block_size} words\")\n",
    "print(\"-\" * 60)\n",
    "print(\"Step | Loss (lower is better)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# --- The training loop ---\n",
    "\n",
    "for step in range(epochs):\n",
    "    # STEP 1: Get a batch of training data\n",
    "    xb, yb = get_batch()\n",
    "    \n",
    "    # STEP 2: Forward pass - make predictions\n",
    "    logits, loss = model(xb, yb)\n",
    "    \n",
    "    # STEP 3: Zero out old gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # STEP 4: Backward pass - calculate gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # STEP 5: Update weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print progress every 300 steps\n",
    "    if step % 300 == 0:\n",
    "        print(f\"{step:4d}  | {loss.item():.4f}\")\n",
    "\n",
    "# --- Training complete ---\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"âœ… Training complete!\")\n",
    "print(f\"ğŸ“‰ Final loss: {loss.item():.4f}\")\n",
    "print()\n",
    "print(\"The model has now learned patterns from Mary Had a Little Lamb!\")\n",
    "print(\"Let's see what it can generate...\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0480d22c",
   "metadata": {},
   "source": [
    "## Part 11: Generating Text! ğŸ¨\n",
    "\n",
    "Now the fun part - let's make our model generate nursery rhyme text!\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”® How Text Generation Works\n",
    "\n",
    "```\n",
    "    Start: \"mary\"\n",
    "       â”‚\n",
    "       â–¼\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚     GPT Model   â”‚ â†’ Predicts \"had\" is most likely next\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "             â”‚\n",
    "             â–¼\n",
    "    Sequence is now: \"mary had\"\n",
    "       â”‚\n",
    "       â–¼\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚     GPT Model   â”‚ â†’ Predicts \"a\" is most likely next\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "             â”‚\n",
    "             â–¼\n",
    "    Sequence is now: \"mary had a\"\n",
    "       â”‚\n",
    "       â–¼\n",
    "    ... and so on!\n",
    "```\n",
    "\n",
    "This is called **autoregressive generation** - each new word becomes part of the context for the next prediction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "4f06daed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ¨ TEXT GENERATION\n",
      "============================================================\n",
      "\n",
      "ğŸš€ Starting word: 'mary'\n",
      "â³ Generating text...\n",
      "\n",
      "============================================================\n",
      "ğŸ“ GENERATED TEXT:\n",
      "============================================================\n",
      "\n",
      "   mary went <END> the lamb was sure to go <END> it followed her to school one\n",
      "\n",
      "============================================================\n",
      "\n",
      "ğŸ’¡ Note: The model learned from 'Mary Had a Little Lamb'\n",
      "   so it should generate text that sounds like the rhyme!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TEXT GENERATION: LET'S SEE WHAT OUR MODEL LEARNED!\n",
    "# ============================================================================\n",
    "# We give the model a starting word and let it generate the rest!\n",
    "\n",
    "# --- Set model to evaluation mode ---\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# --- Choose a starting word ---\n",
    "\n",
    "start_word = \"mary\"\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ¨ TEXT GENERATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nğŸš€ Starting word: '{start_word}'\")\n",
    "print(\"â³ Generating text...\\n\")\n",
    "\n",
    "# --- Prepare the input ---\n",
    "\n",
    "context = torch.tensor([[word2idx[start_word]]], dtype=torch.long)\n",
    "\n",
    "# --- Generate new tokens ---\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated = model.generate(context, max_new_tokens=15)\n",
    "\n",
    "# --- Convert back to words ---\n",
    "\n",
    "generated_text = \" \".join([idx2word[int(i)] for i in generated[0]])\n",
    "\n",
    "# --- Print the result ---\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“ GENERATED TEXT:\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(f\"   {generated_text}\")\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nğŸ’¡ Note: The model learned from 'Mary Had a Little Lamb'\")\n",
    "print(\"   so it should generate text that sounds like the rhyme!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "d8813b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ¨ GENERATING FROM DIFFERENT STARTING WORDS\n",
      "============================================================\n",
      "\n",
      "ğŸš€ Starting with 'mary':\n",
      "   â†’ mary went <END> mary went mary went <END> everywhere that mary went <END>\n",
      "\n",
      "ğŸš€ Starting with 'the':\n",
      "   â†’ the rules <END> it made the children laugh and play <END> laugh and\n",
      "\n",
      "ğŸš€ Starting with 'it':\n",
      "   â†’ it made the children laugh and play <END> to see a lamb at\n",
      "\n",
      "ğŸš€ Starting with 'little':\n",
      "   â†’ little lamb <END> mary had a little lamb <END> its fleece was white\n",
      "\n",
      "ğŸš€ Starting with 'lamb':\n",
      "   â†’ lamb <END> its fleece was white as snow <END> and everywhere that mary\n",
      "\n",
      "============================================================\n",
      "ğŸ’¡ Notice how the model learned patterns from the rhyme!\n",
      "   It predicts words that commonly follow each other.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# BONUS: GENERATE FROM DIFFERENT STARTING WORDS\n",
    "# ============================================================================\n",
    "# Let's try starting with different words to see how the model generates\n",
    "# different text based on its starting context!\n",
    "\n",
    "def generate_from_word(start_word, max_tokens=12):\n",
    "    \"\"\"\n",
    "    Generate text starting from a given word.\n",
    "    \n",
    "    Args:\n",
    "        start_word: The word to start from (must be in vocabulary)\n",
    "        max_tokens: How many new words to generate\n",
    "    \n",
    "    Returns:\n",
    "        The generated text, or None if word not in vocabulary\n",
    "    \"\"\"\n",
    "    # Check if the word exists\n",
    "    if start_word not in word2idx:\n",
    "        print(f\"âŒ '{start_word}' not in vocabulary!\")\n",
    "        print(f\"ğŸ“– Available words: {list(word2idx.keys())}\")\n",
    "        return None\n",
    "    \n",
    "    # Convert to tensor\n",
    "    context = torch.tensor([[word2idx[start_word]]], dtype=torch.long)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        generated = model.generate(context, max_new_tokens=max_tokens)\n",
    "    \n",
    "    # Convert to text\n",
    "    text = \" \".join([idx2word[int(i)] for i in generated[0]])\n",
    "    return text\n",
    "\n",
    "# --- Try different starting words ---\n",
    "\n",
    "start_words = [\"mary\", \"the\", \"it\", \"little\", \"lamb\"]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ¨ GENERATING FROM DIFFERENT STARTING WORDS\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "for word in start_words:\n",
    "    result = generate_from_word(word)\n",
    "    if result:\n",
    "        print(f\"ğŸš€ Starting with '{word}':\")\n",
    "        print(f\"   â†’ {result}\")\n",
    "        print()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ’¡ Notice how the model learned patterns from the rhyme!\")\n",
    "print(\"   It predicts words that commonly follow each other.\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f4dbf5",
   "metadata": {},
   "source": [
    "## Part 12: Understanding What We Built ğŸ“š\n",
    "\n",
    "### ğŸ‰ Congratulations! You've Built a GPT from Scratch!\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‹ Summary of Components\n",
    "\n",
    "Here's everything we built and what each part does:\n",
    "\n",
    "| Component | What It Does | Analogy |\n",
    "|-----------|--------------|---------|\n",
    "| **Token Embedding** | Converts word IDs to 32-number vectors | Dictionary: word â†’ meaning |\n",
    "| **Position Embedding** | Tells model where each word is located | Numbered seats in a theater |\n",
    "| **Self-Attention** | Words look at previous words | Students asking classmates questions |\n",
    "| **Multi-Head Attention** | Multiple perspectives on relationships | Multiple discussions at once |\n",
    "| **Feed-Forward Network** | Each word \"thinks\" independently | Personal reflection time |\n",
    "| **Residual Connections** | Adds input back to output | Remembering what you started with |\n",
    "| **Layer Normalization** | Keeps numbers in reasonable range | Volume control |\n",
    "| **Language Model Head** | Converts to word probabilities | Making a final prediction |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Š Our Model vs. ChatGPT\n",
    "\n",
    "| Feature | Our TinyGPT | GPT-3 | GPT-4 |\n",
    "|---------|-------------|-------|-------|\n",
    "| **Layers** | 2 | 96 | ~120 |\n",
    "| **Embedding Size** | 32 | 12,288 | ~12,000 |\n",
    "| **Attention Heads** | 2 | 96 | ~96 |\n",
    "| **Vocabulary** | 27 words | 50,000 tokens | ~100,000 tokens |\n",
    "| **Parameters** | ~20,000 | 175 Billion | 1.7 Trillion |\n",
    "| **Training Data** | 1 nursery rhyme | 300 billion words | Trillions of words |\n",
    "\n",
    "**Key Insight**: The ARCHITECTURE is the same! ChatGPT is just MUCH bigger.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”‘ Key Concepts You Learned\n",
    "\n",
    "1. **Tokenization**: Converting text to numbers the model can process\n",
    "2. **Embeddings**: Dense vector representations that capture meaning\n",
    "3. **Self-Attention**: The mechanism that makes Transformers powerful\n",
    "4. **Backpropagation**: How neural networks learn from mistakes\n",
    "5. **Gradient Descent**: Moving weights in the direction that reduces loss\n",
    "6. **Autoregressive Generation**: Predicting one word at a time\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ Next Steps to Improve This Model\n",
    "\n",
    "Want to make it better? Try:\n",
    "1. **More training data**: Use longer texts (books, articles)\n",
    "2. **Bigger model**: More layers, larger embeddings\n",
    "3. **Better tokenization**: Use BPE (byte-pair encoding) instead of words\n",
    "4. **GPU training**: Much faster than CPU\n",
    "5. **Learning rate scheduling**: Start fast, slow down later\n",
    "6. **Dropout**: Randomly turn off neurons to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "a82c727a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ“Š FINAL MODEL STATISTICS\n",
      "============================================================\n",
      "\n",
      "ğŸ“– Vocabulary size:     35 words\n",
      "ğŸ“ Context window:      6 tokens (max sequence length)\n",
      "ğŸ”¢ Embedding dimension: 32 numbers per word\n",
      "ğŸ‘€ Attention heads:     2 (parallel attention patterns)\n",
      "ğŸ“š Transformer layers:  2 (stacked blocks)\n",
      "âš™ï¸  Total parameters:   27,747\n",
      "\n",
      "============================================================\n",
      "\n",
      "ğŸ‰ CONGRATULATIONS! ğŸ‰\n",
      "\n",
      "You've successfully built a GPT (Generative Pre-trained Transformer)\n",
      "completely from scratch! This is the same architecture that powers:\n",
      "  â€¢ ChatGPT\n",
      "  â€¢ GPT-4\n",
      "  â€¢ GitHub Copilot\n",
      "  â€¢ Many other AI assistants\n",
      "\n",
      "The only difference is SIZE. Our model has ~20,000 parameters,\n",
      "while GPT-4 has over 1.7 TRILLION parameters!\n",
      "\n",
      "But you now understand how it all works! ğŸ§ \n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FINAL MODEL STATISTICS\n",
    "# ============================================================================\n",
    "# Let's print a summary of everything we built!\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“Š FINAL MODEL STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(f\"ğŸ“– Vocabulary size:     {vocab_size} words\")\n",
    "print(f\"ğŸ“ Context window:      {block_size} tokens (max sequence length)\")\n",
    "print(f\"ğŸ”¢ Embedding dimension: {embedding_dim} numbers per word\")\n",
    "print(f\"ğŸ‘€ Attention heads:     {n_heads} (parallel attention patterns)\")\n",
    "print(f\"ğŸ“š Transformer layers:  {n_layers} (stacked blocks)\")\n",
    "print(f\"âš™ï¸  Total parameters:   {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"ğŸ‰ CONGRATULATIONS! ğŸ‰\")\n",
    "print()\n",
    "print(\"You've successfully built a GPT (Generative Pre-trained Transformer)\")\n",
    "print(\"completely from scratch! This is the same architecture that powers:\")\n",
    "print(\"  â€¢ ChatGPT\")\n",
    "print(\"  â€¢ GPT-4\")\n",
    "print(\"  â€¢ GitHub Copilot\")\n",
    "print(\"  â€¢ Many other AI assistants\")\n",
    "print()\n",
    "print(\"The only difference is SIZE. Our model has ~20,000 parameters,\")\n",
    "print(\"while GPT-4 has over 1.7 TRILLION parameters!\")\n",
    "print()\n",
    "print(\"But you now understand how it all works! ğŸ§ \")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e788c8",
   "metadata": {},
   "source": [
    "## MIT License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
